{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10595134,
          "sourceType": "datasetVersion",
          "datasetId": 6427023
        }
      ],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "3QBDy0Tma2ZO"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "dolachakraborty_misogyny_meme_path = kagglehub.dataset_download('dolachakraborty/misogyny-meme')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "V5tkKxvAa2ZR"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:32:53.658099Z",
          "iopub.execute_input": "2025-01-28T19:32:53.658362Z",
          "iopub.status.idle": "2025-01-28T19:32:57.533358Z",
          "shell.execute_reply.started": "2025-01-28T19:32:53.658335Z",
          "shell.execute_reply": "2025-01-28T19:32:57.532687Z"
        },
        "id": "iLC15Qd8a2ZS",
        "outputId": "f451ba0a-3b4a-4c91-e26a-9022d13bf6a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/misogyny-meme/Test_Labels/45.jpg\n/kaggle/input/misogyny-meme/Test_Labels/239.jpg\n/kaggle/input/misogyny-meme/Test_Labels/187.jpg\n/kaggle/input/misogyny-meme/Test_Labels/76.jpg\n/kaggle/input/misogyny-meme/Test_Labels/474.jpg\n/kaggle/input/misogyny-meme/Test_Labels/501.jpg\n/kaggle/input/misogyny-meme/Test_Labels/760.jpg\n/kaggle/input/misogyny-meme/Test_Labels/342.jpg\n/kaggle/input/misogyny-meme/Test_Labels/646.jpg\n/kaggle/input/misogyny-meme/Test_Labels/544.jpg\n/kaggle/input/misogyny-meme/Test_Labels/795.jpg\n/kaggle/input/misogyny-meme/Test_Labels/270.jpg\n/kaggle/input/misogyny-meme/Test_Labels/182.jpg\n/kaggle/input/misogyny-meme/Test_Labels/215.jpg\n/kaggle/input/misogyny-meme/Test_Labels/115.jpg\n/kaggle/input/misogyny-meme/Test_Labels/425.jpg\n/kaggle/input/misogyny-meme/Test_Labels/824.jpg\n/kaggle/input/misogyny-meme/Test_Labels/622.jpg\n/kaggle/input/misogyny-meme/Test_Labels/440.jpg\n/kaggle/input/misogyny-meme/Test_Labels/944.jpg\n/kaggle/input/misogyny-meme/Test_Labels/67.jpg\n/kaggle/input/misogyny-meme/Test_Labels/973.jpg\n/kaggle/input/misogyny-meme/Test_Labels/950.jpg\n/kaggle/input/misogyny-meme/Test_Labels/176.jpg\n/kaggle/input/misogyny-meme/Test_Labels/279.jpg\n/kaggle/input/misogyny-meme/Test_Labels/757.jpg\n/kaggle/input/misogyny-meme/Test_Labels/806.jpg\n/kaggle/input/misogyny-meme/Test_Labels/463.jpg\n/kaggle/input/misogyny-meme/Test_Labels/33.jpg\n/kaggle/input/misogyny-meme/Test_Labels/234.jpg\n/kaggle/input/misogyny-meme/Test_Labels/818.jpg\n/kaggle/input/misogyny-meme/Test_Labels/990.jpg\n/kaggle/input/misogyny-meme/Test_Labels/35.jpg\n/kaggle/input/misogyny-meme/Test_Labels/61.jpg\n/kaggle/input/misogyny-meme/Test_Labels/190.jpg\n/kaggle/input/misogyny-meme/Test_Labels/353.jpg\n/kaggle/input/misogyny-meme/Test_Labels/191.jpg\n/kaggle/input/misogyny-meme/Test_Labels/427.jpg\n/kaggle/input/misogyny-meme/Test_Labels/604.jpg\n/kaggle/input/misogyny-meme/Test_Labels/73.jpg\n/kaggle/input/misogyny-meme/Test_Labels/871.jpg\n/kaggle/input/misogyny-meme/Test_Labels/98.jpg\n/kaggle/input/misogyny-meme/Test_Labels/396.jpg\n/kaggle/input/misogyny-meme/Test_Labels/620.jpg\n/kaggle/input/misogyny-meme/Test_Labels/123.jpg\n/kaggle/input/misogyny-meme/Test_Labels/710.jpg\n/kaggle/input/misogyny-meme/Test_Labels/408.jpg\n/kaggle/input/misogyny-meme/Test_Labels/559.jpg\n/kaggle/input/misogyny-meme/Test_Labels/112.jpg\n/kaggle/input/misogyny-meme/Test_Labels/639.jpg\n/kaggle/input/misogyny-meme/Test_Labels/412.jpg\n/kaggle/input/misogyny-meme/Test_Labels/9.jpg\n/kaggle/input/misogyny-meme/Test_Labels/101.jpg\n/kaggle/input/misogyny-meme/Test_Labels/311.jpg\n/kaggle/input/misogyny-meme/Test_Labels/832.jpg\n/kaggle/input/misogyny-meme/Test_Labels/529.jpg\n/kaggle/input/misogyny-meme/Test_Labels/963.jpg\n/kaggle/input/misogyny-meme/Test_Labels/943.jpg\n/kaggle/input/misogyny-meme/Test_Labels/390.jpg\n/kaggle/input/misogyny-meme/Test_Labels/254.jpg\n/kaggle/input/misogyny-meme/Test_Labels/410.jpg\n/kaggle/input/misogyny-meme/Test_Labels/874.jpg\n/kaggle/input/misogyny-meme/Test_Labels/774.jpg\n/kaggle/input/misogyny-meme/Test_Labels/903.jpg\n/kaggle/input/misogyny-meme/Test_Labels/525.jpg\n/kaggle/input/misogyny-meme/Test_Labels/563.jpg\n/kaggle/input/misogyny-meme/Test_Labels/467.jpg\n/kaggle/input/misogyny-meme/Test_Labels/140.jpg\n/kaggle/input/misogyny-meme/Test_Labels/531.jpg\n/kaggle/input/misogyny-meme/Test_Labels/713.jpg\n/kaggle/input/misogyny-meme/Test_Labels/322.jpg\n/kaggle/input/misogyny-meme/Test_Labels/79.jpg\n/kaggle/input/misogyny-meme/Test_Labels/284.jpg\n/kaggle/input/misogyny-meme/Test_Labels/387.jpg\n/kaggle/input/misogyny-meme/Test_Labels/695.jpg\n/kaggle/input/misogyny-meme/Test_Labels/649.jpg\n/kaggle/input/misogyny-meme/Test_Labels/634.jpg\n/kaggle/input/misogyny-meme/Test_Labels/856.jpg\n/kaggle/input/misogyny-meme/Test_Labels/317.jpg\n/kaggle/input/misogyny-meme/Test_Labels/356.jpg\n/kaggle/input/misogyny-meme/Test_Labels/794.jpg\n/kaggle/input/misogyny-meme/Test_Labels/246.jpg\n/kaggle/input/misogyny-meme/Test_Labels/252.jpg\n/kaggle/input/misogyny-meme/Test_Labels/763.jpg\n/kaggle/input/misogyny-meme/Test_Labels/430.jpg\n/kaggle/input/misogyny-meme/Test_Labels/22.jpg\n/kaggle/input/misogyny-meme/Test_Labels/378.jpg\n/kaggle/input/misogyny-meme/Test_Labels/677.jpg\n/kaggle/input/misogyny-meme/Test_Labels/400.jpg\n/kaggle/input/misogyny-meme/Test_Labels/171.jpg\n/kaggle/input/misogyny-meme/Test_Labels/809.jpg\n/kaggle/input/misogyny-meme/Test_Labels/258.jpg\n/kaggle/input/misogyny-meme/Test_Labels/942.jpg\n/kaggle/input/misogyny-meme/Test_Labels/543.jpg\n/kaggle/input/misogyny-meme/Test_Labels/346.jpg\n/kaggle/input/misogyny-meme/Test_Labels/687.jpg\n/kaggle/input/misogyny-meme/Test_Labels/339.jpg\n/kaggle/input/misogyny-meme/Test_Labels/656.jpg\n/kaggle/input/misogyny-meme/Test_Labels/421.jpg\n/kaggle/input/misogyny-meme/Test_Labels/545.jpg\n/kaggle/input/misogyny-meme/Test_Labels/449.jpg\n/kaggle/input/misogyny-meme/Test_Labels/104.jpg\n/kaggle/input/misogyny-meme/Test_Labels/24.jpg\n/kaggle/input/misogyny-meme/Test_Labels/269.jpg\n/kaggle/input/misogyny-meme/Test_Labels/326.jpg\n/kaggle/input/misogyny-meme/Test_Labels/168.jpg\n/kaggle/input/misogyny-meme/Test_Labels/31.jpg\n/kaggle/input/misogyny-meme/Test_Labels/841.jpg\n/kaggle/input/misogyny-meme/Test_Labels/582.jpg\n/kaggle/input/misogyny-meme/Test_Labels/209.jpg\n/kaggle/input/misogyny-meme/Test_Labels/939.jpg\n/kaggle/input/misogyny-meme/Test_Labels/376.jpg\n/kaggle/input/misogyny-meme/Test_Labels/667.jpg\n/kaggle/input/misogyny-meme/Test_Labels/789.jpg\n/kaggle/input/misogyny-meme/Test_Labels/644.jpg\n/kaggle/input/misogyny-meme/Test_Labels/350.jpg\n/kaggle/input/misogyny-meme/Test_Labels/417.jpg\n/kaggle/input/misogyny-meme/Test_Labels/852.jpg\n/kaggle/input/misogyny-meme/Test_Labels/206.jpg\n/kaggle/input/misogyny-meme/Test_Labels/164.jpg\n/kaggle/input/misogyny-meme/Test_Labels/571.jpg\n/kaggle/input/misogyny-meme/Test_Labels/161.jpg\n/kaggle/input/misogyny-meme/Test_Labels/698.jpg\n/kaggle/input/misogyny-meme/Test_Labels/681.jpg\n/kaggle/input/misogyny-meme/Test_Labels/445.jpg\n/kaggle/input/misogyny-meme/Test_Labels/800.jpg\n/kaggle/input/misogyny-meme/Test_Labels/488.jpg\n/kaggle/input/misogyny-meme/Test_Labels/220.jpg\n/kaggle/input/misogyny-meme/Test_Labels/783.jpg\n/kaggle/input/misogyny-meme/Test_Labels/157.jpg\n/kaggle/input/misogyny-meme/Test_Labels/889.jpg\n/kaggle/input/misogyny-meme/Test_Labels/183.jpg\n/kaggle/input/misogyny-meme/Test_Labels/858.jpg\n/kaggle/input/misogyny-meme/Test_Labels/971.jpg\n/kaggle/input/misogyny-meme/Test_Labels/742.jpg\n/kaggle/input/misogyny-meme/Test_Labels/373.jpg\n/kaggle/input/misogyny-meme/Test_Labels/360.jpg\n/kaggle/input/misogyny-meme/Test_Labels/86.jpg\n/kaggle/input/misogyny-meme/Test_Labels/557.jpg\n/kaggle/input/misogyny-meme/Test_Labels/test_with_labels.csv\n/kaggle/input/misogyny-meme/Test_Labels/204.jpg\n/kaggle/input/misogyny-meme/Test_Labels/566.jpg\n/kaggle/input/misogyny-meme/Test_Labels/141.jpg\n/kaggle/input/misogyny-meme/Test_Labels/746.jpg\n/kaggle/input/misogyny-meme/Test_Labels/293.jpg\n/kaggle/input/misogyny-meme/Test_Labels/497.jpg\n/kaggle/input/misogyny-meme/Test_Labels/958.jpg\n/kaggle/input/misogyny-meme/Test_Labels/11.jpg\n/kaggle/input/misogyny-meme/Test_Labels/210.jpg\n/kaggle/input/misogyny-meme/Test_Labels/961.jpg\n/kaggle/input/misogyny-meme/Test_Labels/405.jpg\n/kaggle/input/misogyny-meme/Test_Labels/715.jpg\n/kaggle/input/misogyny-meme/Test_Labels/674.jpg\n/kaggle/input/misogyny-meme/Test_Labels/912.jpg\n/kaggle/input/misogyny-meme/Test_Labels/909.jpg\n/kaggle/input/misogyny-meme/Test_Labels/520.jpg\n/kaggle/input/misogyny-meme/Test_Labels/27.jpg\n/kaggle/input/misogyny-meme/Test_Labels/432.jpg\n/kaggle/input/misogyny-meme/Test_Labels/952.jpg\n/kaggle/input/misogyny-meme/Test_Labels/984.jpg\n/kaggle/input/misogyny-meme/Test_Labels/954.jpg\n/kaggle/input/misogyny-meme/Test_Labels/479.jpg\n/kaggle/input/misogyny-meme/Test_Labels/409.jpg\n/kaggle/input/misogyny-meme/Test_Labels/51.jpg\n/kaggle/input/misogyny-meme/Test_Labels/673.jpg\n/kaggle/input/misogyny-meme/Test_Labels/867.jpg\n/kaggle/input/misogyny-meme/Test_Labels/297.jpg\n/kaggle/input/misogyny-meme/Test_Labels/910.jpg\n/kaggle/input/misogyny-meme/Test_Labels/584.jpg\n/kaggle/input/misogyny-meme/Test_Labels/923.jpg\n/kaggle/input/misogyny-meme/Test_Labels/325.jpg\n/kaggle/input/misogyny-meme/Test_Labels/510.jpg\n/kaggle/input/misogyny-meme/Test_Labels/63.jpg\n/kaggle/input/misogyny-meme/Test_Labels/927.jpg\n/kaggle/input/misogyny-meme/Test_Labels/851.jpg\n/kaggle/input/misogyny-meme/Test_Labels/733.jpg\n/kaggle/input/misogyny-meme/Test_Labels/398.jpg\n/kaggle/input/misogyny-meme/Test_Labels/725.jpg\n/kaggle/input/misogyny-meme/Test_Labels/873.jpg\n/kaggle/input/misogyny-meme/Test_Labels/670.jpg\n/kaggle/input/misogyny-meme/Test_Labels/282.jpg\n/kaggle/input/misogyny-meme/Test_Labels/857.jpg\n/kaggle/input/misogyny-meme/Test_Labels/457.jpg\n/kaggle/input/misogyny-meme/Test_Labels/960.jpg\n/kaggle/input/misogyny-meme/Test_Labels/316.jpg\n/kaggle/input/misogyny-meme/Test_Labels/433.jpg\n/kaggle/input/misogyny-meme/Test_Labels/861.jpg\n/kaggle/input/misogyny-meme/Test_Labels/108.jpg\n/kaggle/input/misogyny-meme/Test_Labels/505.jpg\n/kaggle/input/misogyny-meme/Test_Labels/66.jpg\n/kaggle/input/misogyny-meme/Test_Labels/511.jpg\n/kaggle/input/misogyny-meme/Test_Labels/986.jpg\n/kaggle/input/misogyny-meme/Test_Labels/205.jpg\n/kaggle/input/misogyny-meme/Test_Labels/304.jpg\n/kaggle/input/misogyny-meme/Test_Labels/551.jpg\n/kaggle/input/misogyny-meme/Test_Labels/134.jpg\n/kaggle/input/misogyny-meme/Test_Labels/635.jpg\n/kaggle/input/misogyny-meme/Test_Labels/830.jpg\n/kaggle/input/misogyny-meme/Test_Labels/431.jpg\n/kaggle/input/misogyny-meme/Test_Labels/136.jpg\n/kaggle/input/misogyny-meme/Test_Labels/25.jpg\n/kaggle/input/misogyny-meme/Test/45.jpg\n/kaggle/input/misogyny-meme/Test/239.jpg\n/kaggle/input/misogyny-meme/Test/187.jpg\n/kaggle/input/misogyny-meme/Test/76.jpg\n/kaggle/input/misogyny-meme/Test/474.jpg\n/kaggle/input/misogyny-meme/Test/501.jpg\n/kaggle/input/misogyny-meme/Test/760.jpg\n/kaggle/input/misogyny-meme/Test/342.jpg\n/kaggle/input/misogyny-meme/Test/646.jpg\n/kaggle/input/misogyny-meme/Test/544.jpg\n/kaggle/input/misogyny-meme/Test/795.jpg\n/kaggle/input/misogyny-meme/Test/270.jpg\n/kaggle/input/misogyny-meme/Test/182.jpg\n/kaggle/input/misogyny-meme/Test/215.jpg\n/kaggle/input/misogyny-meme/Test/115.jpg\n/kaggle/input/misogyny-meme/Test/425.jpg\n/kaggle/input/misogyny-meme/Test/824.jpg\n/kaggle/input/misogyny-meme/Test/622.jpg\n/kaggle/input/misogyny-meme/Test/440.jpg\n/kaggle/input/misogyny-meme/Test/944.jpg\n/kaggle/input/misogyny-meme/Test/67.jpg\n/kaggle/input/misogyny-meme/Test/973.jpg\n/kaggle/input/misogyny-meme/Test/950.jpg\n/kaggle/input/misogyny-meme/Test/176.jpg\n/kaggle/input/misogyny-meme/Test/279.jpg\n/kaggle/input/misogyny-meme/Test/757.jpg\n/kaggle/input/misogyny-meme/Test/806.jpg\n/kaggle/input/misogyny-meme/Test/463.jpg\n/kaggle/input/misogyny-meme/Test/33.jpg\n/kaggle/input/misogyny-meme/Test/234.jpg\n/kaggle/input/misogyny-meme/Test/818.jpg\n/kaggle/input/misogyny-meme/Test/990.jpg\n/kaggle/input/misogyny-meme/Test/35.jpg\n/kaggle/input/misogyny-meme/Test/61.jpg\n/kaggle/input/misogyny-meme/Test/190.jpg\n/kaggle/input/misogyny-meme/Test/353.jpg\n/kaggle/input/misogyny-meme/Test/191.jpg\n/kaggle/input/misogyny-meme/Test/427.jpg\n/kaggle/input/misogyny-meme/Test/604.jpg\n/kaggle/input/misogyny-meme/Test/73.jpg\n/kaggle/input/misogyny-meme/Test/871.jpg\n/kaggle/input/misogyny-meme/Test/98.jpg\n/kaggle/input/misogyny-meme/Test/396.jpg\n/kaggle/input/misogyny-meme/Test/620.jpg\n/kaggle/input/misogyny-meme/Test/123.jpg\n/kaggle/input/misogyny-meme/Test/710.jpg\n/kaggle/input/misogyny-meme/Test/408.jpg\n/kaggle/input/misogyny-meme/Test/559.jpg\n/kaggle/input/misogyny-meme/Test/112.jpg\n/kaggle/input/misogyny-meme/Test/639.jpg\n/kaggle/input/misogyny-meme/Test/412.jpg\n/kaggle/input/misogyny-meme/Test/9.jpg\n/kaggle/input/misogyny-meme/Test/101.jpg\n/kaggle/input/misogyny-meme/Test/311.jpg\n/kaggle/input/misogyny-meme/Test/832.jpg\n/kaggle/input/misogyny-meme/Test/529.jpg\n/kaggle/input/misogyny-meme/Test/963.jpg\n/kaggle/input/misogyny-meme/Test/943.jpg\n/kaggle/input/misogyny-meme/Test/390.jpg\n/kaggle/input/misogyny-meme/Test/254.jpg\n/kaggle/input/misogyny-meme/Test/410.jpg\n/kaggle/input/misogyny-meme/Test/874.jpg\n/kaggle/input/misogyny-meme/Test/774.jpg\n/kaggle/input/misogyny-meme/Test/903.jpg\n/kaggle/input/misogyny-meme/Test/525.jpg\n/kaggle/input/misogyny-meme/Test/563.jpg\n/kaggle/input/misogyny-meme/Test/467.jpg\n/kaggle/input/misogyny-meme/Test/140.jpg\n/kaggle/input/misogyny-meme/Test/531.jpg\n/kaggle/input/misogyny-meme/Test/713.jpg\n/kaggle/input/misogyny-meme/Test/322.jpg\n/kaggle/input/misogyny-meme/Test/79.jpg\n/kaggle/input/misogyny-meme/Test/284.jpg\n/kaggle/input/misogyny-meme/Test/387.jpg\n/kaggle/input/misogyny-meme/Test/695.jpg\n/kaggle/input/misogyny-meme/Test/649.jpg\n/kaggle/input/misogyny-meme/Test/634.jpg\n/kaggle/input/misogyny-meme/Test/856.jpg\n/kaggle/input/misogyny-meme/Test/317.jpg\n/kaggle/input/misogyny-meme/Test/356.jpg\n/kaggle/input/misogyny-meme/Test/794.jpg\n/kaggle/input/misogyny-meme/Test/246.jpg\n/kaggle/input/misogyny-meme/Test/252.jpg\n/kaggle/input/misogyny-meme/Test/763.jpg\n/kaggle/input/misogyny-meme/Test/430.jpg\n/kaggle/input/misogyny-meme/Test/22.jpg\n/kaggle/input/misogyny-meme/Test/378.jpg\n/kaggle/input/misogyny-meme/Test/677.jpg\n/kaggle/input/misogyny-meme/Test/400.jpg\n/kaggle/input/misogyny-meme/Test/171.jpg\n/kaggle/input/misogyny-meme/Test/809.jpg\n/kaggle/input/misogyny-meme/Test/258.jpg\n/kaggle/input/misogyny-meme/Test/942.jpg\n/kaggle/input/misogyny-meme/Test/543.jpg\n/kaggle/input/misogyny-meme/Test/346.jpg\n/kaggle/input/misogyny-meme/Test/687.jpg\n/kaggle/input/misogyny-meme/Test/339.jpg\n/kaggle/input/misogyny-meme/Test/656.jpg\n/kaggle/input/misogyny-meme/Test/421.jpg\n/kaggle/input/misogyny-meme/Test/545.jpg\n/kaggle/input/misogyny-meme/Test/449.jpg\n/kaggle/input/misogyny-meme/Test/104.jpg\n/kaggle/input/misogyny-meme/Test/24.jpg\n/kaggle/input/misogyny-meme/Test/269.jpg\n/kaggle/input/misogyny-meme/Test/326.jpg\n/kaggle/input/misogyny-meme/Test/168.jpg\n/kaggle/input/misogyny-meme/Test/31.jpg\n/kaggle/input/misogyny-meme/Test/841.jpg\n/kaggle/input/misogyny-meme/Test/582.jpg\n/kaggle/input/misogyny-meme/Test/209.jpg\n/kaggle/input/misogyny-meme/Test/939.jpg\n/kaggle/input/misogyny-meme/Test/376.jpg\n/kaggle/input/misogyny-meme/Test/667.jpg\n/kaggle/input/misogyny-meme/Test/789.jpg\n/kaggle/input/misogyny-meme/Test/644.jpg\n/kaggle/input/misogyny-meme/Test/350.jpg\n/kaggle/input/misogyny-meme/Test/417.jpg\n/kaggle/input/misogyny-meme/Test/852.jpg\n/kaggle/input/misogyny-meme/Test/206.jpg\n/kaggle/input/misogyny-meme/Test/164.jpg\n/kaggle/input/misogyny-meme/Test/571.jpg\n/kaggle/input/misogyny-meme/Test/161.jpg\n/kaggle/input/misogyny-meme/Test/698.jpg\n/kaggle/input/misogyny-meme/Test/681.jpg\n/kaggle/input/misogyny-meme/Test/445.jpg\n/kaggle/input/misogyny-meme/Test/800.jpg\n/kaggle/input/misogyny-meme/Test/488.jpg\n/kaggle/input/misogyny-meme/Test/220.jpg\n/kaggle/input/misogyny-meme/Test/783.jpg\n/kaggle/input/misogyny-meme/Test/157.jpg\n/kaggle/input/misogyny-meme/Test/889.jpg\n/kaggle/input/misogyny-meme/Test/183.jpg\n/kaggle/input/misogyny-meme/Test/858.jpg\n/kaggle/input/misogyny-meme/Test/971.jpg\n/kaggle/input/misogyny-meme/Test/742.jpg\n/kaggle/input/misogyny-meme/Test/373.jpg\n/kaggle/input/misogyny-meme/Test/360.jpg\n/kaggle/input/misogyny-meme/Test/86.jpg\n/kaggle/input/misogyny-meme/Test/557.jpg\n/kaggle/input/misogyny-meme/Test/204.jpg\n/kaggle/input/misogyny-meme/Test/566.jpg\n/kaggle/input/misogyny-meme/Test/141.jpg\n/kaggle/input/misogyny-meme/Test/746.jpg\n/kaggle/input/misogyny-meme/Test/293.jpg\n/kaggle/input/misogyny-meme/Test/497.jpg\n/kaggle/input/misogyny-meme/Test/958.jpg\n/kaggle/input/misogyny-meme/Test/11.jpg\n/kaggle/input/misogyny-meme/Test/210.jpg\n/kaggle/input/misogyny-meme/Test/961.jpg\n/kaggle/input/misogyny-meme/Test/405.jpg\n/kaggle/input/misogyny-meme/Test/715.jpg\n/kaggle/input/misogyny-meme/Test/674.jpg\n/kaggle/input/misogyny-meme/Test/912.jpg\n/kaggle/input/misogyny-meme/Test/909.jpg\n/kaggle/input/misogyny-meme/Test/520.jpg\n/kaggle/input/misogyny-meme/Test/27.jpg\n/kaggle/input/misogyny-meme/Test/432.jpg\n/kaggle/input/misogyny-meme/Test/952.jpg\n/kaggle/input/misogyny-meme/Test/984.jpg\n/kaggle/input/misogyny-meme/Test/954.jpg\n/kaggle/input/misogyny-meme/Test/479.jpg\n/kaggle/input/misogyny-meme/Test/409.jpg\n/kaggle/input/misogyny-meme/Test/51.jpg\n/kaggle/input/misogyny-meme/Test/673.jpg\n/kaggle/input/misogyny-meme/Test/867.jpg\n/kaggle/input/misogyny-meme/Test/297.jpg\n/kaggle/input/misogyny-meme/Test/910.jpg\n/kaggle/input/misogyny-meme/Test/test.csv\n/kaggle/input/misogyny-meme/Test/584.jpg\n/kaggle/input/misogyny-meme/Test/923.jpg\n/kaggle/input/misogyny-meme/Test/325.jpg\n/kaggle/input/misogyny-meme/Test/510.jpg\n/kaggle/input/misogyny-meme/Test/63.jpg\n/kaggle/input/misogyny-meme/Test/927.jpg\n/kaggle/input/misogyny-meme/Test/851.jpg\n/kaggle/input/misogyny-meme/Test/733.jpg\n/kaggle/input/misogyny-meme/Test/398.jpg\n/kaggle/input/misogyny-meme/Test/725.jpg\n/kaggle/input/misogyny-meme/Test/873.jpg\n/kaggle/input/misogyny-meme/Test/670.jpg\n/kaggle/input/misogyny-meme/Test/282.jpg\n/kaggle/input/misogyny-meme/Test/857.jpg\n/kaggle/input/misogyny-meme/Test/457.jpg\n/kaggle/input/misogyny-meme/Test/960.jpg\n/kaggle/input/misogyny-meme/Test/316.jpg\n/kaggle/input/misogyny-meme/Test/433.jpg\n/kaggle/input/misogyny-meme/Test/861.jpg\n/kaggle/input/misogyny-meme/Test/108.jpg\n/kaggle/input/misogyny-meme/Test/505.jpg\n/kaggle/input/misogyny-meme/Test/66.jpg\n/kaggle/input/misogyny-meme/Test/511.jpg\n/kaggle/input/misogyny-meme/Test/986.jpg\n/kaggle/input/misogyny-meme/Test/205.jpg\n/kaggle/input/misogyny-meme/Test/304.jpg\n/kaggle/input/misogyny-meme/Test/551.jpg\n/kaggle/input/misogyny-meme/Test/134.jpg\n/kaggle/input/misogyny-meme/Test/635.jpg\n/kaggle/input/misogyny-meme/Test/830.jpg\n/kaggle/input/misogyny-meme/Test/431.jpg\n/kaggle/input/misogyny-meme/Test/136.jpg\n/kaggle/input/misogyny-meme/Test/25.jpg\n/kaggle/input/misogyny-meme/Train/623.jpg\n/kaggle/input/misogyny-meme/Train/764.jpg\n/kaggle/input/misogyny-meme/Train/771.jpg\n/kaggle/input/misogyny-meme/Train/208.jpg\n/kaggle/input/misogyny-meme/Train/473.jpg\n/kaggle/input/misogyny-meme/Train/333.jpg\n/kaggle/input/misogyny-meme/Train/537.jpg\n/kaggle/input/misogyny-meme/Train/369.jpg\n/kaggle/input/misogyny-meme/Train/654.jpg\n/kaggle/input/misogyny-meme/Train/89.jpg\n/kaggle/input/misogyny-meme/Train/20.jpg\n/kaggle/input/misogyny-meme/Train/275.jpg\n/kaggle/input/misogyny-meme/Train/785.jpg\n/kaggle/input/misogyny-meme/Train/212.jpg\n/kaggle/input/misogyny-meme/Train/792.jpg\n/kaggle/input/misogyny-meme/Train/58.jpg\n/kaggle/input/misogyny-meme/Train/150.jpg\n/kaggle/input/misogyny-meme/Train/6.jpg\n/kaggle/input/misogyny-meme/Train/109.jpg\n/kaggle/input/misogyny-meme/Train/436.jpg\n/kaggle/input/misogyny-meme/Train/539.jpg\n/kaggle/input/misogyny-meme/Train/355.jpg\n/kaggle/input/misogyny-meme/Train/516.jpg\n/kaggle/input/misogyny-meme/Train/71.jpg\n/kaggle/input/misogyny-meme/Train/708.jpg\n/kaggle/input/misogyny-meme/Train/915.jpg\n/kaggle/input/misogyny-meme/Train/815.jpg\n/kaggle/input/misogyny-meme/Train/817.jpg\n/kaggle/input/misogyny-meme/Train/429.jpg\n/kaggle/input/misogyny-meme/Train/682.jpg\n/kaggle/input/misogyny-meme/Train/377.jpg\n/kaggle/input/misogyny-meme/Train/272.jpg\n/kaggle/input/misogyny-meme/Train/489.jpg\n/kaggle/input/misogyny-meme/Train/576.jpg\n/kaggle/input/misogyny-meme/Train/613.jpg\n/kaggle/input/misogyny-meme/Train/930.jpg\n/kaggle/input/misogyny-meme/Train/153.jpg\n/kaggle/input/misogyny-meme/Train/703.jpg\n/kaggle/input/misogyny-meme/Train/189.jpg\n/kaggle/input/misogyny-meme/Train/143.jpg\n/kaggle/input/misogyny-meme/Train/476.jpg\n/kaggle/input/misogyny-meme/Train/717.jpg\n/kaggle/input/misogyny-meme/Train/327.jpg\n/kaggle/input/misogyny-meme/Train/253.jpg\n/kaggle/input/misogyny-meme/Train/343.jpg\n/kaggle/input/misogyny-meme/Train/446.jpg\n/kaggle/input/misogyny-meme/Train/5.jpg\n/kaggle/input/misogyny-meme/Train/366.jpg\n/kaggle/input/misogyny-meme/Train/850.jpg\n/kaggle/input/misogyny-meme/Train/885.jpg\n/kaggle/input/misogyny-meme/Train/151.jpg\n/kaggle/input/misogyny-meme/Train/426.jpg\n/kaggle/input/misogyny-meme/Train/732.jpg\n/kaggle/input/misogyny-meme/Train/503.jpg\n/kaggle/input/misogyny-meme/Train/8.jpg\n/kaggle/input/misogyny-meme/Train/892.jpg\n/kaggle/input/misogyny-meme/Train/988.jpg\n/kaggle/input/misogyny-meme/Train/260.jpg\n/kaggle/input/misogyny-meme/Train/534.jpg\n/kaggle/input/misogyny-meme/Train/202.jpg\n/kaggle/input/misogyny-meme/Train/84.jpg\n/kaggle/input/misogyny-meme/Train/577.jpg\n/kaggle/input/misogyny-meme/Train/751.jpg\n/kaggle/input/misogyny-meme/Train/237.jpg\n/kaggle/input/misogyny-meme/Train/273.jpg\n/kaggle/input/misogyny-meme/Train/286.jpg\n/kaggle/input/misogyny-meme/Train/283.jpg\n/kaggle/input/misogyny-meme/Train/486.jpg\n/kaggle/input/misogyny-meme/Train/513.jpg\n/kaggle/input/misogyny-meme/Train/766.jpg\n/kaggle/input/misogyny-meme/Train/85.jpg\n/kaggle/input/misogyny-meme/Train/564.jpg\n/kaggle/input/misogyny-meme/Train/359.jpg\n/kaggle/input/misogyny-meme/Train/289.jpg\n/kaggle/input/misogyny-meme/Train/768.jpg\n/kaggle/input/misogyny-meme/Train/361.jpg\n/kaggle/input/misogyny-meme/Train/591.jpg\n/kaggle/input/misogyny-meme/Train/82.jpg\n/kaggle/input/misogyny-meme/Train/723.jpg\n/kaggle/input/misogyny-meme/Train/443.jpg\n/kaggle/input/misogyny-meme/Train/334.jpg\n/kaggle/input/misogyny-meme/Train/106.jpg\n/kaggle/input/misogyny-meme/Train/163.jpg\n/kaggle/input/misogyny-meme/Train/160.jpg\n/kaggle/input/misogyny-meme/Train/518.jpg\n/kaggle/input/misogyny-meme/Train/994.jpg\n/kaggle/input/misogyny-meme/Train/38.jpg\n/kaggle/input/misogyny-meme/Train/490.jpg\n/kaggle/input/misogyny-meme/Train/328.jpg\n/kaggle/input/misogyny-meme/Train/788.jpg\n/kaggle/input/misogyny-meme/Train/811.jpg\n/kaggle/input/misogyny-meme/Train/978.jpg\n/kaggle/input/misogyny-meme/Train/863.jpg\n/kaggle/input/misogyny-meme/Train/42.jpg\n/kaggle/input/misogyny-meme/Train/291.jpg\n/kaggle/input/misogyny-meme/Train/949.jpg\n/kaggle/input/misogyny-meme/Train/197.jpg\n/kaggle/input/misogyny-meme/Train/704.jpg\n/kaggle/input/misogyny-meme/Train/10.jpg\n/kaggle/input/misogyny-meme/Train/54.jpg\n/kaggle/input/misogyny-meme/Train/672.jpg\n/kaggle/input/misogyny-meme/Train/271.jpg\n/kaggle/input/misogyny-meme/Train/683.jpg\n/kaggle/input/misogyny-meme/Train/251.jpg\n/kaggle/input/misogyny-meme/Train/759.jpg\n/kaggle/input/misogyny-meme/Train/665.jpg\n/kaggle/input/misogyny-meme/Train/130.jpg\n/kaggle/input/misogyny-meme/Train/916.jpg\n/kaggle/input/misogyny-meme/Train/833.jpg\n/kaggle/input/misogyny-meme/Train/948.jpg\n/kaggle/input/misogyny-meme/Train/951.jpg\n/kaggle/input/misogyny-meme/Train/589.jpg\n/kaggle/input/misogyny-meme/Train/278.jpg\n/kaggle/input/misogyny-meme/Train/743.jpg\n/kaggle/input/misogyny-meme/Train/500.jpg\n/kaggle/input/misogyny-meme/Train/156.jpg\n/kaggle/input/misogyny-meme/Train/380.jpg\n/kaggle/input/misogyny-meme/Train/558.jpg\n/kaggle/input/misogyny-meme/Train/120.jpg\n/kaggle/input/misogyny-meme/Train/629.jpg\n/kaggle/input/misogyny-meme/Train/290.jpg\n/kaggle/input/misogyny-meme/Train/124.jpg\n/kaggle/input/misogyny-meme/Train/441.jpg\n/kaggle/input/misogyny-meme/Train/924.jpg\n/kaggle/input/misogyny-meme/Train/512.jpg\n/kaggle/input/misogyny-meme/Train/59.jpg\n/kaggle/input/misogyny-meme/Train/819.jpg\n/kaggle/input/misogyny-meme/Train/595.jpg\n/kaggle/input/misogyny-meme/Train/379.jpg\n/kaggle/input/misogyny-meme/Train/313.jpg\n/kaggle/input/misogyny-meme/Train/480.jpg\n/kaggle/input/misogyny-meme/Train/527.jpg\n/kaggle/input/misogyny-meme/Train/248.jpg\n/kaggle/input/misogyny-meme/Train/230.jpg\n/kaggle/input/misogyny-meme/Train/580.jpg\n/kaggle/input/misogyny-meme/Train/509.jpg\n/kaggle/input/misogyny-meme/Train/872.jpg\n/kaggle/input/misogyny-meme/Train/776.jpg\n/kaggle/input/misogyny-meme/Train/247.jpg\n/kaggle/input/misogyny-meme/Train/477.jpg\n/kaggle/input/misogyny-meme/Train/94.jpg\n/kaggle/input/misogyny-meme/Train/904.jpg\n/kaggle/input/misogyny-meme/Train/932.jpg\n/kaggle/input/misogyny-meme/Train/362.jpg\n/kaggle/input/misogyny-meme/Train/60.jpg\n/kaggle/input/misogyny-meme/Train/538.jpg\n/kaggle/input/misogyny-meme/Train/221.jpg\n/kaggle/input/misogyny-meme/Train/167.jpg\n/kaggle/input/misogyny-meme/Train/515.jpg\n/kaggle/input/misogyny-meme/Train/894.jpg\n/kaggle/input/misogyny-meme/Train/690.jpg\n/kaggle/input/misogyny-meme/Train/822.jpg\n/kaggle/input/misogyny-meme/Train/664.jpg\n/kaggle/input/misogyny-meme/Train/938.jpg\n/kaggle/input/misogyny-meme/Train/372.jpg\n/kaggle/input/misogyny-meme/Train/388.jpg\n/kaggle/input/misogyny-meme/Train/478.jpg\n/kaggle/input/misogyny-meme/Train/374.jpg\n/kaggle/input/misogyny-meme/Train/452.jpg\n/kaggle/input/misogyny-meme/Train/152.jpg\n/kaggle/input/misogyny-meme/Train/368.jpg\n/kaggle/input/misogyny-meme/Train/745.jpg\n/kaggle/input/misogyny-meme/Train/738.jpg\n/kaggle/input/misogyny-meme/Train/506.jpg\n/kaggle/input/misogyny-meme/Train/192.jpg\n/kaggle/input/misogyny-meme/Train/91.jpg\n/kaggle/input/misogyny-meme/Train/653.jpg\n/kaggle/input/misogyny-meme/Train/705.jpg\n/kaggle/input/misogyny-meme/Train/651.jpg\n/kaggle/input/misogyny-meme/Train/99.jpg\n/kaggle/input/misogyny-meme/Train/382.jpg\n/kaggle/input/misogyny-meme/Train/241.jpg\n/kaggle/input/misogyny-meme/Train/397.jpg\n/kaggle/input/misogyny-meme/Train/37.jpg\n/kaggle/input/misogyny-meme/Train/340.jpg\n/kaggle/input/misogyny-meme/Train/435.jpg\n/kaggle/input/misogyny-meme/Train/1.jpg\n/kaggle/input/misogyny-meme/Train/524.jpg\n/kaggle/input/misogyny-meme/Train/177.jpg\n/kaggle/input/misogyny-meme/Train/868.jpg\n/kaggle/input/misogyny-meme/Train/186.jpg\n/kaggle/input/misogyny-meme/Train/642.jpg\n/kaggle/input/misogyny-meme/Train/720.jpg\n/kaggle/input/misogyny-meme/Train/358.jpg\n/kaggle/input/misogyny-meme/Train/69.jpg\n/kaggle/input/misogyny-meme/Train/468.jpg\n/kaggle/input/misogyny-meme/Train/414.jpg\n/kaggle/input/misogyny-meme/Train/75.jpg\n/kaggle/input/misogyny-meme/Train/660.jpg\n/kaggle/input/misogyny-meme/Train/928.jpg\n/kaggle/input/misogyny-meme/Train/887.jpg\n/kaggle/input/misogyny-meme/Train/117.jpg\n/kaggle/input/misogyny-meme/Train/81.jpg\n/kaggle/input/misogyny-meme/Train/721.jpg\n/kaggle/input/misogyny-meme/Train/974.jpg\n/kaggle/input/misogyny-meme/Train/684.jpg\n/kaggle/input/misogyny-meme/Train/299.jpg\n/kaggle/input/misogyny-meme/Train/381.jpg\n/kaggle/input/misogyny-meme/Train/535.jpg\n/kaggle/input/misogyny-meme/Train/393.jpg\n/kaggle/input/misogyny-meme/Train/424.jpg\n/kaggle/input/misogyny-meme/Train/494.jpg\n/kaggle/input/misogyny-meme/Train/481.jpg\n/kaggle/input/misogyny-meme/Train/401.jpg\n/kaggle/input/misogyny-meme/Train/444.jpg\n/kaggle/input/misogyny-meme/Train/864.jpg\n/kaggle/input/misogyny-meme/Train/276.jpg\n/kaggle/input/misogyny-meme/Train/46.jpg\n/kaggle/input/misogyny-meme/Train/319.jpg\n/kaggle/input/misogyny-meme/Train/137.jpg\n/kaggle/input/misogyny-meme/Train/737.jpg\n/kaggle/input/misogyny-meme/Train/574.jpg\n/kaggle/input/misogyny-meme/Train/905.jpg\n/kaggle/input/misogyny-meme/Train/267.jpg\n/kaggle/input/misogyny-meme/Train/44.jpg\n/kaggle/input/misogyny-meme/Train/561.jpg\n/kaggle/input/misogyny-meme/Train/65.jpg\n/kaggle/input/misogyny-meme/Train/719.jpg\n/kaggle/input/misogyny-meme/Train/50.jpg\n/kaggle/input/misogyny-meme/Train/314.jpg\n/kaggle/input/misogyny-meme/Train/569.jpg\n/kaggle/input/misogyny-meme/Train/235.jpg\n/kaggle/input/misogyny-meme/Train/826.jpg\n/kaggle/input/misogyny-meme/Train/415.jpg\n/kaggle/input/misogyny-meme/Train/482.jpg\n/kaggle/input/misogyny-meme/Train/878.jpg\n/kaggle/input/misogyny-meme/Train/302.jpg\n/kaggle/input/misogyny-meme/Train/179.jpg\n/kaggle/input/misogyny-meme/Train/419.jpg\n/kaggle/input/misogyny-meme/Train/285.jpg\n/kaggle/input/misogyny-meme/Train/590.jpg\n/kaggle/input/misogyny-meme/Train/913.jpg\n/kaggle/input/misogyny-meme/Train/727.jpg\n/kaggle/input/misogyny-meme/Train/105.jpg\n/kaggle/input/misogyny-meme/Train/846.jpg\n/kaggle/input/misogyny-meme/Train/897.jpg\n/kaggle/input/misogyny-meme/Train/371.jpg\n/kaggle/input/misogyny-meme/Train/16.jpg\n/kaggle/input/misogyny-meme/Train/111.jpg\n/kaggle/input/misogyny-meme/Train/55.jpg\n/kaggle/input/misogyny-meme/Train/145.jpg\n/kaggle/input/misogyny-meme/Train/611.jpg\n/kaggle/input/misogyny-meme/Train/758.jpg\n/kaggle/input/misogyny-meme/Train/920.jpg\n/kaggle/input/misogyny-meme/Train/658.jpg\n/kaggle/input/misogyny-meme/Train/135.jpg\n/kaggle/input/misogyny-meme/Train/541.jpg\n/kaggle/input/misogyny-meme/Train/7.jpg\n/kaggle/input/misogyny-meme/Train/659.jpg\n/kaggle/input/misogyny-meme/Train/548.jpg\n/kaggle/input/misogyny-meme/Train/876.jpg\n/kaggle/input/misogyny-meme/Train/765.jpg\n/kaggle/input/misogyny-meme/Train/77.jpg\n/kaggle/input/misogyny-meme/Train/166.jpg\n/kaggle/input/misogyny-meme/Train/292.jpg\n/kaggle/input/misogyny-meme/Train/80.jpg\n/kaggle/input/misogyny-meme/Train/466.jpg\n/kaggle/input/misogyny-meme/Train/159.jpg\n/kaggle/input/misogyny-meme/Train/458.jpg\n/kaggle/input/misogyny-meme/Train/770.jpg\n/kaggle/input/misogyny-meme/Train/880.jpg\n/kaggle/input/misogyny-meme/Train/121.jpg\n/kaggle/input/misogyny-meme/Train/173.jpg\n/kaggle/input/misogyny-meme/Train/645.jpg\n/kaggle/input/misogyny-meme/Train/573.jpg\n/kaggle/input/misogyny-meme/Train/935.jpg\n/kaggle/input/misogyny-meme/Train/967.jpg\n/kaggle/input/misogyny-meme/Train/807.jpg\n/kaggle/input/misogyny-meme/Train/997.jpg\n/kaggle/input/misogyny-meme/Train/671.jpg\n/kaggle/input/misogyny-meme/Train/937.jpg\n/kaggle/input/misogyny-meme/Train/329.jpg\n/kaggle/input/misogyny-meme/Train/498.jpg\n/kaggle/input/misogyny-meme/Train/603.jpg\n/kaggle/input/misogyny-meme/Train/565.jpg\n/kaggle/input/misogyny-meme/Train/392.jpg\n/kaggle/input/misogyny-meme/Train/103.jpg\n/kaggle/input/misogyny-meme/Train/814.jpg\n/kaggle/input/misogyny-meme/Train/174.jpg\n/kaggle/input/misogyny-meme/Train/828.jpg\n/kaggle/input/misogyny-meme/Train/40.jpg\n/kaggle/input/misogyny-meme/Train/391.jpg\n/kaggle/input/misogyny-meme/Train/451.jpg\n/kaggle/input/misogyny-meme/Train/747.jpg\n/kaggle/input/misogyny-meme/Train/854.jpg\n/kaggle/input/misogyny-meme/Train/332.jpg\n/kaggle/input/misogyny-meme/Train/700.jpg\n/kaggle/input/misogyny-meme/Train/750.jpg\n/kaggle/input/misogyny-meme/Train/784.jpg\n/kaggle/input/misogyny-meme/Train/554.jpg\n/kaggle/input/misogyny-meme/Train/308.jpg\n/kaggle/input/misogyny-meme/Train/264.jpg\n/kaggle/input/misogyny-meme/Train/633.jpg\n/kaggle/input/misogyny-meme/Train/199.jpg\n/kaggle/input/misogyny-meme/Train/310.jpg\n/kaggle/input/misogyny-meme/Train/126.jpg\n/kaggle/input/misogyny-meme/Train/142.jpg\n/kaggle/input/misogyny-meme/Train/48.jpg\n/kaggle/input/misogyny-meme/Train/901.jpg\n/kaggle/input/misogyny-meme/Train/917.jpg\n/kaggle/input/misogyny-meme/Train/908.jpg\n/kaggle/input/misogyny-meme/Train/835.jpg\n/kaggle/input/misogyny-meme/Train/730.jpg\n/kaggle/input/misogyny-meme/Train/631.jpg\n/kaggle/input/misogyny-meme/Train/748.jpg\n/kaggle/input/misogyny-meme/Train/931.jpg\n/kaggle/input/misogyny-meme/Train/532.jpg\n/kaggle/input/misogyny-meme/Train/280.jpg\n/kaggle/input/misogyny-meme/Train/555.jpg\n/kaggle/input/misogyny-meme/Train/992.jpg\n/kaggle/input/misogyny-meme/Train/194.jpg\n/kaggle/input/misogyny-meme/Train/663.jpg\n/kaggle/input/misogyny-meme/Train/945.jpg\n/kaggle/input/misogyny-meme/Train/180.jpg\n/kaggle/input/misogyny-meme/Train/567.jpg\n/kaggle/input/misogyny-meme/Train/155.jpg\n/kaggle/input/misogyny-meme/Train/88.jpg\n/kaggle/input/misogyny-meme/Train/465.jpg\n/kaggle/input/misogyny-meme/Train/64.jpg\n/kaggle/input/misogyny-meme/Train/352.jpg\n/kaggle/input/misogyny-meme/Train/734.jpg\n/kaggle/input/misogyny-meme/Train/879.jpg\n/kaggle/input/misogyny-meme/Train/287.jpg\n/kaggle/input/misogyny-meme/Train/955.jpg\n/kaggle/input/misogyny-meme/Train/216.jpg\n/kaggle/input/misogyny-meme/Train/702.jpg\n/kaggle/input/misogyny-meme/Train/450.jpg\n/kaggle/input/misogyny-meme/Train/775.jpg\n/kaggle/input/misogyny-meme/Train/636.jpg\n/kaggle/input/misogyny-meme/Train/158.jpg\n/kaggle/input/misogyny-meme/Train/587.jpg\n/kaggle/input/misogyny-meme/Train/575.jpg\n/kaggle/input/misogyny-meme/Train/772.jpg\n/kaggle/input/misogyny-meme/Train/148.jpg\n/kaggle/input/misogyny-meme/Train/972.jpg\n/kaggle/input/misogyny-meme/Train/586.jpg\n/kaggle/input/misogyny-meme/Train/263.jpg\n/kaggle/input/misogyny-meme/Train/744.jpg\n/kaggle/input/misogyny-meme/Train/195.jpg\n/kaggle/input/misogyny-meme/Train/756.jpg\n/kaggle/input/misogyny-meme/Train/847.jpg\n/kaggle/input/misogyny-meme/Train/999.jpg\n/kaggle/input/misogyny-meme/Train/761.jpg\n/kaggle/input/misogyny-meme/Train/114.jpg\n/kaggle/input/misogyny-meme/Train/906.jpg\n/kaggle/input/misogyny-meme/Train/922.jpg\n/kaggle/input/misogyny-meme/Train/406.jpg\n/kaggle/input/misogyny-meme/Train/281.jpg\n/kaggle/input/misogyny-meme/Train/43.jpg\n/kaggle/input/misogyny-meme/Train/709.jpg\n/kaggle/input/misogyny-meme/Train/138.jpg\n/kaggle/input/misogyny-meme/Train/799.jpg\n/kaggle/input/misogyny-meme/Train/650.jpg\n/kaggle/input/misogyny-meme/Train/496.jpg\n/kaggle/input/misogyny-meme/Train/207.jpg\n/kaggle/input/misogyny-meme/Train/907.jpg\n/kaggle/input/misogyny-meme/Train/598.jpg\n/kaggle/input/misogyny-meme/Train/100.jpg\n/kaggle/input/misogyny-meme/Train/844.jpg\n/kaggle/input/misogyny-meme/Train/13.jpg\n/kaggle/input/misogyny-meme/Train/74.jpg\n/kaggle/input/misogyny-meme/Train/617.jpg\n/kaggle/input/misogyny-meme/Train/882.jpg\n/kaggle/input/misogyny-meme/Train/773.jpg\n/kaggle/input/misogyny-meme/Train/323.jpg\n/kaggle/input/misogyny-meme/Train/223.jpg\n/kaggle/input/misogyny-meme/Train/849.jpg\n/kaggle/input/misogyny-meme/Train/780.jpg\n/kaggle/input/misogyny-meme/Train/762.jpg\n/kaggle/input/misogyny-meme/Train/447.jpg\n/kaggle/input/misogyny-meme/Train/236.jpg\n/kaggle/input/misogyny-meme/Train/363.jpg\n/kaggle/input/misogyny-meme/Train/68.jpg\n/kaggle/input/misogyny-meme/Train/581.jpg\n/kaggle/input/misogyny-meme/Train/53.jpg\n/kaggle/input/misogyny-meme/Train/83.jpg\n/kaggle/input/misogyny-meme/Train/755.jpg\n/kaggle/input/misogyny-meme/Train/588.jpg\n/kaggle/input/misogyny-meme/Train/336.jpg\n/kaggle/input/misogyny-meme/Train/609.jpg\n/kaggle/input/misogyny-meme/Train/347.jpg\n/kaggle/input/misogyny-meme/Train/968.jpg\n/kaggle/input/misogyny-meme/Train/940.jpg\n/kaggle/input/misogyny-meme/Train/354.jpg\n/kaggle/input/misogyny-meme/Train/338.jpg\n/kaggle/input/misogyny-meme/Train/996.jpg\n/kaggle/input/misogyny-meme/Train/146.jpg\n/kaggle/input/misogyny-meme/Train/805.jpg\n/kaggle/input/misogyny-meme/Train/869.jpg\n/kaggle/input/misogyny-meme/Train/198.jpg\n/kaggle/input/misogyny-meme/Train/528.jpg\n/kaggle/input/misogyny-meme/Train/277.jpg\n/kaggle/input/misogyny-meme/Train/877.jpg\n/kaggle/input/misogyny-meme/Train/618.jpg\n/kaggle/input/misogyny-meme/Train/946.jpg\n/kaggle/input/misogyny-meme/Train/331.jpg\n/kaggle/input/misogyny-meme/Train/594.jpg\n/kaggle/input/misogyny-meme/Train/624.jpg\n/kaggle/input/misogyny-meme/Train/72.jpg\n/kaggle/input/misogyny-meme/Train/257.jpg\n/kaggle/input/misogyny-meme/Train/911.jpg\n/kaggle/input/misogyny-meme/Train/578.jpg\n/kaggle/input/misogyny-meme/Train/233.jpg\n/kaggle/input/misogyny-meme/Train/585.jpg\n/kaggle/input/misogyny-meme/Train/139.jpg\n/kaggle/input/misogyny-meme/Train/957.jpg\n/kaggle/input/misogyny-meme/Train/416.jpg\n/kaggle/input/misogyny-meme/Train/802.jpg\n/kaggle/input/misogyny-meme/Train/200.jpg\n/kaggle/input/misogyny-meme/Train/321.jpg\n/kaggle/input/misogyny-meme/Train/32.jpg\n/kaggle/input/misogyny-meme/Train/886.jpg\n/kaggle/input/misogyny-meme/Train/956.jpg\n/kaggle/input/misogyny-meme/Train/676.jpg\n/kaggle/input/misogyny-meme/Train/469.jpg\n/kaggle/input/misogyny-meme/Train/781.jpg\n/kaggle/input/misogyny-meme/Train/244.jpg\n/kaggle/input/misogyny-meme/Train/675.jpg\n/kaggle/input/misogyny-meme/Train/17.jpg\n/kaggle/input/misogyny-meme/Train/394.jpg\n/kaggle/input/misogyny-meme/Train/492.jpg\n/kaggle/input/misogyny-meme/Train/834.jpg\n/kaggle/input/misogyny-meme/Train/884.jpg\n/kaggle/input/misogyny-meme/Train/777.jpg\n/kaggle/input/misogyny-meme/Train/438.jpg\n/kaggle/input/misogyny-meme/Train/696.jpg\n/kaggle/input/misogyny-meme/Train/26.jpg\n/kaggle/input/misogyny-meme/Train/638.jpg\n/kaggle/input/misogyny-meme/Train/726.jpg\n/kaggle/input/misogyny-meme/Train/891.jpg\n/kaggle/input/misogyny-meme/Train/39.jpg\n/kaggle/input/misogyny-meme/Train/219.jpg\n/kaggle/input/misogyny-meme/Train/240.jpg\n/kaggle/input/misogyny-meme/Train/602.jpg\n/kaggle/input/misogyny-meme/Train/288.jpg\n/kaggle/input/misogyny-meme/Train/229.jpg\n/kaggle/input/misogyny-meme/Train/523.jpg\n/kaggle/input/misogyny-meme/Train/870.jpg\n/kaggle/input/misogyny-meme/Train/222.jpg\n/kaggle/input/misogyny-meme/Train/787.jpg\n/kaggle/input/misogyny-meme/Train/330.jpg\n/kaggle/input/misogyny-meme/Train/628.jpg\n/kaggle/input/misogyny-meme/Train/798.jpg\n/kaggle/input/misogyny-meme/Train/383.jpg\n/kaggle/input/misogyny-meme/Train/995.jpg\n/kaggle/input/misogyny-meme/Train/402.jpg\n/kaggle/input/misogyny-meme/Train/865.jpg\n/kaggle/input/misogyny-meme/Train/487.jpg\n/kaggle/input/misogyny-meme/Train/119.jpg\n/kaggle/input/misogyny-meme/Train/165.jpg\n/kaggle/input/misogyny-meme/Train/570.jpg\n/kaggle/input/misogyny-meme/Train/606.jpg\n/kaggle/input/misogyny-meme/Train/979.jpg\n/kaggle/input/misogyny-meme/Train/778.jpg\n/kaggle/input/misogyny-meme/Train/12.jpg\n/kaggle/input/misogyny-meme/Train/722.jpg\n/kaggle/input/misogyny-meme/Train/224.jpg\n/kaggle/input/misogyny-meme/Train/706.jpg\n/kaggle/input/misogyny-meme/Train/749.jpg\n/kaggle/input/misogyny-meme/Train/845.jpg\n/kaggle/input/misogyny-meme/Train/92.jpg\n/kaggle/input/misogyny-meme/Train/893.jpg\n/kaggle/input/misogyny-meme/Train/495.jpg\n/kaggle/input/misogyny-meme/Train/840.jpg\n/kaggle/input/misogyny-meme/Train/859.jpg\n/kaggle/input/misogyny-meme/Train/305.jpg\n/kaggle/input/misogyny-meme/Train/491.jpg\n/kaggle/input/misogyny-meme/Train/70.jpg\n/kaggle/input/misogyny-meme/Train/592.jpg\n/kaggle/input/misogyny-meme/Train/652.jpg\n/kaggle/input/misogyny-meme/Train/843.jpg\n/kaggle/input/misogyny-meme/Train/439.jpg\n/kaggle/input/misogyny-meme/Train/966.jpg\n/kaggle/input/misogyny-meme/Train/453.jpg\n/kaggle/input/misogyny-meme/Train/181.jpg\n/kaggle/input/misogyny-meme/Train/484.jpg\n/kaggle/input/misogyny-meme/Train/842.jpg\n/kaggle/input/misogyny-meme/Train/596.jpg\n/kaggle/input/misogyny-meme/Train/533.jpg\n/kaggle/input/misogyny-meme/Train/579.jpg\n/kaggle/input/misogyny-meme/Train/779.jpg\n/kaggle/input/misogyny-meme/Train/568.jpg\n/kaggle/input/misogyny-meme/Train/550.jpg\n/kaggle/input/misogyny-meme/Train/472.jpg\n/kaggle/input/misogyny-meme/Train/853.jpg\n/kaggle/input/misogyny-meme/Train/259.jpg\n/kaggle/input/misogyny-meme/Train/389.jpg\n/kaggle/input/misogyny-meme/Train/34.jpg\n/kaggle/input/misogyny-meme/Train/661.jpg\n/kaggle/input/misogyny-meme/Train/411.jpg\n/kaggle/input/misogyny-meme/Train/718.jpg\n/kaggle/input/misogyny-meme/Train/370.jpg\n/kaggle/input/misogyny-meme/Train/294.jpg\n/kaggle/input/misogyny-meme/Train/791.jpg\n/kaggle/input/misogyny-meme/Train/448.jpg\n/kaggle/input/misogyny-meme/Train/132.jpg\n/kaggle/input/misogyny-meme/Train/546.jpg\n/kaggle/input/misogyny-meme/Train/52.jpg\n/kaggle/input/misogyny-meme/Train/21.jpg\n/kaggle/input/misogyny-meme/Train/4.jpg\n/kaggle/input/misogyny-meme/Train/804.jpg\n/kaggle/input/misogyny-meme/Train/786.jpg\n/kaggle/input/misogyny-meme/Train/975.jpg\n/kaggle/input/misogyny-meme/Train/309.jpg\n/kaggle/input/misogyny-meme/Train/895.jpg\n/kaggle/input/misogyny-meme/Train/365.jpg\n/kaggle/input/misogyny-meme/Train/312.jpg\n/kaggle/input/misogyny-meme/Train/862.jpg\n/kaggle/input/misogyny-meme/Train/914.jpg\n/kaggle/input/misogyny-meme/Train/583.jpg\n/kaggle/input/misogyny-meme/Train/688.jpg\n/kaggle/input/misogyny-meme/Train/556.jpg\n/kaggle/input/misogyny-meme/Train/540.jpg\n/kaggle/input/misogyny-meme/Train/125.jpg\n/kaggle/input/misogyny-meme/Train/666.jpg\n/kaggle/input/misogyny-meme/Train/128.jpg\n/kaggle/input/misogyny-meme/Train/172.jpg\n/kaggle/input/misogyny-meme/Train/95.jpg\n/kaggle/input/misogyny-meme/Train/553.jpg\n/kaggle/input/misogyny-meme/Train/3.jpg\n/kaggle/input/misogyny-meme/Train/442.jpg\n/kaggle/input/misogyny-meme/Train/36.jpg\n/kaggle/input/misogyny-meme/Train/train.csv\n/kaggle/input/misogyny-meme/Train/384.jpg\n/kaggle/input/misogyny-meme/Train/838.jpg\n/kaggle/input/misogyny-meme/Train/816.jpg\n/kaggle/input/misogyny-meme/Train/217.jpg\n/kaggle/input/misogyny-meme/Train/615.jpg\n/kaggle/input/misogyny-meme/Train/694.jpg\n/kaggle/input/misogyny-meme/Train/657.jpg\n/kaggle/input/misogyny-meme/Train/608.jpg\n/kaggle/input/misogyny-meme/Train/386.jpg\n/kaggle/input/misogyny-meme/Train/933.jpg\n/kaggle/input/misogyny-meme/Train/736.jpg\n/kaggle/input/misogyny-meme/Train/977.jpg\n/kaggle/input/misogyny-meme/Train/96.jpg\n/kaggle/input/misogyny-meme/Train/600.jpg\n/kaggle/input/misogyny-meme/Train/348.jpg\n/kaggle/input/misogyny-meme/Train/981.jpg\n/kaggle/input/misogyny-meme/Train/404.jpg\n/kaggle/input/misogyny-meme/Train/144.jpg\n/kaggle/input/misogyny-meme/Train/662.jpg\n/kaggle/input/misogyny-meme/Train/941.jpg\n/kaggle/input/misogyny-meme/Train/964.jpg\n/kaggle/input/misogyny-meme/Train/407.jpg\n/kaggle/input/misogyny-meme/Train/522.jpg\n/kaggle/input/misogyny-meme/Train/790.jpg\n/kaggle/input/misogyny-meme/Train/739.jpg\n/kaggle/input/misogyny-meme/Train/437.jpg\n/kaggle/input/misogyny-meme/Train/896.jpg\n/kaggle/input/misogyny-meme/Train/712.jpg\n/kaggle/input/misogyny-meme/Train/812.jpg\n/kaggle/input/misogyny-meme/Train/475.jpg\n/kaggle/input/misogyny-meme/Train/782.jpg\n/kaggle/input/misogyny-meme/Train/888.jpg\n/kaggle/input/misogyny-meme/Train/162.jpg\n/kaggle/input/misogyny-meme/Train/231.jpg\n/kaggle/input/misogyny-meme/Train/643.jpg\n/kaggle/input/misogyny-meme/Train/499.jpg\n/kaggle/input/misogyny-meme/Train/508.jpg\n/kaggle/input/misogyny-meme/Train/19.jpg\n/kaggle/input/misogyny-meme/Train/201.jpg\n/kaggle/input/misogyny-meme/Train/303.jpg\n/kaggle/input/misogyny-meme/Train/514.jpg\n/kaggle/input/misogyny-meme/Train/866.jpg\n/kaggle/input/misogyny-meme/Train/87.jpg\n/kaggle/input/misogyny-meme/Train/47.jpg\n/kaggle/input/misogyny-meme/Train/612.jpg\n/kaggle/input/misogyny-meme/Train/562.jpg\n/kaggle/input/misogyny-meme/Train/547.jpg\n/kaggle/input/misogyny-meme/Train/93.jpg\n/kaggle/input/misogyny-meme/Train/470.jpg\n/kaggle/input/misogyny-meme/Train/170.jpg\n/kaggle/input/misogyny-meme/Train/983.jpg\n/kaggle/input/misogyny-meme/Train/707.jpg\n/kaggle/input/misogyny-meme/Train/428.jpg\n/kaggle/input/misogyny-meme/Train/324.jpg\n/kaggle/input/misogyny-meme/Train/689.jpg\n/kaggle/input/misogyny-meme/Train/14.jpg\n/kaggle/input/misogyny-meme/Train/823.jpg\n/kaggle/input/misogyny-meme/Train/860.jpg\n/kaggle/input/misogyny-meme/Train/519.jpg\n/kaggle/input/misogyny-meme/Train/697.jpg\n/kaggle/input/misogyny-meme/Train/175.jpg\n/kaggle/input/misogyny-meme/Train/517.jpg\n/kaggle/input/misogyny-meme/Train/526.jpg\n/kaggle/input/misogyny-meme/Train/226.jpg\n/kaggle/input/misogyny-meme/Train/462.jpg\n/kaggle/input/misogyny-meme/Train/116.jpg\n/kaggle/input/misogyny-meme/Train/898.jpg\n/kaggle/input/misogyny-meme/Train/985.jpg\n/kaggle/input/misogyny-meme/Train/827.jpg\n/kaggle/input/misogyny-meme/Train/614.jpg\n/kaggle/input/misogyny-meme/Train/78.jpg\n/kaggle/input/misogyny-meme/Train/320.jpg\n/kaggle/input/misogyny-meme/Train/899.jpg\n/kaggle/input/misogyny-meme/Train/335.jpg\n/kaggle/input/misogyny-meme/Train/460.jpg\n/kaggle/input/misogyny-meme/Train/232.jpg\n/kaggle/input/misogyny-meme/Train/883.jpg\n/kaggle/input/misogyny-meme/Train/752.jpg\n/kaggle/input/misogyny-meme/Train/969.jpg\n/kaggle/input/misogyny-meme/Train/797.jpg\n/kaggle/input/misogyny-meme/Train/203.jpg\n/kaggle/input/misogyny-meme/Train/640.jpg\n/kaggle/input/misogyny-meme/Train/679.jpg\n/kaggle/input/misogyny-meme/Train/542.jpg\n/kaggle/input/misogyny-meme/Train/801.jpg\n/kaggle/input/misogyny-meme/Train/632.jpg\n/kaggle/input/misogyny-meme/Train/298.jpg\n/kaggle/input/misogyny-meme/Train/483.jpg\n/kaggle/input/misogyny-meme/Train/306.jpg\n/kaggle/input/misogyny-meme/Train/881.jpg\n/kaggle/input/misogyny-meme/Train/395.jpg\n/kaggle/input/misogyny-meme/Train/423.jpg\n/kaggle/input/misogyny-meme/Train/307.jpg\n/kaggle/input/misogyny-meme/Train/593.jpg\n/kaggle/input/misogyny-meme/Train/504.jpg\n/kaggle/input/misogyny-meme/Train/129.jpg\n/kaggle/input/misogyny-meme/Train/735.jpg\n/kaggle/input/misogyny-meme/Train/875.jpg\n/kaggle/input/misogyny-meme/Train/422.jpg\n/kaggle/input/misogyny-meme/Train/133.jpg\n/kaggle/input/misogyny-meme/Train/315.jpg\n/kaggle/input/misogyny-meme/Train/810.jpg\n/kaggle/input/misogyny-meme/Train/989.jpg\n/kaggle/input/misogyny-meme/Train/987.jpg\n/kaggle/input/misogyny-meme/Train/218.jpg\n/kaggle/input/misogyny-meme/Train/249.jpg\n/kaggle/input/misogyny-meme/Train/213.jpg\n/kaggle/input/misogyny-meme/Train/597.jpg\n/kaggle/input/misogyny-meme/Train/461.jpg\n/kaggle/input/misogyny-meme/Train/925.jpg\n/kaggle/input/misogyny-meme/Train/454.jpg\n/kaggle/input/misogyny-meme/Train/605.jpg\n/kaggle/input/misogyny-meme/Train/754.jpg\n/kaggle/input/misogyny-meme/Train/753.jpg\n/kaggle/input/misogyny-meme/Train/90.jpg\n/kaggle/input/misogyny-meme/Train/599.jpg\n/kaggle/input/misogyny-meme/Train/921.jpg\n/kaggle/input/misogyny-meme/Train/728.jpg\n/kaggle/input/misogyny-meme/Dev/820.jpg\n/kaggle/input/misogyny-meme/Dev/dev.csv\n/kaggle/input/misogyny-meme/Dev/56.jpg\n/kaggle/input/misogyny-meme/Dev/149.jpg\n/kaggle/input/misogyny-meme/Dev/521.jpg\n/kaggle/input/misogyny-meme/Dev/185.jpg\n/kaggle/input/misogyny-meme/Dev/243.jpg\n/kaggle/input/misogyny-meme/Dev/131.jpg\n/kaggle/input/misogyny-meme/Dev/626.jpg\n/kaggle/input/misogyny-meme/Dev/641.jpg\n/kaggle/input/misogyny-meme/Dev/265.jpg\n/kaggle/input/misogyny-meme/Dev/118.jpg\n/kaggle/input/misogyny-meme/Dev/295.jpg\n/kaggle/input/misogyny-meme/Dev/668.jpg\n/kaggle/input/misogyny-meme/Dev/30.jpg\n/kaggle/input/misogyny-meme/Dev/97.jpg\n/kaggle/input/misogyny-meme/Dev/113.jpg\n/kaggle/input/misogyny-meme/Dev/349.jpg\n/kaggle/input/misogyny-meme/Dev/211.jpg\n/kaggle/input/misogyny-meme/Dev/829.jpg\n/kaggle/input/misogyny-meme/Dev/630.jpg\n/kaggle/input/misogyny-meme/Dev/178.jpg\n/kaggle/input/misogyny-meme/Dev/62.jpg\n/kaggle/input/misogyny-meme/Dev/998.jpg\n/kaggle/input/misogyny-meme/Dev/456.jpg\n/kaggle/input/misogyny-meme/Dev/965.jpg\n/kaggle/input/misogyny-meme/Dev/680.jpg\n/kaggle/input/misogyny-meme/Dev/601.jpg\n/kaggle/input/misogyny-meme/Dev/188.jpg\n/kaggle/input/misogyny-meme/Dev/274.jpg\n/kaggle/input/misogyny-meme/Dev/375.jpg\n/kaggle/input/misogyny-meme/Dev/41.jpg\n/kaggle/input/misogyny-meme/Dev/685.jpg\n/kaggle/input/misogyny-meme/Dev/256.jpg\n/kaggle/input/misogyny-meme/Dev/627.jpg\n/kaggle/input/misogyny-meme/Dev/57.jpg\n/kaggle/input/misogyny-meme/Dev/227.jpg\n/kaggle/input/misogyny-meme/Dev/193.jpg\n/kaggle/input/misogyny-meme/Dev/701.jpg\n/kaggle/input/misogyny-meme/Dev/836.jpg\n/kaggle/input/misogyny-meme/Dev/831.jpg\n/kaggle/input/misogyny-meme/Dev/793.jpg\n/kaggle/input/misogyny-meme/Dev/403.jpg\n/kaggle/input/misogyny-meme/Dev/560.jpg\n/kaggle/input/misogyny-meme/Dev/716.jpg\n/kaggle/input/misogyny-meme/Dev/455.jpg\n/kaggle/input/misogyny-meme/Dev/530.jpg\n/kaggle/input/misogyny-meme/Dev/127.jpg\n/kaggle/input/misogyny-meme/Dev/196.jpg\n/kaggle/input/misogyny-meme/Dev/29.jpg\n/kaggle/input/misogyny-meme/Dev/929.jpg\n/kaggle/input/misogyny-meme/Dev/619.jpg\n/kaggle/input/misogyny-meme/Dev/23.jpg\n/kaggle/input/misogyny-meme/Dev/434.jpg\n/kaggle/input/misogyny-meme/Dev/214.jpg\n/kaggle/input/misogyny-meme/Dev/699.jpg\n/kaggle/input/misogyny-meme/Dev/225.jpg\n/kaggle/input/misogyny-meme/Dev/464.jpg\n/kaggle/input/misogyny-meme/Dev/28.jpg\n/kaggle/input/misogyny-meme/Dev/837.jpg\n/kaggle/input/misogyny-meme/Dev/691.jpg\n/kaggle/input/misogyny-meme/Dev/678.jpg\n/kaggle/input/misogyny-meme/Dev/261.jpg\n/kaggle/input/misogyny-meme/Dev/357.jpg\n/kaggle/input/misogyny-meme/Dev/296.jpg\n/kaggle/input/misogyny-meme/Dev/993.jpg\n/kaggle/input/misogyny-meme/Dev/918.jpg\n/kaggle/input/misogyny-meme/Dev/572.jpg\n/kaggle/input/misogyny-meme/Dev/169.jpg\n/kaggle/input/misogyny-meme/Dev/821.jpg\n/kaggle/input/misogyny-meme/Dev/947.jpg\n/kaggle/input/misogyny-meme/Dev/926.jpg\n/kaggle/input/misogyny-meme/Dev/344.jpg\n/kaggle/input/misogyny-meme/Dev/420.jpg\n/kaggle/input/misogyny-meme/Dev/485.jpg\n/kaggle/input/misogyny-meme/Dev/502.jpg\n/kaggle/input/misogyny-meme/Dev/255.jpg\n/kaggle/input/misogyny-meme/Dev/552.jpg\n/kaggle/input/misogyny-meme/Dev/693.jpg\n/kaggle/input/misogyny-meme/Dev/228.jpg\n/kaggle/input/misogyny-meme/Dev/669.jpg\n/kaggle/input/misogyny-meme/Dev/337.jpg\n/kaggle/input/misogyny-meme/Dev/507.jpg\n/kaggle/input/misogyny-meme/Dev/934.jpg\n/kaggle/input/misogyny-meme/Dev/107.jpg\n/kaggle/input/misogyny-meme/Dev/154.jpg\n/kaggle/input/misogyny-meme/Dev/301.jpg\n/kaggle/input/misogyny-meme/Dev/318.jpg\n/kaggle/input/misogyny-meme/Dev/769.jpg\n/kaggle/input/misogyny-meme/Dev/741.jpg\n/kaggle/input/misogyny-meme/Dev/102.jpg\n/kaggle/input/misogyny-meme/Dev/970.jpg\n/kaggle/input/misogyny-meme/Dev/980.jpg\n/kaggle/input/misogyny-meme/Dev/848.jpg\n/kaggle/input/misogyny-meme/Dev/351.jpg\n/kaggle/input/misogyny-meme/Dev/459.jpg\n/kaggle/input/misogyny-meme/Dev/991.jpg\n/kaggle/input/misogyny-meme/Dev/242.jpg\n/kaggle/input/misogyny-meme/Dev/1000.jpg\n/kaggle/input/misogyny-meme/Dev/15.jpg\n/kaggle/input/misogyny-meme/Dev/692.jpg\n/kaggle/input/misogyny-meme/Dev/813.jpg\n/kaggle/input/misogyny-meme/Dev/808.jpg\n/kaggle/input/misogyny-meme/Dev/936.jpg\n/kaggle/input/misogyny-meme/Dev/238.jpg\n/kaggle/input/misogyny-meme/Dev/616.jpg\n/kaggle/input/misogyny-meme/Dev/767.jpg\n/kaggle/input/misogyny-meme/Dev/122.jpg\n/kaggle/input/misogyny-meme/Dev/686.jpg\n/kaggle/input/misogyny-meme/Dev/976.jpg\n/kaggle/input/misogyny-meme/Dev/959.jpg\n/kaggle/input/misogyny-meme/Dev/900.jpg\n/kaggle/input/misogyny-meme/Dev/399.jpg\n/kaggle/input/misogyny-meme/Dev/982.jpg\n/kaggle/input/misogyny-meme/Dev/262.jpg\n/kaggle/input/misogyny-meme/Dev/729.jpg\n/kaggle/input/misogyny-meme/Dev/341.jpg\n/kaggle/input/misogyny-meme/Dev/418.jpg\n/kaggle/input/misogyny-meme/Dev/825.jpg\n/kaggle/input/misogyny-meme/Dev/184.jpg\n/kaggle/input/misogyny-meme/Dev/300.jpg\n/kaggle/input/misogyny-meme/Dev/250.jpg\n/kaggle/input/misogyny-meme/Dev/919.jpg\n/kaggle/input/misogyny-meme/Dev/711.jpg\n/kaggle/input/misogyny-meme/Dev/855.jpg\n/kaggle/input/misogyny-meme/Dev/493.jpg\n/kaggle/input/misogyny-meme/Dev/610.jpg\n/kaggle/input/misogyny-meme/Dev/536.jpg\n/kaggle/input/misogyny-meme/Dev/902.jpg\n/kaggle/input/misogyny-meme/Dev/110.jpg\n/kaggle/input/misogyny-meme/Dev/655.jpg\n/kaggle/input/misogyny-meme/Dev/413.jpg\n/kaggle/input/misogyny-meme/Dev/724.jpg\n/kaggle/input/misogyny-meme/Dev/385.jpg\n/kaggle/input/misogyny-meme/Dev/625.jpg\n/kaggle/input/misogyny-meme/Dev/471.jpg\n/kaggle/input/misogyny-meme/Dev/731.jpg\n/kaggle/input/misogyny-meme/Dev/621.jpg\n/kaggle/input/misogyny-meme/Dev/803.jpg\n/kaggle/input/misogyny-meme/Dev/549.jpg\n/kaggle/input/misogyny-meme/Dev/18.jpg\n/kaggle/input/misogyny-meme/Dev/268.jpg\n/kaggle/input/misogyny-meme/Dev/953.jpg\n/kaggle/input/misogyny-meme/Dev/607.jpg\n/kaggle/input/misogyny-meme/Dev/345.jpg\n/kaggle/input/misogyny-meme/Dev/740.jpg\n/kaggle/input/misogyny-meme/Dev/637.jpg\n/kaggle/input/misogyny-meme/Dev/962.jpg\n/kaggle/input/misogyny-meme/Dev/367.jpg\n/kaggle/input/misogyny-meme/Dev/266.jpg\n/kaggle/input/misogyny-meme/Dev/245.jpg\n/kaggle/input/misogyny-meme/Dev/648.jpg\n/kaggle/input/misogyny-meme/Dev/49.jpg\n/kaggle/input/misogyny-meme/Dev/890.jpg\n/kaggle/input/misogyny-meme/Dev/796.jpg\n/kaggle/input/misogyny-meme/Dev/714.jpg\n/kaggle/input/misogyny-meme/Dev/839.jpg\n/kaggle/input/misogyny-meme/Dev/647.jpg\n/kaggle/input/misogyny-meme/Dev/2.jpg\n/kaggle/input/misogyny-meme/Dev/364.jpg\n/kaggle/input/misogyny-meme/Dev/147.jpg\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, GlobalMaxPooling2D, concatenate, Dropout, Conv2D, MaxPooling2D, Flatten\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:32:57.534342Z",
          "iopub.execute_input": "2025-01-28T19:32:57.534722Z",
          "iopub.status.idle": "2025-01-28T19:33:05.71324Z",
          "shell.execute_reply.started": "2025-01-28T19:32:57.534699Z",
          "shell.execute_reply": "2025-01-28T19:33:05.712327Z"
        },
        "id": "PxHIEkvra2ZU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "img_width = 224\n",
        "img_height = 224\n",
        "\n",
        "# Set text parameters\n",
        "max_words = 10000\n",
        "max_len = 200\n",
        "\n",
        "# Set batch size and number of epochs\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "# Load and preprocess image data\n",
        "def load_image(path):\n",
        "    img = load_img(path, target_size=(img_width, img_height))\n",
        "    img = img_to_array(img)\n",
        "    img = img / 255.0\n",
        "    return img\n",
        "\n",
        "def preprocess_images(data, img_dir):\n",
        "    images = []\n",
        "    for image_id in data['image_id']:\n",
        "        image_path = os.path.join(img_dir, f\"{image_id}.jpg\")\n",
        "        img = load_image(image_path)\n",
        "        images.append(img)\n",
        "    images = np.array(images)\n",
        "    return images\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:33:05.714787Z",
          "iopub.execute_input": "2025-01-28T19:33:05.715419Z",
          "iopub.status.idle": "2025-01-28T19:33:05.720479Z",
          "shell.execute_reply.started": "2025-01-28T19:33:05.715393Z",
          "shell.execute_reply": "2025-01-28T19:33:05.719865Z"
        },
        "id": "MPZpgV1Qa2ZU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "image_dir = '/kaggle/input/misogyny-meme/Train'\n",
        "image_dir_dev='/kaggle/input/misogyny-meme/Dev'\n",
        "image_dir_test = '/kaggle/input/misogyny-meme/Test'\n",
        "# Load and preprocess training, validation, and test images\n",
        "train_data = pd.read_csv('/kaggle/input/misogyny-meme/Train/train.csv')\n",
        "val_data = pd.read_csv('/kaggle/input/misogyny-meme/Dev/dev.csv')\n",
        "test_data = pd.read_csv('/kaggle/input/misogyny-meme/Test/test.csv')\n",
        "test_data_labels = pd.read_csv('/kaggle/input/misogyny-meme/Test_Labels/test_with_labels.csv')\n",
        "\n",
        "train_images = preprocess_images(train_data, image_dir)\n",
        "val_images = preprocess_images(val_data, image_dir_dev)\n",
        "test_images = preprocess_images(test_data, image_dir_test)\n",
        "test_images_labels = preprocess_images(test_data_labels, image_dir_test)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:33:05.721363Z",
          "iopub.execute_input": "2025-01-28T19:33:05.721603Z",
          "iopub.status.idle": "2025-01-28T19:33:27.996129Z",
          "shell.execute_reply.started": "2025-01-28T19:33:05.721584Z",
          "shell.execute_reply": "2025-01-28T19:33:27.995419Z"
        },
        "id": "M5otbmuia2ZV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_labels.head(60)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:34:02.596249Z",
          "iopub.execute_input": "2025-01-28T19:34:02.59655Z",
          "iopub.status.idle": "2025-01-28T19:34:02.611538Z",
          "shell.execute_reply.started": "2025-01-28T19:34:02.596523Z",
          "shell.execute_reply": "2025-01-28T19:34:02.610387Z"
        },
        "id": "bcrLOk76a2ZV",
        "outputId": "14698aa5-d2f5-402a-dcf2-64c0791e1a20"
      },
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "    image_id  labels  \\\n0        954       0   \n1        239       0   \n2         61       1   \n3        984       0   \n4        774       0   \n5        427       1   \n6        960       0   \n7        387       0   \n8        520       0   \n9        563       1   \n10       856       0   \n11       545       1   \n12       511       0   \n13       252       0   \n14       742       0   \n15       873       0   \n16       566       1   \n17       325       0   \n18       760       0   \n19        27       1   \n20       544       1   \n21       297       0   \n22       258       0   \n23        63       1   \n24       857       0   \n25       433       1   \n26       543       0   \n27       794       0   \n28       710       1   \n29       725       1   \n30        76       1   \n31       390       0   \n32       939       0   \n33        45       1   \n34       373       0   \n35       677       1   \n36       205       0   \n37       410       1   \n38       557       1   \n39        33       1   \n40       861       0   \n41        35       1   \n42       800       0   \n43       818       0   \n44       190       0   \n45       215       0   \n46       187       0   \n47       316       0   \n48       488       1   \n49       635       1   \n50       559       1   \n51       789       0   \n52        11       1   \n53       973       0   \n54       634       1   \n55       673       1   \n56       342       0   \n57       958       0   \n58       971       0   \n59       432       1   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                      transcriptions  \n0                                                                                                                                                                                                                                                                                                                                                                                        cilma??....  \n1                                                                                                                                                                                                                                                                                                                                    5     . ...      \n2                                                                                                                                                                                                                                                                                                                                                                                      ..  \n3                                                                                                                                                                                                                                                                                                                                         DIS 2 GOAL AGAINST PERU   \n4                                                                                                                                                                                                             * \\n \\n..  equipment   \\n\\n  !!\\n..     \\n\\n   ..  \\n !!!  !  \n5                                                                                                                                                                                                                                                                                                                                                                *..  \n6                                                                                                                                                                                                                                                                                                                                                                 MUN-SHU  *Haters *Harry Maquire 1 Harry Maguire is the MOTM!  \n7   www\\nAnju jr\\nTODAY\\nHi Chetta\\n01:02 PM\\nAthey\\n01:02 PM\\nenik ariyamayrunnu kuttikk enne stamanenn\\n01:03 PM\\nentayalum kaathirunnath veruthe aayilla\\n01:03 PM\\nenikkum istamanu\\n01:03 PM\\nhttps://www.instagram.com /p/B80ezrXFy-h/?igshid=\\n17ki4rfn78t4k\\n01:03 PM\\nee link onn like cheyyane\\n01:04 PM\\nente friendinte oru competition\\naanu\\n01:04 PM\\nType a message\\n*  \\nAbin A Kottarakkara  \n8                                                                                                                                                                                                                   \\n\\n \\nbhaai\\n  \n9                                                                                                                                                                                                                                                                                                                                             ?   .    \n10                                                                                                                                                                                                                                                                                           \\n     ...\\n* \\n\\n      .\\nTernational  \n11                                                                                                                                                                                                                                                                                                                   *   ..\\n* .     \n12                                                                                                                                                                                                                                                                                                                                                              \n13                                                                                                                                                                                                                                                                                                                               \"      \n14                                                                                                                                                                                                                                                                                      \\n    \\n .\\n  \\n  \\n  \\n  \n15                                                                                                                                                                                                                                                                                                      \\n    (Sunny)     ( S.Unny )     ( Sunny )  \n16                                                                                                                                                                                                                                                                                                                \n17                                                                                                                                                                                                                                                                        Girls  \\n*\\n     \\nBoys  \\n        \n18                                                                                                                                                                                                                    07.10.2023\\n'\\n;\\n ,    \\n   \\n-   \\n !\\n  ?  \n19                                                                                                                                                                                                                                                                                                                                          .  \n20                                                                                                                                                                                                                                           *    *(         ) \\n.     \\n  \n21                                                                                                                                                                                                                                                                                                                                                               ..\\n    ..  \n22                                                                                                                                                                                                                                                                                                \\n       \n23                                                                                                                                                                                                                                                                                                                                         \\n*        \n24                                                                                                                                                                                                                                                                                                                                                       Unni Mukundan \\n\\nMera BHARAT\\nMy India\\n\\n\\n \\n  \n25                                                                                                                                                                                                                                                                                                                                                                                  .     \n26                                                                                                                                                                                                                                                                                                                              .. .. ...  ..       \n27                                                                                                                                                                                                                                                                                                                                                                   \\nfTMM\\nExpectation\\nTMM\\nReality\\nR177\\nA  \n28                                                                                                                                 |                              \n29                                                                                                                                                                                                                                                                                                                                                                                                             BACK     \n30                                                                                                                                                                                                                                                                                                                            VS  \\n ..  .  \\n    .  \n31                                                                                                                                                                                                                                                                                                                                                                \\n \\n  \n32                                                                                                                                                                                                                                                                                                                                                                                                  Sj\\npha\\nS\\n    \n33                                                                                                                                                                                            *     *    KTM   \\n*    * \\n*     *    Apache...!  \n34                                                                                                                                                                                                                                                                 20    \\n\\n@\\n     \\n        \n35                                                                                                                                                                                                                                                                                                                                        \n36                                                                                                                                                                                                                                                                                10  experience \\n   \\n\\nexperience     \\n  \n37                                                                                                                                                                                                                                                                                                       *            \n38                                                                                                                                                                                                                                                                                                                                                              \n39                                                                                                                                                                                    * ..   ..    . *....       \\n*   ..     ..  \n40                                                                                                                                                                                                                 * \\n \\n   , |   \\n\\n    \\n   8  \\n    \n41                                                                                                                                                                       * .       ..   , \\n  .. *  ..     .. \\n     ..!!  \n42                                                                                                                                                                                                                                                          \\n       \\n\\n*MV \\n \\n*\\n      \\n  \n43                                                                                                                                                                        \\n    \\n\\n   ..?? -\\n.\\n   ..?    .  ...\\n   ...\\n   ...\\n  \n44                                                                                                                                                                                                                                                                                                                                              2  .  ?  \n45                                                                                                                                                                                                                                                                                                       2      :   .   \\n ..  \n46                                                                                                                                                                                                                                                                                                                                             100  500  1000  250  000  \n47                                                                                                                                                                                                                                                                                                                                                                                            \\n   \n48                                                                                                                                                                                                                                                                                                                                                                 \n49                                                                                                                                                                                                                                                                                                                          \n50                                                                                                                                                                                                                                                                                                                                                                                                     \n51                                                                                                                                                                                                  \\n \\n  . \\n*\\n\\nHelp!\\nHelp!\\n \\n      \\n\\n     \n52                                                                                                                                                                                                                                                                                                                                               \n53                                                                                                                                                                            . #neymar #FIFAWorldCupQualifiers i mathrubhumi.com   ;     ,          \n54                                                                                                                                                                                                                                                                                         ,       ..         \n55                                                                                                                                                                                                                                                \n56                                                                                                                                                                                                                                                                                                                                                            *\\n ...!\\nTM   \\n. ...!  \n57                                                                                                                                                                                                                                                                                                                                                                 \n58                                                                                                                                                                                                                     TMM            TMM      TMM  \n59                                                                                                                                                                                                                                                                                                                                                VPN Connect         *  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>labels</th>\n      <th>transcriptions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>954</td>\n      <td>0</td>\n      <td>    cilma??....</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>239</td>\n      <td>0</td>\n      <td>    5     . ...    </td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>61</td>\n      <td>1</td>\n      <td>     ..</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>984</td>\n      <td>0</td>\n      <td>       DIS 2 GOAL AGAINST PERU </td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>774</td>\n      <td>0</td>\n      <td>* \\n \\n..  equipment   \\n\\n  !!\\n..     \\n\\n   ..  \\n !!!  !</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>427</td>\n      <td>1</td>\n      <td>        *..</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>960</td>\n      <td>0</td>\n      <td>MUN-SHU  *Haters *Harry Maquire 1 Harry Maguire is the MOTM!</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>387</td>\n      <td>0</td>\n      <td>www\\nAnju jr\\nTODAY\\nHi Chetta\\n01:02 PM\\nAthey\\n01:02 PM\\nenik ariyamayrunnu kuttikk enne stamanenn\\n01:03 PM\\nentayalum kaathirunnath veruthe aayilla\\n01:03 PM\\nenikkum istamanu\\n01:03 PM\\nhttps://www.instagram.com /p/B80ezrXFy-h/?igshid=\\n17ki4rfn78t4k\\n01:03 PM\\nee link onn like cheyyane\\n01:04 PM\\nente friendinte oru competition\\naanu\\n01:04 PM\\nType a message\\n*  \\nAbin A Kottarakkara</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>520</td>\n      <td>0</td>\n      <td>                    \\n\\n \\nbhaai\\n</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>563</td>\n      <td>1</td>\n      <td>       ?   .  </td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>856</td>\n      <td>0</td>\n      <td>\\n     ...\\n* \\n\\n      .\\nTernational</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>545</td>\n      <td>1</td>\n      <td>     *   ..\\n* .   </td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>511</td>\n      <td>0</td>\n      <td>          </td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>252</td>\n      <td>0</td>\n      <td>      \"    </td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>742</td>\n      <td>0</td>\n      <td>\\n    \\n .\\n  \\n  \\n  \\n</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>873</td>\n      <td>0</td>\n      <td>\\n    (Sunny)     ( S.Unny )     ( Sunny )</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>566</td>\n      <td>1</td>\n      <td>             </td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>325</td>\n      <td>0</td>\n      <td>Girls  \\n*\\n     \\nBoys  \\n      </td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>760</td>\n      <td>0</td>\n      <td>07.10.2023\\n'\\n;\\n ,    \\n   \\n-   \\n !\\n  ?</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>27</td>\n      <td>1</td>\n      <td>            .</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>544</td>\n      <td>1</td>\n      <td> *    *(         ) \\n.     \\n</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>297</td>\n      <td>0</td>\n      <td> ..\\n    ..</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>258</td>\n      <td>0</td>\n      <td>       \\n     </td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>63</td>\n      <td>1</td>\n      <td>    \\n*      </td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>857</td>\n      <td>0</td>\n      <td>Unni Mukundan \\n\\nMera BHARAT\\nMy India\\n\\n\\n \\n</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>433</td>\n      <td>1</td>\n      <td>.   </td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>543</td>\n      <td>0</td>\n      <td>    .. .. ...  ..     </td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>794</td>\n      <td>0</td>\n      <td>    \\nfTMM\\nExpectation\\nTMM\\nReality\\nR177\\nA</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>710</td>\n      <td>1</td>\n      <td>          |                            </td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>725</td>\n      <td>1</td>\n      <td>BACK   </td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>76</td>\n      <td>1</td>\n      <td>VS  \\n ..  .  \\n    .</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>390</td>\n      <td>0</td>\n      <td>   \\n \\n</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>939</td>\n      <td>0</td>\n      <td>Sj\\npha\\nS\\n  </td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>45</td>\n      <td>1</td>\n      <td>      *     *    KTM   \\n*    * \\n*     *    Apache...!</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>373</td>\n      <td>0</td>\n      <td>20    \\n\\n@\\n     \\n      </td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>677</td>\n      <td>1</td>\n      <td>          </td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>205</td>\n      <td>0</td>\n      <td>      10  experience \\n   \\n\\nexperience     \\n</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>410</td>\n      <td>1</td>\n      <td>    *          </td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>557</td>\n      <td>1</td>\n      <td>      </td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>33</td>\n      <td>1</td>\n      <td>* ..   ..    . *....       \\n*   ..     ..</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>861</td>\n      <td>0</td>\n      <td>* \\n \\n   , |   \\n\\n    \\n   8  \\n  </td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>35</td>\n      <td>1</td>\n      <td>* .       ..   , \\n  .. *  ..     .. \\n     ..!!</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>800</td>\n      <td>0</td>\n      <td>\\n       \\n\\n*MV \\n \\n*\\n      \\n</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>818</td>\n      <td>0</td>\n      <td>\\n    \\n\\n   ..?? -\\n.\\n   ..?    .  ...\\n   ...\\n   ...\\n</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>190</td>\n      <td>0</td>\n      <td>        2  .  ?</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>215</td>\n      <td>0</td>\n      <td>    2      :   .   \\n ..</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>187</td>\n      <td>0</td>\n      <td>       100  500  1000  250  000</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>316</td>\n      <td>0</td>\n      <td>  \\n </td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>488</td>\n      <td>1</td>\n      <td>          </td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>635</td>\n      <td>1</td>\n      <td>              </td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>559</td>\n      <td>1</td>\n      <td>    </td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>789</td>\n      <td>0</td>\n      <td>   \\n \\n  . \\n*\\n\\nHelp!\\nHelp!\\n \\n      \\n\\n   </td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>11</td>\n      <td>1</td>\n      <td>           </td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>973</td>\n      <td>0</td>\n      <td>    . #neymar #FIFAWorldCupQualifiers i mathrubhumi.com   ;     ,        </td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>634</td>\n      <td>1</td>\n      <td>  ,       ..       </td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>673</td>\n      <td>1</td>\n      <td>                     </td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>342</td>\n      <td>0</td>\n      <td>*\\n ...!\\nTM   \\n. ...!</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>958</td>\n      <td>0</td>\n      <td>            </td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>971</td>\n      <td>0</td>\n      <td>           TMM            TMM      TMM</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>432</td>\n      <td>1</td>\n      <td>VPN Connect         *</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Textual***"
      ],
      "metadata": {
        "id": "Sq2NgtnncaWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TextCNN***"
      ],
      "metadata": {
        "id": "88ErOHpXci1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Set parameters\n",
        "max_words = 10000\n",
        "max_len = 200\n",
        "embedding_dim = 100\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "# Load and preprocess text data\n",
        "def preprocess_text(data, tokenizer=None, fit_tokenizer=False):\n",
        "    if fit_tokenizer:\n",
        "        tokenizer = Tokenizer(num_words=max_words)\n",
        "        tokenizer.fit_on_texts(data['transcriptions'])\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(data['transcriptions'])\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "    return padded_sequences, tokenizer\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv('/kaggle/input/misogyny-meme/Train/train.csv')\n",
        "val_data = pd.read_csv('/kaggle/input/misogyny-meme/Dev/dev.csv')\n",
        "test_data = pd.read_csv('/kaggle/input/misogyny-meme/Test/test.csv')\n",
        "test_labels = pd.read_csv('/kaggle/input/misogyny-meme/Test_Labels/test_with_labels.csv')\n",
        "\n",
        "# Preprocess text data\n",
        "train_sequences, tokenizer = preprocess_text(train_data, fit_tokenizer=True)\n",
        "val_sequences, _ = preprocess_text(val_data, tokenizer)\n",
        "test_sequences, _ = preprocess_text(test_data, tokenizer)\n",
        "\n",
        "# Prepare labels\n",
        "def preprocess_labels(data):\n",
        "    return np.array(data['labels'])\n",
        "\n",
        "train_labels = preprocess_labels(train_data)\n",
        "val_labels = preprocess_labels(val_data)\n",
        "test_labels = preprocess_labels(test_labels)\n",
        "\n",
        "# Build the Text-CNN model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_sequences, train_labels,\n",
        "    validation_data=(val_sequences, val_labels),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_predictions = model.predict(test_sequences)\n",
        "test_predictions = (test_predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "precision = precision_score(test_labels, test_predictions)\n",
        "recall = recall_score(test_labels, test_predictions)\n",
        "f1 = f1_score(test_labels, test_predictions, average='macro')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\" F1-score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T15:58:23.931831Z",
          "iopub.execute_input": "2025-01-28T15:58:23.932221Z",
          "iopub.status.idle": "2025-01-28T15:58:37.991127Z",
          "shell.execute_reply.started": "2025-01-28T15:58:23.932192Z",
          "shell.execute_reply": "2025-01-28T15:58:37.990214Z"
        },
        "id": "BJOEa7i_a2ZX",
        "outputId": "f23ded0a-1be5-4483-a6c1-bae81126a9aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.5339 - loss: 0.6855 - val_accuracy: 0.6062 - val_loss: 0.6701\nEpoch 2/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5705 - loss: 0.6711 - val_accuracy: 0.6062 - val_loss: 0.6672\nEpoch 3/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5982 - loss: 0.6346 - val_accuracy: 0.6125 - val_loss: 0.6578\nEpoch 4/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6522 - loss: 0.5522 - val_accuracy: 0.6313 - val_loss: 0.6154\nEpoch 5/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9214 - loss: 0.3597 - val_accuracy: 0.6875 - val_loss: 0.5621\nEpoch 6/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9909 - loss: 0.1184 - val_accuracy: 0.6812 - val_loss: 0.6937\nEpoch 7/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9995 - loss: 0.0255 - val_accuracy: 0.6687 - val_loss: 0.8196\nEpoch 8/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0101 - val_accuracy: 0.6625 - val_loss: 1.0442\nEpoch 9/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.6750 - val_loss: 0.9375\nEpoch 10/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.6562 - val_loss: 1.0903\nEpoch 11/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.6750 - val_loss: 1.0381\nEpoch 12/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.6750 - val_loss: 1.0881\nEpoch 13/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.6625 - val_loss: 1.1547\nEpoch 14/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.6562 - val_loss: 1.1657\nEpoch 15/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 8.1886e-04 - val_accuracy: 0.6625 - val_loss: 1.2053\nEpoch 16/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 8.8083e-04 - val_accuracy: 0.6562 - val_loss: 1.2478\nEpoch 17/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.8125e-04 - val_accuracy: 0.6500 - val_loss: 1.2886\nEpoch 18/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 6.6292e-04 - val_accuracy: 0.6562 - val_loss: 1.2904\nEpoch 19/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 7.6767e-04 - val_accuracy: 0.6500 - val_loss: 1.3513\nEpoch 20/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.8271e-04 - val_accuracy: 0.6500 - val_loss: 1.3994\nEpoch 21/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.5245e-04 - val_accuracy: 0.6500 - val_loss: 1.4072\nEpoch 22/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.1260e-04 - val_accuracy: 0.6562 - val_loss: 1.4394\nEpoch 23/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.3677e-04 - val_accuracy: 0.6500 - val_loss: 1.4632\nEpoch 24/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.0513e-04 - val_accuracy: 0.6562 - val_loss: 1.4513\nEpoch 25/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.0644e-04 - val_accuracy: 0.6562 - val_loss: 1.4181\nEpoch 26/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.4593e-04 - val_accuracy: 0.6562 - val_loss: 1.4523\nEpoch 27/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.0055e-04 - val_accuracy: 0.6562 - val_loss: 1.4678\nEpoch 28/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.0348e-04 - val_accuracy: 0.6562 - val_loss: 1.4832\nEpoch 29/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.9971e-04 - val_accuracy: 0.6562 - val_loss: 1.4754\nEpoch 30/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.0307e-04 - val_accuracy: 0.6500 - val_loss: 1.4935\nEpoch 31/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.7553e-04 - val_accuracy: 0.6562 - val_loss: 1.5278\nEpoch 32/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.5703e-04 - val_accuracy: 0.6562 - val_loss: 1.5508\nEpoch 33/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.6545e-04 - val_accuracy: 0.6562 - val_loss: 1.5618\nEpoch 34/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.1628e-04 - val_accuracy: 0.6562 - val_loss: 1.5955\nEpoch 35/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.1303e-04 - val_accuracy: 0.6562 - val_loss: 1.5994\nEpoch 36/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.8637e-04 - val_accuracy: 0.6625 - val_loss: 1.5823\nEpoch 37/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.2275e-04 - val_accuracy: 0.6625 - val_loss: 1.6122\nEpoch 38/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.0419e-04 - val_accuracy: 0.6625 - val_loss: 1.6276\nEpoch 39/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.1568e-04 - val_accuracy: 0.6625 - val_loss: 1.6309\nEpoch 40/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.1059e-04 - val_accuracy: 0.6625 - val_loss: 1.6151\nEpoch 41/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 9.0311e-05 - val_accuracy: 0.6625 - val_loss: 1.6391\nEpoch 42/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 7.8608e-05 - val_accuracy: 0.6625 - val_loss: 1.6606\nEpoch 43/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.0431e-04 - val_accuracy: 0.6625 - val_loss: 1.6542\nEpoch 44/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.0451e-04 - val_accuracy: 0.6625 - val_loss: 1.6523\nEpoch 45/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.0579e-04 - val_accuracy: 0.6687 - val_loss: 1.6554\nEpoch 46/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.8343e-05 - val_accuracy: 0.6687 - val_loss: 1.6662\nEpoch 47/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.0605e-04 - val_accuracy: 0.6625 - val_loss: 1.6961\nEpoch 48/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.2949e-04 - val_accuracy: 0.6625 - val_loss: 1.6951\nEpoch 49/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 6.3498e-05 - val_accuracy: 0.6687 - val_loss: 1.6870\nEpoch 50/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 7.0737e-05 - val_accuracy: 0.6562 - val_loss: 1.7032\nEpoch 51/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.9648e-05 - val_accuracy: 0.6562 - val_loss: 1.7114\nEpoch 52/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.8776e-04 - val_accuracy: 0.6625 - val_loss: 1.6603\nEpoch 53/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.5670e-05 - val_accuracy: 0.6625 - val_loss: 1.6537\nEpoch 54/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 8.8737e-05 - val_accuracy: 0.6687 - val_loss: 1.6611\nEpoch 55/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.1810e-05 - val_accuracy: 0.6750 - val_loss: 1.6680\nEpoch 56/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 7.8977e-05 - val_accuracy: 0.6687 - val_loss: 1.6916\nEpoch 57/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.1553e-05 - val_accuracy: 0.6687 - val_loss: 1.6989\nEpoch 58/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.4195e-05 - val_accuracy: 0.6687 - val_loss: 1.7165\nEpoch 59/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 6.4472e-05 - val_accuracy: 0.6750 - val_loss: 1.7162\nEpoch 60/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.8810e-05 - val_accuracy: 0.6750 - val_loss: 1.7283\nEpoch 61/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.9984e-05 - val_accuracy: 0.6750 - val_loss: 1.7270\nEpoch 62/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.0511e-05 - val_accuracy: 0.6687 - val_loss: 1.7492\nEpoch 63/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 6.0136e-05 - val_accuracy: 0.6687 - val_loss: 1.7497\nEpoch 64/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.1982e-05 - val_accuracy: 0.6750 - val_loss: 1.7450\nEpoch 65/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.6717e-05 - val_accuracy: 0.6750 - val_loss: 1.7465\nEpoch 66/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.8953e-05 - val_accuracy: 0.6687 - val_loss: 1.7743\nEpoch 67/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.1386e-05 - val_accuracy: 0.6687 - val_loss: 1.7855\nEpoch 68/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.4636e-05 - val_accuracy: 0.6687 - val_loss: 1.7923\nEpoch 69/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.1287e-05 - val_accuracy: 0.6687 - val_loss: 1.8002\nEpoch 70/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.7266e-05 - val_accuracy: 0.6687 - val_loss: 1.8109\nEpoch 71/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.1069e-05 - val_accuracy: 0.6687 - val_loss: 1.8171\nEpoch 72/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.1344e-05 - val_accuracy: 0.6687 - val_loss: 1.8230\nEpoch 73/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.9792e-05 - val_accuracy: 0.6687 - val_loss: 1.8329\nEpoch 74/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.3992e-05 - val_accuracy: 0.6687 - val_loss: 1.8352\nEpoch 75/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.6546e-05 - val_accuracy: 0.6687 - val_loss: 1.8270\nEpoch 76/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.5929e-05 - val_accuracy: 0.6687 - val_loss: 1.8281\nEpoch 77/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.0262e-05 - val_accuracy: 0.6687 - val_loss: 1.8249\nEpoch 78/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.8918e-05 - val_accuracy: 0.6687 - val_loss: 1.8429\nEpoch 79/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.2770e-05 - val_accuracy: 0.6687 - val_loss: 1.8533\nEpoch 80/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.3952e-05 - val_accuracy: 0.6687 - val_loss: 1.8583\nEpoch 81/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 6.8177e-05 - val_accuracy: 0.6687 - val_loss: 1.8554\nEpoch 82/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.3717e-05 - val_accuracy: 0.6687 - val_loss: 1.8662\nEpoch 83/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.4309e-05 - val_accuracy: 0.6687 - val_loss: 1.8946\nEpoch 84/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.1599e-05 - val_accuracy: 0.6687 - val_loss: 1.8894\nEpoch 85/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.5140e-05 - val_accuracy: 0.6687 - val_loss: 1.8732\nEpoch 86/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.6379e-05 - val_accuracy: 0.6687 - val_loss: 1.8449\nEpoch 87/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 8.3577e-06 - val_accuracy: 0.6687 - val_loss: 1.8492\nEpoch 88/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.1135e-05 - val_accuracy: 0.6687 - val_loss: 1.8584\nEpoch 89/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.6707e-05 - val_accuracy: 0.6687 - val_loss: 1.8577\nEpoch 90/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.2365e-05 - val_accuracy: 0.6687 - val_loss: 1.8625\nEpoch 91/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 9.1582e-06 - val_accuracy: 0.6687 - val_loss: 1.8677\nEpoch 92/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.3209e-05 - val_accuracy: 0.6687 - val_loss: 1.8728\nEpoch 93/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.7972e-05 - val_accuracy: 0.6687 - val_loss: 1.8738\nEpoch 94/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.4516e-05 - val_accuracy: 0.6687 - val_loss: 1.8929\nEpoch 95/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.1494e-05 - val_accuracy: 0.6625 - val_loss: 1.9073\nEpoch 96/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.1785e-05 - val_accuracy: 0.6625 - val_loss: 1.9210\nEpoch 97/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 9.4599e-06 - val_accuracy: 0.6625 - val_loss: 1.9305\nEpoch 98/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.1864e-05 - val_accuracy: 0.6562 - val_loss: 1.9680\nEpoch 99/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.1879e-05 - val_accuracy: 0.6562 - val_loss: 1.9823\nEpoch 100/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.2068e-05 - val_accuracy: 0.6562 - val_loss: 1.9887\n\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\nAccuracy: 0.7100\nPrecision: 0.5943\nRecall: 0.8077\n F1-score: 0.7081\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "***LSTM + CNN***"
      ],
      "metadata": {
        "id": "jiej3nQecsF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, LSTM, Bidirectional\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Set parameters\n",
        "max_words = 10000\n",
        "max_len = 200\n",
        "embedding_dim = 100\n",
        "batch_size = 32\n",
        "epochs = 15\n",
        "\n",
        "# Load and preprocess text data\n",
        "def preprocess_text(data, tokenizer=None, fit_tokenizer=False):\n",
        "    if fit_tokenizer:\n",
        "        tokenizer = Tokenizer(num_words=max_words)\n",
        "        tokenizer.fit_on_texts(data['transcriptions'])\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(data['transcriptions'])\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "    return padded_sequences, tokenizer\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv('/kaggle/input/misogyny-meme/Train/train.csv')\n",
        "val_data = pd.read_csv('/kaggle/input/misogyny-meme/Dev/dev.csv')\n",
        "test_data = pd.read_csv('/kaggle/input/misogyny-meme/Test/test.csv')\n",
        "test_labels = pd.read_csv('/kaggle/input/misogyny-meme/Test_Labels/test_with_labels.csv')\n",
        "\n",
        "# Preprocess text data\n",
        "train_sequences, tokenizer = preprocess_text(train_data, fit_tokenizer=True)\n",
        "val_sequences, _ = preprocess_text(val_data, tokenizer)\n",
        "test_sequences, _ = preprocess_text(test_data, tokenizer)\n",
        "\n",
        "# Prepare labels\n",
        "def preprocess_labels(data):\n",
        "    return np.array(data['labels'])\n",
        "\n",
        "train_labels = preprocess_labels(train_data)\n",
        "val_labels = preprocess_labels(val_data)\n",
        "test_labels = preprocess_labels(test_labels)\n",
        "\n",
        "# Build the LSTM+Text-CNN model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_sequences, train_labels,\n",
        "    validation_data=(val_sequences, val_labels),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_predictions = model.predict(test_sequences)\n",
        "test_predictions = (test_predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "precision = precision_score(test_labels, test_predictions)\n",
        "recall = recall_score(test_labels, test_predictions)\n",
        "micro_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Micro F1-score: {micro_f1:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T16:03:23.952169Z",
          "iopub.execute_input": "2025-01-28T16:03:23.952696Z",
          "iopub.status.idle": "2025-01-28T16:03:34.52045Z",
          "shell.execute_reply.started": "2025-01-28T16:03:23.952652Z",
          "shell.execute_reply": "2025-01-28T16:03:34.519715Z"
        },
        "id": "Yxsc4sq7a2ZY",
        "outputId": "e0122933-5f81-4538-887c-39a0087b7557"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/15\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.5865 - loss: 0.6825 - val_accuracy: 0.6062 - val_loss: 0.6819\nEpoch 2/15\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5985 - loss: 0.6762 - val_accuracy: 0.6062 - val_loss: 0.6644\nEpoch 3/15\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6425 - loss: 0.6138 - val_accuracy: 0.6500 - val_loss: 0.6260\nEpoch 4/15\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9268 - loss: 0.2193 - val_accuracy: 0.7188 - val_loss: 0.7131\nEpoch 5/15\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9861 - loss: 0.0726 - val_accuracy: 0.7375 - val_loss: 1.0110\nEpoch 6/15\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9979 - loss: 0.0216 - val_accuracy: 0.7437 - val_loss: 1.1750\nEpoch 7/15\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0075 - val_accuracy: 0.7375 - val_loss: 1.3025\nEpoch 8/15\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9956 - loss: 0.0067 - val_accuracy: 0.7375 - val_loss: 1.3665\nEpoch 9/15\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9996 - loss: 0.0031 - val_accuracy: 0.7688 - val_loss: 1.4396\nEpoch 10/15\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.7750 - val_loss: 1.4820\nEpoch 11/15\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.7437 - val_loss: 1.4992\nEpoch 12/15\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 3.6879e-04 - val_accuracy: 0.7500 - val_loss: 1.5794\nEpoch 13/15\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9999 - loss: 8.6443e-04 - val_accuracy: 0.7563 - val_loss: 1.6316\nEpoch 14/15\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.7250 - val_loss: 1.5983\nEpoch 15/15\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7250 - val_loss: 1.7592\n\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\nAccuracy: 0.7150\nPrecision: 0.6207\nRecall: 0.6923\nMicro F1-score: 0.7060\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "***VISUAL***"
      ],
      "metadata": {
        "id": "iS014CcJcyGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Vision Transformer***"
      ],
      "metadata": {
        "id": "VBJEHJtbc1IK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "import torch\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# Load the pretrained ViT model and feature extractor from Hugging Face\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "# This function will preprocess the image data for ViT\n",
        "def preprocess_images_vit(images):\n",
        "    # Convert images to PyTorch tensors and normalize using the ViT feature extractor\n",
        "    return feature_extractor(images, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Add a classification head on top of the Vision Transformer\n",
        "# The ViT model will be used as a feature extractor\n",
        "model = models.Sequential([\n",
        "    layers.Lambda(lambda x: preprocess_images_vit(x)),\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # For binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T06:44:51.485601Z",
          "iopub.execute_input": "2025-01-28T06:44:51.48608Z",
          "iopub.status.idle": "2025-01-28T06:44:54.241234Z",
          "shell.execute_reply.started": "2025-01-28T06:44:51.48604Z",
          "shell.execute_reply": "2025-01-28T06:44:54.240556Z"
        },
        "colab": {
          "referenced_widgets": [
            "fd995bdf9c4342e7bd63cd13665bb03b",
            "8051964dad5743babdc956b416515bac",
            "c96fbcb2e851422a9047f74a391fd0bb"
          ]
        },
        "id": "paO5HLGwa2ZZ",
        "outputId": "fadbba78-70b6-4c41-91bf-19b5a07f889c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd995bdf9c4342e7bd63cd13665bb03b"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8051964dad5743babdc956b416515bac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c96fbcb2e851422a9047f74a391fd0bb"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from transformers import AdamW\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set image parameters\n",
        "img_width = 224\n",
        "img_height = 224\n",
        "batch_size = 16  # Adjust as needed\n",
        "epochs = 15  # Number of epochs\n",
        "learning_rate = 1e-5  # Learning rate\n",
        "\n",
        "# Load the pretrained ViT model and feature extractor from Hugging Face\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "# Freeze the pre-trained ViT layers\n",
        "for param in vit_model.parameters():\n",
        "    param.requires_grad = False  # Freeze layers to retain pre-trained weights\n",
        "\n",
        "# Add a custom classification head on top of ViT\n",
        "vit_model.classifier = nn.Sequential(\n",
        "    nn.Linear(vit_model.config.hidden_size, 512),  # Add fully connected layer\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 1),  # Output layer for binary classification (0 or 1)\n",
        "    nn.Sigmoid()  # Sigmoid activation for binary classification\n",
        ")\n",
        "\n",
        "# Ensure the model is on the correct device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit_model.to(device)\n",
        "\n",
        "# Create a custom dataset class to load images\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, feature_extractor):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.feature_extractor = feature_extractor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        img = load_img(image_path, target_size=(img_width, img_height))\n",
        "        img = img_to_array(img)\n",
        "        img = self.feature_extractor(images=img, return_tensors=\"pt\")  # Preprocess using ViT\n",
        "\n",
        "        return img['pixel_values'].squeeze(0), torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# Prepare the dataset and DataLoader\n",
        "image_dir_test = '/kaggle/input/misogyny-meme/Test'  # Adjust as per your data path\n",
        "test_data = pd.read_csv('/kaggle/input/misogyny-meme/Test/test.csv')  # Path to your test CSV\n",
        "test_image_paths = [f\"{image_dir_test}/{image_id}.jpg\" for image_id in test_data['image_id']]\n",
        "test_labels = pd.read_csv('/kaggle/input/misogyny-meme/Test_Labels/test_with_labels.csv')  # Your labels file\n",
        "test_actual_labels = test_labels['labels'].values\n",
        "\n",
        "# Convert the paths and labels to a dataset\n",
        "dataset = ImageDataset(test_image_paths, test_actual_labels, feature_extractor)\n",
        "\n",
        "# Split dataset into train and validation (80% train, 20% validation)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader for both training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Set up optimizer and loss function\n",
        "optimizer = AdamW(vit_model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.BCELoss()  # Binary Cross-Entropy for binary classification\n",
        "\n",
        "# Fine-tune the model (forward pass, loss computation, backward pass)\n",
        "def train(model, dataloader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for batch in tqdm(dataloader):\n",
        "        images, labels = batch\n",
        "\n",
        "        # Ensure both images and labels are on the same device as the model\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Extract logits from the outputs and calculate loss\n",
        "        logits = outputs.logits  # Access the logits from ImageClassifierOutput\n",
        "        loss = loss_fn(logits.squeeze(), labels)  # Squeeze to match the labels shape\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predictions = (logits > 0.5).squeeze().cpu().numpy()  # Apply threshold of 0.5 for binary classification\n",
        "        correct_predictions += (predictions == labels.cpu().numpy()).sum()\n",
        "        total_predictions += len(labels)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Validation loop\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            images, labels = batch\n",
        "\n",
        "            # Ensure both images and labels are on the same device as the model\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            logits = outputs.logits  # Access the logits\n",
        "\n",
        "            # Apply threshold for binary classification\n",
        "            predictions = (logits > 0.5).squeeze().cpu().numpy()\n",
        "            correct_predictions += (predictions == labels.cpu().numpy()).sum()\n",
        "            total_predictions += len(labels)\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return accuracy\n",
        "\n",
        "# Training and evaluation loop\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    avg_loss, train_accuracy = train(vit_model, train_loader, optimizer, loss_fn)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_loss:.4f} - Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    val_accuracy = evaluate(vit_model, val_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Now, make predictions on the test data\n",
        "def predict_vit(model, image_paths):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for image_path in image_paths:\n",
        "            img = preprocess_image(image_path)  # Preprocess image\n",
        "            img = img.to(device)  # Ensure the image is on the same device as the model\n",
        "            outputs = model(**img)  # Pass through ViT model\n",
        "            logits = outputs.logits  # Get the logits\n",
        "            pred = (logits > 0.5).squeeze().cpu().numpy()  # Apply threshold\n",
        "            predictions.append(pred)\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Get predictions on the test data\n",
        "test_predictions = predict_vit(vit_model, test_image_paths)\n",
        "\n",
        "# Evaluate using the provided metrics\n",
        "accuracy = accuracy_score(test_actual_labels, test_predictions)\n",
        "precision = precision_score(test_actual_labels, test_predictions)\n",
        "recall = recall_score(test_actual_labels, test_predictions)\n",
        "macro_f1 = f1_score(test_actual_labels, test_predictions, average='macro')\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test Macro F1-Score: {macro_f1:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T06:56:38.590821Z",
          "iopub.execute_input": "2025-01-28T06:56:38.591268Z",
          "iopub.status.idle": "2025-01-28T06:58:03.336381Z",
          "shell.execute_reply.started": "2025-01-28T06:56:38.591228Z",
          "shell.execute_reply": "2025-01-28T06:58:03.335449Z"
        },
        "id": "KV9kuAhwa2ZZ",
        "outputId": "818612b3-5b7c-451e-dd07-4e689b05676e"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n100%|| 10/10 [00:04<00:00,  2.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/15 - Train Loss: 0.6960 - Train Accuracy: 0.4688\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:01<00:00,  2.94it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/15 - Validation Accuracy: 0.4250\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 10/10 [00:04<00:00,  2.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2/15 - Train Loss: 0.6905 - Train Accuracy: 0.5625\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:00<00:00,  3.03it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2/15 - Validation Accuracy: 0.5000\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 10/10 [00:04<00:00,  2.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3/15 - Train Loss: 0.6854 - Train Accuracy: 0.6625\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:01<00:00,  2.96it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3/15 - Validation Accuracy: 0.6000\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 10/10 [00:04<00:00,  2.38it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4/15 - Train Loss: 0.6802 - Train Accuracy: 0.7000\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:01<00:00,  2.93it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4/15 - Validation Accuracy: 0.7000\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 10/10 [00:04<00:00,  2.44it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5/15 - Train Loss: 0.6753 - Train Accuracy: 0.7937\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:01<00:00,  2.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5/15 - Validation Accuracy: 0.7750\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 10/10 [00:04<00:00,  2.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6/15 - Train Loss: 0.6701 - Train Accuracy: 0.8063\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:01<00:00,  2.95it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6/15 - Validation Accuracy: 0.7750\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 10/10 [00:04<00:00,  2.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7/15 - Train Loss: 0.6654 - Train Accuracy: 0.8125\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:01<00:00,  2.91it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7/15 - Validation Accuracy: 0.7250\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 10/10 [00:04<00:00,  2.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8/15 - Train Loss: 0.6608 - Train Accuracy: 0.8313\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:01<00:00,  2.90it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8/15 - Validation Accuracy: 0.7500\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 10/10 [00:04<00:00,  2.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 9/15 - Train Loss: 0.6563 - Train Accuracy: 0.8438\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:01<00:00,  2.93it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 9/15 - Validation Accuracy: 0.7500\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 10/10 [00:04<00:00,  2.36it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 10/15 - Train Loss: 0.6514 - Train Accuracy: 0.8438\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:01<00:00,  2.91it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 10/15 - Validation Accuracy: 0.7500\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 10/10 [00:04<00:00,  2.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 11/15 - Train Loss: 0.6468 - Train Accuracy: 0.8500\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:01<00:00,  2.81it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 11/15 - Validation Accuracy: 0.7750\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 10/10 [00:04<00:00,  2.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 12/15 - Train Loss: 0.6423 - Train Accuracy: 0.8500\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:01<00:00,  2.89it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 12/15 - Validation Accuracy: 0.7750\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 10/10 [00:04<00:00,  2.34it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 13/15 - Train Loss: 0.6382 - Train Accuracy: 0.8438\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:01<00:00,  2.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 13/15 - Validation Accuracy: 0.7750\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 10/10 [00:04<00:00,  2.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 14/15 - Train Loss: 0.6336 - Train Accuracy: 0.8438\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:01<00:00,  2.92it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 14/15 - Validation Accuracy: 0.7500\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 10/10 [00:04<00:00,  2.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 15/15 - Train Loss: 0.6292 - Train Accuracy: 0.8438\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 3/3 [00:01<00:00,  2.80it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 15/15 - Validation Accuracy: 0.7500\nTest Accuracy: 0.8250\nTest Precision: 0.9778\nTest Recall: 0.5641\nTest Macro F1-Score: 0.7945\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from transformers import AdamW\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "cm = confusion_matrix(test_actual_labels, test_predictions)\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Misogynistic', 'Misogynistic'])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T07:06:48.549591Z",
          "iopub.execute_input": "2025-01-28T07:06:48.549941Z",
          "iopub.status.idle": "2025-01-28T07:06:48.818827Z",
          "shell.execute_reply.started": "2025-01-28T07:06:48.549918Z",
          "shell.execute_reply": "2025-01-28T07:06:48.817803Z"
        },
        "id": "WwzlGehVa2Za",
        "outputId": "2afff538-7bf7-47ea-feaa-204cd78ebfb8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 640x480 with 2 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "***VGG16***"
      ],
      "metadata": {
        "id": "wJm4tavFdAxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Define the VGG16 model for image processing\n",
        "def create_vgg16_model(input_shape):\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # Freeze the base model layers\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Add custom layers on top\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(2, activation='softmax')(x)  # Binary classification (0 or 1)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Prepare labels for training and validation\n",
        "def preprocess_labels(labels):\n",
        "    return to_categorical(labels, num_classes=2)\n",
        "\n",
        "# Convert labels\n",
        "train_labels = preprocess_labels(train_data['labels'])\n",
        "val_labels = preprocess_labels(val_data['labels'])\n",
        "\n",
        "# Create and train the model\n",
        "input_shape = (img_width, img_height, 3)\n",
        "vgg16_model = create_vgg16_model(input_shape)\n",
        "\n",
        "# Train the model\n",
        "vgg16_model.fit(\n",
        "    train_images, train_labels,\n",
        "    validation_data=(val_images, val_labels),\n",
        "    batch_size=batch_size,\n",
        "    epochs=100,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model using macro F1-score\n",
        "def evaluate_model_with_macro_f1(model, test_images, test_labels):\n",
        "    predictions = model.predict(test_images)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    actual_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(actual_labels, predicted_labels)\n",
        "    precision = precision_score(actual_labels, predicted_labels, average='macro')\n",
        "    recall = recall_score(actual_labels, predicted_labels, average='macro')\n",
        "    f1 = f1_score(actual_labels, predicted_labels, average='macro')\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(\n",
        "        actual_labels, predicted_labels,\n",
        "        target_names=['Not Misogynistic (0)', 'Misogynistic (1)']\n",
        "    ))\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Prepare test labels\n",
        "test_labels = preprocess_labels(test_data_labels['labels'])\n",
        "\n",
        "# Evaluate the model on test data\n",
        "accuracy, precision, recall, f1 = evaluate_model_with_macro_f1(vgg16_model, test_images, test_labels)\n",
        "\n",
        "print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"Macro Precision: {:.2f}\".format(precision))\n",
        "print(\"Macro Recall: {:.2f}\".format(recall))\n",
        "print(\"Macro F1-Score: {:.2f}\".format(f1))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T05:40:10.394937Z",
          "iopub.execute_input": "2025-01-28T05:40:10.39526Z",
          "iopub.status.idle": "2025-01-28T05:47:15.070195Z",
          "shell.execute_reply.started": "2025-01-28T05:40:10.395232Z",
          "shell.execute_reply": "2025-01-28T05:47:15.069508Z"
        },
        "id": "S-QNvM5Ra2Za",
        "outputId": "8b02a288-9d30-4dc6-d1df-a3ebc78876ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 253ms/step - accuracy: 0.5619 - loss: 0.7403 - val_accuracy: 0.6062 - val_loss: 0.6644\nEpoch 2/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 207ms/step - accuracy: 0.5610 - loss: 0.7498 - val_accuracy: 0.6062 - val_loss: 0.6621\nEpoch 3/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 211ms/step - accuracy: 0.5617 - loss: 0.7584 - val_accuracy: 0.6062 - val_loss: 0.6598\nEpoch 4/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 217ms/step - accuracy: 0.5446 - loss: 0.7442 - val_accuracy: 0.6062 - val_loss: 0.6576\nEpoch 5/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 222ms/step - accuracy: 0.5405 - loss: 0.7630 - val_accuracy: 0.6062 - val_loss: 0.6557\nEpoch 6/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 229ms/step - accuracy: 0.5742 - loss: 0.7470 - val_accuracy: 0.6062 - val_loss: 0.6537\nEpoch 7/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 231ms/step - accuracy: 0.5621 - loss: 0.7450 - val_accuracy: 0.6187 - val_loss: 0.6519\nEpoch 8/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 232ms/step - accuracy: 0.5566 - loss: 0.7241 - val_accuracy: 0.6187 - val_loss: 0.6503\nEpoch 9/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 229ms/step - accuracy: 0.5435 - loss: 0.7639 - val_accuracy: 0.6438 - val_loss: 0.6484\nEpoch 10/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 223ms/step - accuracy: 0.5904 - loss: 0.7410 - val_accuracy: 0.6438 - val_loss: 0.6468\nEpoch 11/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 217ms/step - accuracy: 0.5542 - loss: 0.7474 - val_accuracy: 0.6438 - val_loss: 0.6450\nEpoch 12/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 213ms/step - accuracy: 0.5630 - loss: 0.7324 - val_accuracy: 0.6438 - val_loss: 0.6434\nEpoch 13/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 211ms/step - accuracy: 0.6006 - loss: 0.7169 - val_accuracy: 0.6500 - val_loss: 0.6417\nEpoch 14/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 207ms/step - accuracy: 0.5407 - loss: 0.7279 - val_accuracy: 0.6562 - val_loss: 0.6401\nEpoch 15/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 205ms/step - accuracy: 0.5929 - loss: 0.7006 - val_accuracy: 0.6562 - val_loss: 0.6385\nEpoch 16/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 205ms/step - accuracy: 0.5415 - loss: 0.7493 - val_accuracy: 0.6687 - val_loss: 0.6369\nEpoch 17/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 204ms/step - accuracy: 0.5738 - loss: 0.7303 - val_accuracy: 0.6625 - val_loss: 0.6355\nEpoch 18/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 203ms/step - accuracy: 0.5847 - loss: 0.6955 - val_accuracy: 0.6625 - val_loss: 0.6341\nEpoch 19/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 204ms/step - accuracy: 0.5647 - loss: 0.7150 - val_accuracy: 0.6625 - val_loss: 0.6326\nEpoch 20/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 205ms/step - accuracy: 0.5276 - loss: 0.7447 - val_accuracy: 0.6687 - val_loss: 0.6313\nEpoch 21/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 205ms/step - accuracy: 0.6146 - loss: 0.6798 - val_accuracy: 0.6687 - val_loss: 0.6297\nEpoch 22/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 206ms/step - accuracy: 0.5586 - loss: 0.7286 - val_accuracy: 0.6750 - val_loss: 0.6285\nEpoch 23/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.5923 - loss: 0.7153 - val_accuracy: 0.6750 - val_loss: 0.6271\nEpoch 24/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.5894 - loss: 0.6770 - val_accuracy: 0.6812 - val_loss: 0.6257\nEpoch 25/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 212ms/step - accuracy: 0.5636 - loss: 0.7004 - val_accuracy: 0.6875 - val_loss: 0.6244\nEpoch 26/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 213ms/step - accuracy: 0.5718 - loss: 0.6973 - val_accuracy: 0.6875 - val_loss: 0.6230\nEpoch 27/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 214ms/step - accuracy: 0.5583 - loss: 0.7269 - val_accuracy: 0.6875 - val_loss: 0.6218\nEpoch 28/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 214ms/step - accuracy: 0.5893 - loss: 0.6945 - val_accuracy: 0.6938 - val_loss: 0.6206\nEpoch 29/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 214ms/step - accuracy: 0.5745 - loss: 0.7037 - val_accuracy: 0.6938 - val_loss: 0.6194\nEpoch 30/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 214ms/step - accuracy: 0.5905 - loss: 0.6928 - val_accuracy: 0.7063 - val_loss: 0.6181\nEpoch 31/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 212ms/step - accuracy: 0.5540 - loss: 0.7437 - val_accuracy: 0.7125 - val_loss: 0.6168\nEpoch 32/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 211ms/step - accuracy: 0.5577 - loss: 0.7211 - val_accuracy: 0.7125 - val_loss: 0.6156\nEpoch 33/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.6041 - loss: 0.6984 - val_accuracy: 0.7125 - val_loss: 0.6143\nEpoch 34/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.5986 - loss: 0.6880 - val_accuracy: 0.7188 - val_loss: 0.6132\nEpoch 35/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.5933 - loss: 0.7073 - val_accuracy: 0.7188 - val_loss: 0.6121\nEpoch 36/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6452 - loss: 0.6558 - val_accuracy: 0.7188 - val_loss: 0.6109\nEpoch 37/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6002 - loss: 0.6848 - val_accuracy: 0.7063 - val_loss: 0.6098\nEpoch 38/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6113 - loss: 0.7162 - val_accuracy: 0.7125 - val_loss: 0.6090\nEpoch 39/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6015 - loss: 0.6473 - val_accuracy: 0.7125 - val_loss: 0.6078\nEpoch 40/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6400 - loss: 0.6627 - val_accuracy: 0.7125 - val_loss: 0.6067\nEpoch 41/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6064 - loss: 0.6706 - val_accuracy: 0.7125 - val_loss: 0.6056\nEpoch 42/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6098 - loss: 0.6773 - val_accuracy: 0.7125 - val_loss: 0.6045\nEpoch 43/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.5761 - loss: 0.6828 - val_accuracy: 0.7375 - val_loss: 0.6035\nEpoch 44/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6015 - loss: 0.6770 - val_accuracy: 0.7375 - val_loss: 0.6024\nEpoch 45/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.5879 - loss: 0.7091 - val_accuracy: 0.7437 - val_loss: 0.6013\nEpoch 46/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6301 - loss: 0.6496 - val_accuracy: 0.7375 - val_loss: 0.6004\nEpoch 47/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.6315 - loss: 0.6642 - val_accuracy: 0.7437 - val_loss: 0.5995\nEpoch 48/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6196 - loss: 0.6754 - val_accuracy: 0.7375 - val_loss: 0.5984\nEpoch 49/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6371 - loss: 0.6645 - val_accuracy: 0.7437 - val_loss: 0.5974\nEpoch 50/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.6172 - loss: 0.6658 - val_accuracy: 0.7437 - val_loss: 0.5964\nEpoch 51/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.6551 - loss: 0.6327 - val_accuracy: 0.7437 - val_loss: 0.5955\nEpoch 52/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.6250 - loss: 0.6465 - val_accuracy: 0.7437 - val_loss: 0.5944\nEpoch 53/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.6388 - loss: 0.6221 - val_accuracy: 0.7437 - val_loss: 0.5934\nEpoch 54/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.6386 - loss: 0.6539 - val_accuracy: 0.7437 - val_loss: 0.5927\nEpoch 55/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.5964 - loss: 0.6458 - val_accuracy: 0.7437 - val_loss: 0.5916\nEpoch 56/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6233 - loss: 0.6669 - val_accuracy: 0.7500 - val_loss: 0.5907\nEpoch 57/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.5891 - loss: 0.6768 - val_accuracy: 0.7500 - val_loss: 0.5896\nEpoch 58/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6334 - loss: 0.6447 - val_accuracy: 0.7500 - val_loss: 0.5886\nEpoch 59/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6209 - loss: 0.6421 - val_accuracy: 0.7500 - val_loss: 0.5877\nEpoch 60/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6315 - loss: 0.6529 - val_accuracy: 0.7500 - val_loss: 0.5869\nEpoch 61/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.5911 - loss: 0.6494 - val_accuracy: 0.7500 - val_loss: 0.5861\nEpoch 62/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6445 - loss: 0.6335 - val_accuracy: 0.7500 - val_loss: 0.5851\nEpoch 63/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6207 - loss: 0.6671 - val_accuracy: 0.7500 - val_loss: 0.5844\nEpoch 64/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6223 - loss: 0.6607 - val_accuracy: 0.7500 - val_loss: 0.5835\nEpoch 65/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6840 - loss: 0.6104 - val_accuracy: 0.7500 - val_loss: 0.5827\nEpoch 66/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6681 - loss: 0.6408 - val_accuracy: 0.7625 - val_loss: 0.5821\nEpoch 67/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6503 - loss: 0.6482 - val_accuracy: 0.7625 - val_loss: 0.5814\nEpoch 68/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.6248 - loss: 0.6513 - val_accuracy: 0.7563 - val_loss: 0.5808\nEpoch 69/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 211ms/step - accuracy: 0.6252 - loss: 0.6655 - val_accuracy: 0.7625 - val_loss: 0.5803\nEpoch 70/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 211ms/step - accuracy: 0.6459 - loss: 0.6387 - val_accuracy: 0.7625 - val_loss: 0.5796\nEpoch 71/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 212ms/step - accuracy: 0.6283 - loss: 0.6464 - val_accuracy: 0.7625 - val_loss: 0.5788\nEpoch 72/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 212ms/step - accuracy: 0.6624 - loss: 0.6109 - val_accuracy: 0.7625 - val_loss: 0.5778\nEpoch 73/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 212ms/step - accuracy: 0.6589 - loss: 0.6202 - val_accuracy: 0.7625 - val_loss: 0.5772\nEpoch 74/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 212ms/step - accuracy: 0.6615 - loss: 0.6223 - val_accuracy: 0.7625 - val_loss: 0.5764\nEpoch 75/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 212ms/step - accuracy: 0.6557 - loss: 0.6305 - val_accuracy: 0.7625 - val_loss: 0.5758\nEpoch 76/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 212ms/step - accuracy: 0.6439 - loss: 0.6277 - val_accuracy: 0.7625 - val_loss: 0.5750\nEpoch 77/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 211ms/step - accuracy: 0.7051 - loss: 0.5878 - val_accuracy: 0.7625 - val_loss: 0.5741\nEpoch 78/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 211ms/step - accuracy: 0.6120 - loss: 0.6729 - val_accuracy: 0.7625 - val_loss: 0.5736\nEpoch 79/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 211ms/step - accuracy: 0.6235 - loss: 0.6455 - val_accuracy: 0.7625 - val_loss: 0.5727\nEpoch 80/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.6868 - loss: 0.5988 - val_accuracy: 0.7625 - val_loss: 0.5719\nEpoch 81/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.6669 - loss: 0.6303 - val_accuracy: 0.7625 - val_loss: 0.5713\nEpoch 82/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6658 - loss: 0.6239 - val_accuracy: 0.7625 - val_loss: 0.5704\nEpoch 83/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.6456 - loss: 0.6310 - val_accuracy: 0.7688 - val_loss: 0.5699\nEpoch 84/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6710 - loss: 0.6248 - val_accuracy: 0.7688 - val_loss: 0.5690\nEpoch 85/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6800 - loss: 0.6146 - val_accuracy: 0.7625 - val_loss: 0.5683\nEpoch 86/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6611 - loss: 0.6292 - val_accuracy: 0.7688 - val_loss: 0.5679\nEpoch 87/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.7093 - loss: 0.6044 - val_accuracy: 0.7688 - val_loss: 0.5671\nEpoch 88/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6899 - loss: 0.5865 - val_accuracy: 0.7688 - val_loss: 0.5663\nEpoch 89/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6888 - loss: 0.6090 - val_accuracy: 0.7688 - val_loss: 0.5658\nEpoch 90/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6933 - loss: 0.6207 - val_accuracy: 0.7688 - val_loss: 0.5649\nEpoch 91/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6840 - loss: 0.6024 - val_accuracy: 0.7688 - val_loss: 0.5644\nEpoch 92/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.7005 - loss: 0.6056 - val_accuracy: 0.7688 - val_loss: 0.5639\nEpoch 93/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.6755 - loss: 0.6196 - val_accuracy: 0.7688 - val_loss: 0.5634\nEpoch 94/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6872 - loss: 0.6014 - val_accuracy: 0.7625 - val_loss: 0.5631\nEpoch 95/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6468 - loss: 0.6345 - val_accuracy: 0.7688 - val_loss: 0.5625\nEpoch 96/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.7115 - loss: 0.6111 - val_accuracy: 0.7688 - val_loss: 0.5618\nEpoch 97/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.6785 - loss: 0.6068 - val_accuracy: 0.7688 - val_loss: 0.5616\nEpoch 98/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.6689 - loss: 0.5969 - val_accuracy: 0.7688 - val_loss: 0.5611\nEpoch 99/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.6754 - loss: 0.6008 - val_accuracy: 0.7688 - val_loss: 0.5604\nEpoch 100/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.6554 - loss: 0.6236 - val_accuracy: 0.7688 - val_loss: 0.5601\n\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 196ms/step\nClassification Report:\n                      precision    recall  f1-score   support\n\nNot Misogynistic (0)       0.71      0.95      0.81       122\n    Misogynistic (1)       0.84      0.40      0.54        78\n\n            accuracy                           0.73       200\n           macro avg       0.77      0.67      0.68       200\n        weighted avg       0.76      0.73      0.71       200\n\nTest Accuracy: 73.50%\nMacro Precision: 0.77\nMacro Recall: 0.67\nMacro F1-Score: 0.68\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "***CNN***"
      ],
      "metadata": {
        "id": "Mn1Z3fj4dL7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the CNN model for image processing\n",
        "def create_cnn_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional layers\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # Fully connected layers\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(2, activation='softmax')(x)  # Binary classification (0 or 1)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=5e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Prepare labels for training and validation\n",
        "def preprocess_labels(labels):\n",
        "    return to_categorical(labels, num_classes=2)\n",
        "\n",
        "train_labels = preprocess_labels(train_data['labels'])\n",
        "val_labels = preprocess_labels(val_data['labels'])\n",
        "\n",
        "# Create and train the model\n",
        "input_shape = (img_width, img_height, 3)\n",
        "cnn_model = create_cnn_model(input_shape)\n",
        "\n",
        "cnn_model.fit(\n",
        "    train_images, train_labels,\n",
        "    validation_data=(val_images, val_labels),\n",
        "    batch_size=batch_size,\n",
        "    epochs=100,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Make predictions on the test set\n",
        "def evaluate_model(model, test_images, test_labels):\n",
        "    predictions = model.predict(test_images)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    actual_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(actual_labels, predicted_labels)\n",
        "    precision = precision_score(actual_labels, predicted_labels)\n",
        "    recall = recall_score(actual_labels, predicted_labels)\n",
        "    f1 = f1_score(actual_labels, predicted_labels)\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Prepare test labels\n",
        "test_labels = preprocess_labels(test_data_labels['labels'])\n",
        "\n",
        "# Evaluate the model on test data\n",
        "accuracy, precision, recall, f1 = evaluate_model(cnn_model, test_images, test_labels)\n",
        "\n",
        "print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"Test Precision: {:.2f}\".format(precision))\n",
        "print(\"Test Recall: {:.2f}\".format(recall))\n",
        "print(\"Test F1-Score: {:.2f}\".format(f1))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T05:47:52.947592Z",
          "iopub.execute_input": "2025-01-28T05:47:52.947924Z",
          "iopub.status.idle": "2025-01-28T05:50:19.725873Z",
          "shell.execute_reply.started": "2025-01-28T05:47:52.947894Z",
          "shell.execute_reply": "2025-01-28T05:50:19.725133Z"
        },
        "id": "vC9L9HJWa2Za",
        "outputId": "dfa3432a-f70d-499f-c55b-fc07b6aca7e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - accuracy: 0.5411 - loss: 0.6868 - val_accuracy: 0.6062 - val_loss: 0.6570\nEpoch 2/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.5962 - loss: 0.6481 - val_accuracy: 0.6062 - val_loss: 0.6399\nEpoch 3/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.6293 - loss: 0.6321 - val_accuracy: 0.7063 - val_loss: 0.6060\nEpoch 4/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.7094 - loss: 0.5944 - val_accuracy: 0.7250 - val_loss: 0.5708\nEpoch 5/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.7520 - loss: 0.5758 - val_accuracy: 0.7375 - val_loss: 0.5511\nEpoch 6/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.7668 - loss: 0.5256 - val_accuracy: 0.7125 - val_loss: 0.5644\nEpoch 7/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.7334 - loss: 0.5413 - val_accuracy: 0.7625 - val_loss: 0.5299\nEpoch 8/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7329 - loss: 0.5374 - val_accuracy: 0.7688 - val_loss: 0.5265\nEpoch 9/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7138 - loss: 0.5457 - val_accuracy: 0.7500 - val_loss: 0.5254\nEpoch 10/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7655 - loss: 0.4810 - val_accuracy: 0.7563 - val_loss: 0.5239\nEpoch 11/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.7535 - loss: 0.4871 - val_accuracy: 0.7375 - val_loss: 0.5371\nEpoch 12/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.7791 - loss: 0.4821 - val_accuracy: 0.7437 - val_loss: 0.5239\nEpoch 13/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.7406 - loss: 0.4978 - val_accuracy: 0.7625 - val_loss: 0.5180\nEpoch 14/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.7754 - loss: 0.4509 - val_accuracy: 0.7625 - val_loss: 0.5222\nEpoch 15/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.7983 - loss: 0.4186 - val_accuracy: 0.7750 - val_loss: 0.5420\nEpoch 16/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.8320 - loss: 0.3936 - val_accuracy: 0.7437 - val_loss: 0.5346\nEpoch 17/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.7905 - loss: 0.4097 - val_accuracy: 0.7188 - val_loss: 0.5522\nEpoch 18/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8033 - loss: 0.4071 - val_accuracy: 0.7563 - val_loss: 0.5341\nEpoch 19/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8306 - loss: 0.3607 - val_accuracy: 0.7563 - val_loss: 0.5328\nEpoch 20/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8062 - loss: 0.3741 - val_accuracy: 0.7563 - val_loss: 0.5658\nEpoch 21/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.8588 - loss: 0.3505 - val_accuracy: 0.7625 - val_loss: 0.5586\nEpoch 22/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8540 - loss: 0.3283 - val_accuracy: 0.6812 - val_loss: 0.5976\nEpoch 23/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8431 - loss: 0.3337 - val_accuracy: 0.7375 - val_loss: 0.5551\nEpoch 24/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.8649 - loss: 0.3111 - val_accuracy: 0.7500 - val_loss: 0.5691\nEpoch 25/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8943 - loss: 0.2624 - val_accuracy: 0.7437 - val_loss: 0.5559\nEpoch 26/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9255 - loss: 0.2216 - val_accuracy: 0.7625 - val_loss: 0.5774\nEpoch 27/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9353 - loss: 0.1992 - val_accuracy: 0.7437 - val_loss: 0.5950\nEpoch 28/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9261 - loss: 0.2072 - val_accuracy: 0.7500 - val_loss: 0.6030\nEpoch 29/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9344 - loss: 0.1872 - val_accuracy: 0.7250 - val_loss: 0.6296\nEpoch 30/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9440 - loss: 0.1519 - val_accuracy: 0.7250 - val_loss: 0.6329\nEpoch 31/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9539 - loss: 0.1278 - val_accuracy: 0.6750 - val_loss: 0.7099\nEpoch 32/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9427 - loss: 0.1584 - val_accuracy: 0.7750 - val_loss: 0.7324\nEpoch 33/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9690 - loss: 0.1267 - val_accuracy: 0.7312 - val_loss: 0.7030\nEpoch 34/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9706 - loss: 0.1018 - val_accuracy: 0.7688 - val_loss: 0.7133\nEpoch 35/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9682 - loss: 0.1053 - val_accuracy: 0.7500 - val_loss: 0.7113\nEpoch 36/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9737 - loss: 0.0937 - val_accuracy: 0.7500 - val_loss: 0.7595\nEpoch 37/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9839 - loss: 0.0712 - val_accuracy: 0.7500 - val_loss: 0.8436\nEpoch 38/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9842 - loss: 0.0660 - val_accuracy: 0.7437 - val_loss: 0.7776\nEpoch 39/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9872 - loss: 0.0641 - val_accuracy: 0.7375 - val_loss: 0.8490\nEpoch 40/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9908 - loss: 0.0483 - val_accuracy: 0.7063 - val_loss: 0.8604\nEpoch 41/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9956 - loss: 0.0432 - val_accuracy: 0.7437 - val_loss: 0.8515\nEpoch 42/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9986 - loss: 0.0399 - val_accuracy: 0.7375 - val_loss: 0.9172\nEpoch 43/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9988 - loss: 0.0263 - val_accuracy: 0.6875 - val_loss: 0.9622\nEpoch 44/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9739 - loss: 0.0717 - val_accuracy: 0.7500 - val_loss: 0.8995\nEpoch 45/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9855 - loss: 0.0477 - val_accuracy: 0.7437 - val_loss: 0.9974\nEpoch 46/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9934 - loss: 0.0389 - val_accuracy: 0.7625 - val_loss: 0.8709\nEpoch 47/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9935 - loss: 0.0354 - val_accuracy: 0.7437 - val_loss: 0.9996\nEpoch 48/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9962 - loss: 0.0260 - val_accuracy: 0.7312 - val_loss: 1.0495\nEpoch 49/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9906 - loss: 0.0312 - val_accuracy: 0.7188 - val_loss: 1.0599\nEpoch 50/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9956 - loss: 0.0199 - val_accuracy: 0.7188 - val_loss: 1.0373\nEpoch 51/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9991 - loss: 0.0163 - val_accuracy: 0.7312 - val_loss: 1.0696\nEpoch 52/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9954 - loss: 0.0194 - val_accuracy: 0.7312 - val_loss: 1.1194\nEpoch 53/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9984 - loss: 0.0196 - val_accuracy: 0.7312 - val_loss: 1.1271\nEpoch 54/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9973 - loss: 0.0162 - val_accuracy: 0.7312 - val_loss: 1.0997\nEpoch 55/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9999 - loss: 0.0102 - val_accuracy: 0.7375 - val_loss: 1.1990\nEpoch 56/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9991 - loss: 0.0099 - val_accuracy: 0.7437 - val_loss: 1.1542\nEpoch 57/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9938 - loss: 0.0127 - val_accuracy: 0.7437 - val_loss: 1.1641\nEpoch 58/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9921 - loss: 0.0181 - val_accuracy: 0.7563 - val_loss: 1.1662\nEpoch 59/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9987 - loss: 0.0135 - val_accuracy: 0.7375 - val_loss: 1.3773\nEpoch 60/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9973 - loss: 0.0109 - val_accuracy: 0.7437 - val_loss: 1.3993\nEpoch 61/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9995 - loss: 0.0113 - val_accuracy: 0.7312 - val_loss: 1.2162\nEpoch 62/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9989 - loss: 0.0084 - val_accuracy: 0.7375 - val_loss: 1.2123\nEpoch 63/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9940 - loss: 0.0162 - val_accuracy: 0.7500 - val_loss: 1.2044\nEpoch 64/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9989 - loss: 0.0109 - val_accuracy: 0.7437 - val_loss: 1.3253\nEpoch 65/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9984 - loss: 0.0118 - val_accuracy: 0.7063 - val_loss: 1.2881\nEpoch 66/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.0080 - val_accuracy: 0.7250 - val_loss: 1.2865\nEpoch 67/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.0084 - val_accuracy: 0.7188 - val_loss: 1.3369\nEpoch 68/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.7188 - val_loss: 1.3561\nEpoch 69/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9980 - loss: 0.0108 - val_accuracy: 0.7375 - val_loss: 1.3107\nEpoch 70/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9988 - loss: 0.0071 - val_accuracy: 0.7250 - val_loss: 1.3549\nEpoch 71/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9979 - loss: 0.0102 - val_accuracy: 0.7125 - val_loss: 1.4244\nEpoch 72/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.0039 - val_accuracy: 0.7437 - val_loss: 1.3126\nEpoch 73/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9952 - loss: 0.0074 - val_accuracy: 0.7375 - val_loss: 1.3693\nEpoch 74/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 0.7312 - val_loss: 1.4404\nEpoch 75/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9994 - loss: 0.0038 - val_accuracy: 0.7188 - val_loss: 1.4293\nEpoch 76/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9943 - loss: 0.0075 - val_accuracy: 0.7125 - val_loss: 1.3696\nEpoch 77/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.7188 - val_loss: 1.4913\nEpoch 78/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9998 - loss: 0.0035 - val_accuracy: 0.7125 - val_loss: 1.7006\nEpoch 79/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9998 - loss: 0.0054 - val_accuracy: 0.7312 - val_loss: 1.3467\nEpoch 80/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9995 - loss: 0.0063 - val_accuracy: 0.7312 - val_loss: 1.3446\nEpoch 81/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9966 - loss: 0.0110 - val_accuracy: 0.7500 - val_loss: 1.4144\nEpoch 82/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9995 - loss: 0.0030 - val_accuracy: 0.7250 - val_loss: 1.4524\nEpoch 83/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 0.7312 - val_loss: 1.5020\nEpoch 84/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9973 - loss: 0.0070 - val_accuracy: 0.7188 - val_loss: 1.4133\nEpoch 85/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9968 - loss: 0.0098 - val_accuracy: 0.7250 - val_loss: 1.3933\nEpoch 86/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.7437 - val_loss: 1.4966\nEpoch 87/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9995 - loss: 0.0045 - val_accuracy: 0.7437 - val_loss: 1.4650\nEpoch 88/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9941 - loss: 0.0077 - val_accuracy: 0.6938 - val_loss: 1.4118\nEpoch 89/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9996 - loss: 0.0047 - val_accuracy: 0.7375 - val_loss: 1.4621\nEpoch 90/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9946 - loss: 0.0084 - val_accuracy: 0.7188 - val_loss: 1.4981\nEpoch 91/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.7250 - val_loss: 1.5362\nEpoch 92/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9991 - loss: 0.0095 - val_accuracy: 0.7125 - val_loss: 1.5132\nEpoch 93/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9986 - loss: 0.0092 - val_accuracy: 0.7375 - val_loss: 1.4395\nEpoch 94/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.7188 - val_loss: 1.5635\nEpoch 95/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9972 - loss: 0.0092 - val_accuracy: 0.7250 - val_loss: 1.5358\nEpoch 96/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9981 - loss: 0.0040 - val_accuracy: 0.7125 - val_loss: 1.6797\nEpoch 97/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9965 - loss: 0.0077 - val_accuracy: 0.7375 - val_loss: 1.4754\nEpoch 98/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9967 - loss: 0.0078 - val_accuracy: 0.7312 - val_loss: 1.5732\nEpoch 99/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9961 - loss: 0.0113 - val_accuracy: 0.7312 - val_loss: 1.4268\nEpoch 100/100\n\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9966 - loss: 0.0094 - val_accuracy: 0.7125 - val_loss: 1.6220\n\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step\nTest Accuracy: 66.50%\nTest Precision: 0.58\nTest Recall: 0.51\nTest F1-Score: 0.54\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_labels"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T15:54:05.153237Z",
          "iopub.execute_input": "2025-01-28T15:54:05.153557Z",
          "iopub.status.idle": "2025-01-28T15:54:05.167256Z",
          "shell.execute_reply.started": "2025-01-28T15:54:05.153533Z",
          "shell.execute_reply": "2025-01-28T15:54:05.166483Z"
        },
        "id": "cIhfnSHya2Za",
        "outputId": "6c04209e-3bf5-4830-dc3e-d931b2c64196"
      },
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "     image_id  labels  \\\n0         954       0   \n1         239       0   \n2          61       1   \n3         984       0   \n4         774       0   \n..        ...     ...   \n195       400       0   \n196       421       1   \n197       254       0   \n198       463       1   \n199       425       1   \n\n                                                                                                                                                                                                             transcriptions  \n0                                                                                                                                                                               cilma??....  \n1                                                                                                                           5     . ...      \n2                                                                                                                                                                             ..  \n3                                                                                                                                DIS 2 GOAL AGAINST PERU   \n4    * \\n \\n..  equipment   \\n\\n  !!\\n..     \\n\\n   ..  \\n !!!  !  \n..                                                                                                                                                                                                                      ...  \n195                                                                     ..\\nFOXENTE KIDUVE\\n TMM\\n..     \n196                                                                                         .. *     .  \n197                               \\n        \\n ,    \n198                                                                                                                                                                                                      \n199                                                            *      Instagram       \\n       !  \n\n[200 rows x 3 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>labels</th>\n      <th>transcriptions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>954</td>\n      <td>0</td>\n      <td>    cilma??....</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>239</td>\n      <td>0</td>\n      <td>    5     . ...    </td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>61</td>\n      <td>1</td>\n      <td>     ..</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>984</td>\n      <td>0</td>\n      <td>       DIS 2 GOAL AGAINST PERU </td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>774</td>\n      <td>0</td>\n      <td>* \\n \\n..  equipment   \\n\\n  !!\\n..     \\n\\n   ..  \\n !!!  !</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>400</td>\n      <td>0</td>\n      <td>          ..\\nFOXENTE KIDUVE\\n TMM\\n..   </td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>421</td>\n      <td>1</td>\n      <td>       .. *     .</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>254</td>\n      <td>0</td>\n      <td>           \\n        \\n ,  </td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>463</td>\n      <td>1</td>\n      <td>  </td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>425</td>\n      <td>1</td>\n      <td>*      Instagram       \\n       !</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows  3 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define a function to count total words for each label\n",
        "def count_words_by_label(data, label_column, text_column):\n",
        "    label_word_counts = {}\n",
        "    for label in data[label_column].unique():\n",
        "        # Filter rows by label\n",
        "        label_data = data[data[label_column] == label]\n",
        "        # Concatenate all text and split into words\n",
        "        total_words = label_data[text_column].str.split().apply(len).sum()\n",
        "        label_word_counts[label] = total_words\n",
        "    return label_word_counts\n",
        "\n",
        "# Assuming your CSV files have columns like 'label' and 'text'\n",
        "train_data['word_count'] = train_data['transcriptions'].str.split().apply(len)\n",
        "val_data['word_count'] = val_data['transcriptions'].str.split().apply(len)\n",
        "test_data_labels['word_count'] = test_data_labels['transcriptions'].str.split().apply(len)\n",
        "\n",
        "# Combine all datasets into one (train, validation, and test)\n",
        "all_data = pd.concat([train_data, val_data, test_data_labels])\n",
        "\n",
        "# Count total words for each label in the combined dataset\n",
        "label_word_counts_all = count_words_by_label(all_data, label_column='labels', text_column='transcriptions')\n",
        "\n",
        "# Print the word counts for each label in the entire dataset\n",
        "print(\"Total words for each label in the entire dataset (train + val + test):\")\n",
        "for label, count in label_word_counts_all.items():\n",
        "    print(f\"Label {label}: {count}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-27T17:22:28.182454Z",
          "iopub.execute_input": "2025-01-27T17:22:28.182736Z",
          "iopub.status.idle": "2025-01-27T17:22:28.20099Z",
          "shell.execute_reply.started": "2025-01-27T17:22:28.182716Z",
          "shell.execute_reply": "2025-01-27T17:22:28.200152Z"
        },
        "id": "etOTGX3ra2Za",
        "outputId": "59754c36-c13c-4402-ffec-c7f1cb521237"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total words for each label in the entire dataset (train + val + test):\nLabel 0: 11004\nLabel 1: 6398\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "val_data"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-27T17:21:04.257231Z",
          "iopub.execute_input": "2025-01-27T17:21:04.257649Z",
          "iopub.status.idle": "2025-01-27T17:21:04.268014Z",
          "shell.execute_reply.started": "2025-01-27T17:21:04.257611Z",
          "shell.execute_reply": "2025-01-27T17:21:04.266929Z"
        },
        "id": "dryENvF8a2Zb",
        "outputId": "f2728397-166c-4563-d503-abea3601327f"
      },
      "outputs": [
        {
          "execution_count": 26,
          "output_type": "execute_result",
          "data": {
            "text/plain": "     image_id  labels  \\\n0         685       1   \n1         796       0   \n2         641       1   \n3         701       1   \n4         926       0   \n..        ...     ...   \n155       127       0   \n156       668       1   \n157       113       1   \n158       238       0   \n159       902       0   \n\n                                                                                                                                                                                                                       transcriptions  \n0                                                                                                                                                                                     \n1     \\nED      \\n\\n      .  \\n \\n\\n     .  \n2                                                                                                                 \n3                                                                                                                     \n4                                                                                                   \\n    \\n         \n..                                                                                                                                                                                                                                ...  \n155                                                                                                               \\n      \n156     INPUT -         PROCESS -      OUTPUT =          \n157                                                                                                                    ...    ?      \\n ...  \n158                                                                                                                       #  Present      \"K. Uthara\"      M U. Devi  \n159                                                                                                                          \\nSCT\\n\\n  or  \\n  \n\n[160 rows x 3 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>labels</th>\n      <th>transcriptions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>685</td>\n      <td>1</td>\n      <td>       </td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>796</td>\n      <td>0</td>\n      <td> \\nED      \\n\\n      .  \\n \\n\\n     .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>641</td>\n      <td>1</td>\n      <td>             </td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>701</td>\n      <td>1</td>\n      <td>              </td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>926</td>\n      <td>0</td>\n      <td>   \\n    \\n       </td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>127</td>\n      <td>0</td>\n      <td>            \\n    </td>\n    </tr>\n    <tr>\n      <th>156</th>\n      <td>668</td>\n      <td>1</td>\n      <td>   INPUT -         PROCESS -      OUTPUT =        </td>\n    </tr>\n    <tr>\n      <th>157</th>\n      <td>113</td>\n      <td>1</td>\n      <td>...    ?      \\n ...</td>\n    </tr>\n    <tr>\n      <th>158</th>\n      <td>238</td>\n      <td>0</td>\n      <td>#  Present      \"K. Uthara\"      M U. Devi</td>\n    </tr>\n    <tr>\n      <th>159</th>\n      <td>902</td>\n      <td>0</td>\n      <td>     \\nSCT\\n\\n  or  \\n</td>\n    </tr>\n  </tbody>\n</table>\n<p>160 rows  3 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to evaluate test predictions\n",
        "def evaluate_predictions(true_labels, predicted_labels):\n",
        "    # Accuracy\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    # Precision\n",
        "    precision = precision_score(true_labels, predicted_labels, average='macro')\n",
        "    # Recall\n",
        "    recall = recall_score(true_labels, predicted_labels, average='macro')\n",
        "    # F1-Score\n",
        "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision (Macro): {precision:.4f}\")\n",
        "    print(f\"Recall (Macro): {recall:.4f}\")\n",
        "    print(f\"F1-Score (Macro): {f1:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, predicted_labels, target_names=[\"Not Misogynistic\", \"Misogynistic\"]))\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Calculate confusion matrix and plot it\n",
        "def plot_confusion_matrix(true_labels, predicted_labels, class_names=[\"Non-Misogynistic\", \"Misogynistic\"]):\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.ylabel(\"Actual Labels\")\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.show()\n",
        "\n",
        "# Assuming you have generated `test_predictions` and have `test_data_labels`:\n",
        "true_labels = test_data_labels['labels'].values\n",
        "predicted_labels = test_predictions\n",
        "\n",
        "# Evaluate and plot\n",
        "accuracy, precision, recall, f1 = evaluate_predictions(true_labels, predicted_labels)\n",
        "plot_confusion_matrix(true_labels, predicted_labels)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9xI4BiLGa2Zb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Multimodal***"
      ],
      "metadata": {
        "id": "9EjedJNndhiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Malyalam BERT + ResNet18***"
      ],
      "metadata": {
        "id": "EFM8-VGIdj5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load tokenizer and pretrained BERT model\n",
        "bert_model_name = \"l3cube-pune/malayalam-bert\"  # Correct Malayalam BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = BertModel.from_pretrained(bert_model_name).to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T09:42:19.6581Z",
          "iopub.execute_input": "2025-01-28T09:42:19.658504Z",
          "iopub.status.idle": "2025-01-28T09:42:20.88198Z",
          "shell.execute_reply.started": "2025-01-28T09:42:19.658476Z",
          "shell.execute_reply": "2025-01-28T09:42:20.881293Z"
        },
        "id": "6Gp7MrE7a2Zc",
        "outputId": "3f4fe3a7-8552-4a82-ce71-eee2e31c3b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of BertModel were not initialized from the model checkpoint at l3cube-pune/malayalam-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "***IndicBERT + ResNet18***"
      ],
      "metadata": {
        "id": "aOOqRSU1dtR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, ViTModel, ViTFeatureExtractor\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.metrics import classification_report\n",
        "from torchvision import transforms, models  # Import models here\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load tokenizer and pretrained IndicBERT model\n",
        "bert_model_name = \"ai4bharat/IndicBERTv2-MLM-only\"  # IndicBERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = AutoModel.from_pretrained(bert_model_name).to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:19:56.120012Z",
          "iopub.execute_input": "2025-01-28T19:19:56.120433Z",
          "iopub.status.idle": "2025-01-28T19:20:23.291617Z",
          "shell.execute_reply.started": "2025-01-28T19:19:56.1204Z",
          "shell.execute_reply": "2025-01-28T19:20:23.290826Z"
        },
        "colab": {
          "referenced_widgets": [
            "4332c44dafd54c48ad1f91bcafa2d2d9",
            "b56bcbb7c51a4bfe8ebf1fbf34a3dc89",
            "d7dca6ae9d2744e1bd727516149c603d",
            "d5fb03e0b98a446c938b8fa6e4bac29f",
            "981b67d1aac843569d5b4aad60bde03e"
          ]
        },
        "id": "IfUBw_Nna2Zc",
        "outputId": "a8d66819-f0b7-4862-e76a-6b73553e8894"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4332c44dafd54c48ad1f91bcafa2d2d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/7.75M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b56bcbb7c51a4bfe8ebf1fbf34a3dc89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7dca6ae9d2744e1bd727516149c603d"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/639 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5fb03e0b98a446c938b8fa6e4bac29f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/1.12G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "981b67d1aac843569d5b4aad60bde03e"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class MalayalamMemeDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, max_len, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.max_len = max_len\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        text = row[\"transcriptions\"]\n",
        "        label = torch.tensor(row[\"labels\"], dtype=torch.long)\n",
        "        img_path = os.path.join(self.img_dir, f\"{row['image_id']}.jpg\")\n",
        "\n",
        "        # Process text\n",
        "        encoded_text = tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoded_text[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoded_text[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Process image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return input_ids, attention_mask, image, label\n",
        "\n",
        "# Define transformations for image data\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Dataset paths\n",
        "image_dir = '/kaggle/input/misogyny-meme/Train'\n",
        "image_dir_dev = '/kaggle/input/misogyny-meme/Dev'\n",
        "image_dir_test = '/kaggle/input/misogyny-meme/Test'\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = MalayalamMemeDataset(\n",
        "    csv_file=\"/kaggle/input/misogyny-meme/Train/train.csv\",\n",
        "    img_dir=image_dir,\n",
        "    max_len=128,\n",
        "    transform=image_transforms\n",
        ")\n",
        "\n",
        "val_dataset = MalayalamMemeDataset(\n",
        "    csv_file=\"/kaggle/input/misogyny-meme/Dev/dev.csv\",\n",
        "    img_dir=image_dir_dev,\n",
        "    max_len=128,\n",
        "    transform=image_transforms\n",
        ")\n",
        "\n",
        "test_dataset = MalayalamMemeDataset(\n",
        "    csv_file=\"/kaggle/input/misogyny-meme/Test/test.csv\",\n",
        "    img_dir=image_dir_test,\n",
        "    max_len=128,\n",
        "    transform=image_transforms\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:21:26.730246Z",
          "iopub.execute_input": "2025-01-28T19:21:26.730636Z",
          "iopub.status.idle": "2025-01-28T19:21:26.756041Z",
          "shell.execute_reply.started": "2025-01-28T19:21:26.730609Z",
          "shell.execute_reply": "2025-01-28T19:21:26.755092Z"
        },
        "id": "Qu_BrJPda2Zc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:21:28.616005Z",
          "iopub.execute_input": "2025-01-28T19:21:28.616346Z",
          "iopub.status.idle": "2025-01-28T19:21:28.620517Z",
          "shell.execute_reply.started": "2025-01-28T19:21:28.616318Z",
          "shell.execute_reply": "2025-01-28T19:21:28.619428Z"
        },
        "id": "HDsP0kJXa2Zc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, num_classes=2):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.cnn = models.resnet18(pretrained=True)\n",
        "        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, 128)\n",
        "        self.text_fc = nn.Linear(bert_model.config.hidden_size, 128)\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images):\n",
        "        # Text features\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = self.text_fc(bert_output.last_hidden_state[:, 0, :])\n",
        "\n",
        "        # Image features\n",
        "        image_features = self.cnn(images)\n",
        "\n",
        "        # Combine features\n",
        "        combined_features = torch.cat((text_features, image_features), dim=1)\n",
        "        logits = self.classifier(combined_features)\n",
        "        return logits\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = MultimodalClassifier(bert_model).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:21:30.058678Z",
          "iopub.execute_input": "2025-01-28T19:21:30.059016Z",
          "iopub.status.idle": "2025-01-28T19:21:30.649672Z",
          "shell.execute_reply.started": "2025-01-28T19:21:30.058981Z",
          "shell.execute_reply": "2025-01-28T19:21:30.649017Z"
        },
        "id": "aIZxYPJHa2Zc",
        "outputId": "3445336d-24d9-40bd-8791-e970523f4927"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|| 44.7M/44.7M [00:00<00:00, 205MB/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss, val_correct = 0, 0\n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, images, labels in val_loader:\n",
        "            input_ids, attention_mask, images, labels = (\n",
        "                input_ids.to(device),\n",
        "                attention_mask.to(device),\n",
        "                images.to(device),\n",
        "                labels.to(device),\n",
        "            )\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Get predicted labels\n",
        "            pred_labels = outputs.argmax(1)\n",
        "            all_true_labels.extend(labels.cpu().numpy())\n",
        "            all_pred_labels.extend(pred_labels.cpu().numpy())\n",
        "\n",
        "            val_correct += (pred_labels == labels).sum().item()\n",
        "\n",
        "    val_acc = val_correct / len(val_loader.dataset)\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_true_labels, all_pred_labels))\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss, train_correct = 0, 0\n",
        "        for input_ids, attention_mask, images, labels in train_loader:\n",
        "            input_ids, attention_mask, images, labels = (\n",
        "                input_ids.to(device),\n",
        "                attention_mask.to(device),\n",
        "                images.to(device),\n",
        "                labels.to(device),\n",
        "            )\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "        train_acc = train_correct / len(train_loader.dataset)\n",
        "\n",
        "        # Validation and classification report\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)\n",
        "evaluate_model(model, val_loader, criterion)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:21:33.744598Z",
          "iopub.execute_input": "2025-01-28T19:21:33.744955Z",
          "iopub.status.idle": "2025-01-28T19:24:22.719264Z",
          "shell.execute_reply.started": "2025-01-28T19:21:33.744925Z",
          "shell.execute_reply": "2025-01-28T19:24:22.717907Z"
        },
        "id": "CfkR1xc2a2Zd",
        "outputId": "9f1f0ddc-5b5d-409a-a41c-b25b6d8bd7f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/5, Train Loss: 23.8721, Train Acc: 0.7219\nEpoch 2/5, Train Loss: 12.3897, Train Acc: 0.8953\nEpoch 3/5, Train Loss: 6.0545, Train Acc: 0.9500\nEpoch 4/5, Train Loss: 3.3239, Train Acc: 0.9750\nEpoch 5/5, Train Loss: 1.4514, Train Acc: 0.9969\nValidation Loss: 0.3862, Validation Accuracy: 0.8750\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.90      0.90        97\n           1       0.84      0.84      0.84        63\n\n    accuracy                           0.88       160\n   macro avg       0.87      0.87      0.87       160\nweighted avg       0.88      0.88      0.88       160\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, ViTModel, ViTFeatureExtractor\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Modified Dataset class for test data (without labels)\n",
        "class MalayalamMemeTestDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, max_len, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.max_len = max_len\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        text = row[\"transcriptions\"]\n",
        "        img_path = os.path.join(self.img_dir, f\"{row['image_id']}.jpg\")\n",
        "\n",
        "        # Process text\n",
        "        encoded_text = tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoded_text[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoded_text[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Process image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return input_ids, attention_mask, image\n",
        "\n",
        "# Define transformations for image data\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:25:46.070405Z",
          "iopub.execute_input": "2025-01-28T19:25:46.070763Z",
          "iopub.status.idle": "2025-01-28T19:25:46.078919Z",
          "shell.execute_reply.started": "2025-01-28T19:25:46.070734Z",
          "shell.execute_reply": "2025-01-28T19:25:46.077656Z"
        },
        "id": "9QFFAnTza2Zd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Function to generate predictions on the test dataset\n",
        "# Function to generate predictions on the test dataset\n",
        "def generate_predictions(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ids = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (input_ids, attention_mask, images) in enumerate(test_loader):  # Track index (idx)\n",
        "            input_ids, attention_mask, images = (\n",
        "                input_ids.to(device),\n",
        "                attention_mask.to(device),\n",
        "                images.to(device),\n",
        "            )\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            pred_labels = outputs.argmax(1).cpu().numpy()\n",
        "\n",
        "            predictions.extend(pred_labels)\n",
        "            # Correct the indexing of image_ids\n",
        "            start_idx = input_ids.size(0) * idx\n",
        "            end_idx = input_ids.size(0) * (idx + 1)\n",
        "            ids.extend(test_loader.dataset.data['image_id'].iloc[start_idx:end_idx].values)\n",
        "\n",
        "    return ids, predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the test dataset and create DataLoader\n",
        "test_dataset = MalayalamMemeTestDataset(\n",
        "    csv_file=\"/kaggle/input/misogyny-meme/Test/test.csv\",\n",
        "    img_dir=image_dir_test,\n",
        "    max_len=128,\n",
        "    transform=image_transforms\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Generate predictions for the test dataset\n",
        "test_ids, test_predictions = generate_predictions(model, test_loader)\n",
        "\n",
        "# Create a DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    \"id\": test_ids,\n",
        "    \"predictions\": test_predictions\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_file = \"/kaggle/working/One_by_zero_Malayalam_run3.csv\"\n",
        "submission_df.to_csv(submission_file, index=False, header=False)\n",
        "\n",
        "# Create the zip file for submission\n",
        "import zipfile\n",
        "\n",
        "zip_file = \"/kaggle/working/One_by_zero.run3.zip\"\n",
        "with zipfile.ZipFile(zip_file, 'w') as zf:\n",
        "    zf.write(submission_file, os.path.basename(submission_file))\n",
        "\n",
        "# Output path to the zip file\n",
        "zip_file\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:25:48.181575Z",
          "iopub.execute_input": "2025-01-28T19:25:48.181918Z",
          "iopub.status.idle": "2025-01-28T19:25:53.718521Z",
          "shell.execute_reply.started": "2025-01-28T19:25:48.181849Z",
          "shell.execute_reply": "2025-01-28T19:25:53.717749Z"
        },
        "id": "rZqjH6D3a2Zd",
        "outputId": "f2805156-29e0-418f-be3c-d96f611f798d"
      },
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/kaggle/working/One_by_zero.run3.zip'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the test data with actual labels\n",
        "test_data_labels = pd.read_csv('/kaggle/input/misogyny-meme/Test_Labels/test_with_labels.csv')\n",
        "\n",
        "# Merge predicted labels with actual labels\n",
        "test_data_labels['predictions'] = test_predictions\n",
        "merged_data = test_data_labels[['image_id', 'labels', 'predictions']]\n",
        "\n",
        "# Step 2: Calculate evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(merged_data['labels'], merged_data['predictions'])\n",
        "\n",
        "# Precision, Recall, and F1-score (Macro)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(merged_data['labels'], merged_data['predictions'], average='macro')\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (Macro): {precision:.4f}\")\n",
        "print(f\"Recall (Macro): {recall:.4f}\")\n",
        "print(f\"Macro F1-Score: {f1:.4f}\")\n",
        "\n",
        "# You can also print a detailed classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(merged_data['labels'], merged_data['predictions']))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:25:56.246825Z",
          "iopub.execute_input": "2025-01-28T19:25:56.247188Z",
          "iopub.status.idle": "2025-01-28T19:25:56.279707Z",
          "shell.execute_reply.started": "2025-01-28T19:25:56.247163Z",
          "shell.execute_reply": "2025-01-28T19:25:56.278955Z"
        },
        "id": "V5ujWmMEa2Zd",
        "outputId": "8e6c0823-ba38-4173-f460-a01ec5a8d3e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 0.9000\nPrecision (Macro): 0.8925\nRecall (Macro): 0.9065\nMacro F1-Score: 0.8970\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      0.88      0.91       122\n           1       0.83      0.94      0.88        78\n\n    accuracy                           0.90       200\n   macro avg       0.89      0.91      0.90       200\nweighted avg       0.91      0.90      0.90       200\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(merged_data['labels'], merged_data['predictions'])\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non-Misogynistic\", \"Misogynistic\"], yticklabels=[\"Non-Misogynistic\", \"Misogynistic\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:26:11.177849Z",
          "iopub.execute_input": "2025-01-28T19:26:11.178247Z",
          "iopub.status.idle": "2025-01-28T19:26:11.685094Z",
          "shell.execute_reply.started": "2025-01-28T19:26:11.17822Z",
          "shell.execute_reply": "2025-01-28T19:26:11.683987Z"
        },
        "id": "F6ufKq6Ka2Zd",
        "outputId": "60ff18c8-cfae-4285-84aa-3f9246a7d7ee"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 600x500 with 2 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Print first five samples with their actual and predicted labels\n",
        "print(\"First 5 Samples with Actual and Predicted Labels:\")\n",
        "for index, row in merged_data.head(30).iterrows():\n",
        "    print(f\"Image ID: {row['image_id']}, Actual Label: {row['labels']}, Predicted Label: {row['predictions']}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T19:28:06.543634Z",
          "iopub.execute_input": "2025-01-28T19:28:06.543989Z",
          "iopub.status.idle": "2025-01-28T19:28:06.55581Z",
          "shell.execute_reply.started": "2025-01-28T19:28:06.543961Z",
          "shell.execute_reply": "2025-01-28T19:28:06.554966Z"
        },
        "id": "bq10xgIOa2Ze",
        "outputId": "40dc5fa1-b59b-40ac-aa08-77f410788021"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "First 5 Samples with Actual and Predicted Labels:\nImage ID: 954, Actual Label: 0, Predicted Label: 0\nImage ID: 239, Actual Label: 0, Predicted Label: 0\nImage ID: 61, Actual Label: 1, Predicted Label: 1\nImage ID: 984, Actual Label: 0, Predicted Label: 0\nImage ID: 774, Actual Label: 0, Predicted Label: 0\nImage ID: 427, Actual Label: 1, Predicted Label: 1\nImage ID: 960, Actual Label: 0, Predicted Label: 0\nImage ID: 387, Actual Label: 0, Predicted Label: 0\nImage ID: 520, Actual Label: 0, Predicted Label: 0\nImage ID: 563, Actual Label: 1, Predicted Label: 1\nImage ID: 856, Actual Label: 0, Predicted Label: 0\nImage ID: 545, Actual Label: 1, Predicted Label: 1\nImage ID: 511, Actual Label: 0, Predicted Label: 0\nImage ID: 252, Actual Label: 0, Predicted Label: 1\nImage ID: 742, Actual Label: 0, Predicted Label: 0\nImage ID: 873, Actual Label: 0, Predicted Label: 0\nImage ID: 566, Actual Label: 1, Predicted Label: 1\nImage ID: 325, Actual Label: 0, Predicted Label: 0\nImage ID: 760, Actual Label: 0, Predicted Label: 0\nImage ID: 27, Actual Label: 1, Predicted Label: 1\nImage ID: 544, Actual Label: 1, Predicted Label: 1\nImage ID: 297, Actual Label: 0, Predicted Label: 0\nImage ID: 258, Actual Label: 0, Predicted Label: 1\nImage ID: 63, Actual Label: 1, Predicted Label: 1\nImage ID: 857, Actual Label: 0, Predicted Label: 0\nImage ID: 433, Actual Label: 1, Predicted Label: 1\nImage ID: 543, Actual Label: 0, Predicted Label: 0\nImage ID: 794, Actual Label: 0, Predicted Label: 0\nImage ID: 710, Actual Label: 1, Predicted Label: 1\nImage ID: 725, Actual Label: 1, Predicted Label: 0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Vision Transformer + IndicBERT***"
      ],
      "metadata": {
        "id": "8mXaRQX3d81o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Vision Transformer + Malayalam BERT***"
      ],
      "metadata": {
        "id": "X9pL3SaoeDDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, ViTModel, ViTFeatureExtractor\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "vit_model_name = \"google/vit-base-patch16-224\"\n",
        "vit_model = ViTModel.from_pretrained(vit_model_name).to(device)\n",
        "vit_feature_extractor = ViTFeatureExtractor.from_pretrained(vit_model_name)\n",
        "\n",
        "# Define Dataset class\n",
        "class MalayalamMemeDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, max_len, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.max_len = max_len\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        text = row[\"transcriptions\"]\n",
        "        label = torch.tensor(row[\"labels\"], dtype=torch.long)\n",
        "        img_path = os.path.join(self.img_dir, f\"{row['image_id']}.jpg\")\n",
        "\n",
        "        # Process text\n",
        "        encoded_text = tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoded_text[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoded_text[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Process image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return input_ids, attention_mask, image, label\n",
        "\n",
        "# Define transformations for image data\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to match ViT input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=vit_feature_extractor.image_mean, std=vit_feature_extractor.image_std),\n",
        "])\n",
        "\n",
        "# Define dataset paths\n",
        "image_dir = '/kaggle/input/misogyny-meme/Train'\n",
        "image_dir_dev = '/kaggle/input/misogyny-meme/Dev'\n",
        "image_dir_test = '/kaggle/input/misogyny-meme/Test'\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = MalayalamMemeDataset(\n",
        "    csv_file=\"/kaggle/input/misogyny-meme/Train/train.csv\",\n",
        "    img_dir=image_dir,\n",
        "    max_len=128,\n",
        "    transform=image_transforms\n",
        ")\n",
        "\n",
        "val_dataset = MalayalamMemeDataset(\n",
        "    csv_file=\"/kaggle/input/misogyny-meme/Dev/dev.csv\",\n",
        "    img_dir=image_dir_dev,\n",
        "    max_len=128,\n",
        "    transform=image_transforms\n",
        ")\n",
        "\n",
        "test_dataset = MalayalamMemeDataset(\n",
        "    csv_file=\"/kaggle/input/misogyny-meme/Test/test.csv\",\n",
        "    img_dir=image_dir_test,\n",
        "    max_len=128,\n",
        "    transform=image_transforms\n",
        ")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T10:45:12.956741Z",
          "iopub.execute_input": "2025-01-28T10:45:12.957331Z",
          "iopub.status.idle": "2025-01-28T10:45:15.805281Z",
          "shell.execute_reply.started": "2025-01-28T10:45:12.957304Z",
          "shell.execute_reply": "2025-01-28T10:45:15.804397Z"
        },
        "colab": {
          "referenced_widgets": [
            "158d4d128c3c41fcbe3b87bd65754db0",
            "ee6de8e8762548ecbbd23d9ea8e184c6",
            "d518d8d094b24e0caeeed2d8d85a7c30"
          ]
        },
        "id": "aYgrknZOa2Ze",
        "outputId": "5a8c521a-e3b9-43ef-a306-0ad15271ece5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "158d4d128c3c41fcbe3b87bd65754db0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee6de8e8762548ecbbd23d9ea8e184c6"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d518d8d094b24e0caeeed2d8d85a7c30"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, vit_model, num_classes=2):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.vit = vit_model\n",
        "        self.text_fc = nn.Linear(bert_model.config.hidden_size, 128)\n",
        "        self.image_fc = nn.Linear(vit_model.config.hidden_size, 128)\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images):\n",
        "        # Text features\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = self.text_fc(bert_output.last_hidden_state[:, 0, :])\n",
        "\n",
        "        # Image features\n",
        "        vit_output = self.vit(pixel_values=images)\n",
        "        image_features = self.image_fc(vit_output.last_hidden_state[:, 0, :])\n",
        "\n",
        "        # Combine features\n",
        "        combined_features = torch.cat((text_features, image_features), dim=1)\n",
        "        logits = self.classifier(combined_features)\n",
        "        return logits\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = MultimodalClassifier(bert_model, vit_model).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T10:45:17.55723Z",
          "iopub.execute_input": "2025-01-28T10:45:17.557512Z",
          "iopub.status.idle": "2025-01-28T10:45:17.578138Z",
          "shell.execute_reply.started": "2025-01-28T10:45:17.55749Z",
          "shell.execute_reply": "2025-01-28T10:45:17.577326Z"
        },
        "id": "n-SB3iv0a2Ze"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss, train_correct = 0, 0\n",
        "        for input_ids, attention_mask, images, labels in train_loader:\n",
        "            input_ids, attention_mask, images, labels = (\n",
        "                input_ids.to(device),\n",
        "                attention_mask.to(device),\n",
        "                images.to(device),\n",
        "                labels.to(device),\n",
        "            )\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "        train_acc = train_correct / len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        evaluate_model(model, val_loader, criterion)\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss, val_correct = 0, 0\n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, images, labels in val_loader:\n",
        "            input_ids, attention_mask, images, labels = (\n",
        "                input_ids.to(device),\n",
        "                attention_mask.to(device),\n",
        "                images.to(device),\n",
        "                labels.to(device),\n",
        "            )\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Get predicted labels\n",
        "            pred_labels = outputs.argmax(1)\n",
        "            all_true_labels.extend(labels.cpu().numpy())\n",
        "            all_pred_labels.extend(pred_labels.cpu().numpy())\n",
        "\n",
        "            val_correct += (pred_labels == labels).sum().item()\n",
        "\n",
        "    val_acc = val_correct / len(val_loader.dataset)\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_true_labels, all_pred_labels))\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)\n",
        "evaluate_model(model, val_loader, criterion)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T10:45:19.909297Z",
          "iopub.execute_input": "2025-01-28T10:45:19.909728Z",
          "iopub.status.idle": "2025-01-28T10:50:31.307168Z",
          "shell.execute_reply.started": "2025-01-28T10:45:19.909673Z",
          "shell.execute_reply": "2025-01-28T10:50:31.306393Z"
        },
        "id": "5P--T9-6a2Ze",
        "outputId": "5189ba1d-2f07-421e-c7eb-4b8bacaf3bdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/5, Train Loss: 23.3307, Train Acc: 0.7266\nValidation Loss: 0.4750, Validation Accuracy: 0.8562\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.81      0.99      0.89        97\n           1       0.98      0.65      0.78        63\n\n    accuracy                           0.86       160\n   macro avg       0.89      0.82      0.84       160\nweighted avg       0.88      0.86      0.85       160\n\nEpoch 2/5, Train Loss: 14.0286, Train Acc: 0.8953\nValidation Loss: 0.3627, Validation Accuracy: 0.8750\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.87      0.93      0.90        97\n           1       0.88      0.79      0.83        63\n\n    accuracy                           0.88       160\n   macro avg       0.88      0.86      0.87       160\nweighted avg       0.88      0.88      0.87       160\n\nEpoch 3/5, Train Loss: 7.2237, Train Acc: 0.9547\nValidation Loss: 0.3387, Validation Accuracy: 0.8750\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.89      0.91      0.90        97\n           1       0.85      0.83      0.84        63\n\n    accuracy                           0.88       160\n   macro avg       0.87      0.87      0.87       160\nweighted avg       0.87      0.88      0.87       160\n\nEpoch 4/5, Train Loss: 3.5862, Train Acc: 0.9875\nValidation Loss: 0.3265, Validation Accuracy: 0.8812\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.89      0.92      0.90        97\n           1       0.87      0.83      0.85        63\n\n    accuracy                           0.88       160\n   macro avg       0.88      0.87      0.87       160\nweighted avg       0.88      0.88      0.88       160\n\nEpoch 5/5, Train Loss: 1.7892, Train Acc: 0.9953\nValidation Loss: 0.3418, Validation Accuracy: 0.8938\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.93      0.91        97\n           1       0.88      0.84      0.86        63\n\n    accuracy                           0.89       160\n   macro avg       0.89      0.88      0.89       160\nweighted avg       0.89      0.89      0.89       160\n\nValidation Loss: 0.3418, Validation Accuracy: 0.8938\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.93      0.91        97\n           1       0.88      0.84      0.86        63\n\n    accuracy                           0.89       160\n   macro avg       0.89      0.88      0.89       160\nweighted avg       0.89      0.89      0.89       160\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, ViTModel, ViTFeatureExtractor\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Modified Dataset class for test data (without labels)\n",
        "class MalayalamMemeTestDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, max_len, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.max_len = max_len\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        text = row[\"transcriptions\"]\n",
        "        img_path = os.path.join(self.img_dir, f\"{row['image_id']}.jpg\")\n",
        "\n",
        "        # Process text\n",
        "        encoded_text = tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoded_text[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoded_text[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Process image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return input_ids, attention_mask, image\n",
        "\n",
        "# Define transformations for image data\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Function to generate predictions on the test dataset\n",
        "# Function to generate predictions on the test dataset\n",
        "def generate_predictions(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ids = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (input_ids, attention_mask, images) in enumerate(test_loader):  # Track index (idx)\n",
        "            input_ids, attention_mask, images = (\n",
        "                input_ids.to(device),\n",
        "                attention_mask.to(device),\n",
        "                images.to(device),\n",
        "            )\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            pred_labels = outputs.argmax(1).cpu().numpy()\n",
        "\n",
        "            predictions.extend(pred_labels)\n",
        "            # Correct the indexing of image_ids\n",
        "            start_idx = input_ids.size(0) * idx\n",
        "            end_idx = input_ids.size(0) * (idx + 1)\n",
        "            ids.extend(test_loader.dataset.data['image_id'].iloc[start_idx:end_idx].values)\n",
        "\n",
        "    return ids, predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the test dataset and create DataLoader\n",
        "test_dataset = MalayalamMemeTestDataset(\n",
        "    csv_file=\"/kaggle/input/misogyny-meme/Test/test.csv\",\n",
        "    img_dir=image_dir_test,\n",
        "    max_len=128,\n",
        "    transform=image_transforms\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Generate predictions for the test dataset\n",
        "test_ids, test_predictions = generate_predictions(model, test_loader)\n",
        "\n",
        "# Create a DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    \"id\": test_ids,\n",
        "    \"predictions\": test_predictions\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_file = \"/kaggle/working/One_by_zero_Malayalam_run4.csv\"\n",
        "submission_df.to_csv(submission_file, index=False, header=False)\n",
        "\n",
        "# Create the zip file for submission\n",
        "import zipfile\n",
        "\n",
        "zip_file = \"/kaggle/working/One_by_zero.run4.zip\"\n",
        "with zipfile.ZipFile(zip_file, 'w') as zf:\n",
        "    zf.write(submission_file, os.path.basename(submission_file))\n",
        "\n",
        "# Output path to the zip file\n",
        "zip_file\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T10:51:34.037289Z",
          "iopub.execute_input": "2025-01-28T10:51:34.037579Z",
          "iopub.status.idle": "2025-01-28T10:51:41.710036Z",
          "shell.execute_reply.started": "2025-01-28T10:51:34.037555Z",
          "shell.execute_reply": "2025-01-28T10:51:41.70921Z"
        },
        "id": "P6KYIp3ga2Zk",
        "outputId": "f492dbc7-8c18-402c-cfff-32d3e8765b6b"
      },
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/kaggle/working/One_by_zero.run4.zip'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the test data with actual labels\n",
        "test_data_labels = pd.read_csv('/kaggle/input/misogyny-meme/Test_Labels/test_with_labels.csv')\n",
        "\n",
        "# Merge predicted labels with actual labels\n",
        "test_data_labels['predictions'] = test_predictions\n",
        "merged_data = test_data_labels[['image_id', 'labels', 'predictions']]\n",
        "\n",
        "# Step 2: Calculate evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(merged_data['labels'], merged_data['predictions'])\n",
        "\n",
        "# Precision, Recall, and F1-score (Macro)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(merged_data['labels'], merged_data['predictions'], average='macro')\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (Macro): {precision:.4f}\")\n",
        "print(f\"Recall (Macro): {recall:.4f}\")\n",
        "print(f\"Macro F1-Score: {f1:.4f}\")\n",
        "\n",
        "# You can also print a detailed classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(merged_data['labels'], merged_data['predictions']))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T10:51:42.91882Z",
          "iopub.execute_input": "2025-01-28T10:51:42.919104Z",
          "iopub.status.idle": "2025-01-28T10:51:42.946015Z",
          "shell.execute_reply.started": "2025-01-28T10:51:42.919083Z",
          "shell.execute_reply": "2025-01-28T10:51:42.94531Z"
        },
        "id": "0uB0lrXta2Zk",
        "outputId": "994d7ea3-e71f-4034-dffb-ee8ce40197d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 0.8850\nPrecision (Macro): 0.8882\nRecall (Macro): 0.8687\nMacro F1-Score: 0.8763\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.94      0.91       122\n           1       0.90      0.79      0.84        78\n\n    accuracy                           0.89       200\n   macro avg       0.89      0.87      0.88       200\nweighted avg       0.89      0.89      0.88       200\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "***CLIP + Malayalam BERT***"
      ],
      "metadata": {
        "id": "9Mdw4nMleNFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load tokenizer and pretrained BERT model\n",
        "bert_model_name = \"l3cube-pune/malayalam-bert\"\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = BertModel.from_pretrained(bert_model_name).to(device)\n",
        "\n",
        "# Load CLIP model and processor\n",
        "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "\n",
        "# Define Dataset\n",
        "class MalayalamMemeDatasetCLIP(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, max_len):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        text = row[\"transcriptions\"]\n",
        "        label = torch.tensor(row[\"labels\"], dtype=torch.long)\n",
        "        img_path = os.path.join(self.img_dir, f\"{row['image_id']}.jpg\")\n",
        "\n",
        "        # Process text\n",
        "        encoded_text = tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoded_text[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoded_text[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Process image with CLIP processor\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        clip_image = clip_processor(images=image, return_tensors=\"pt\")\n",
        "        return input_ids, attention_mask, clip_image[\"pixel_values\"].squeeze(0), label\n",
        "\n",
        "\n",
        "class CLIPBertMultimodalClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, clip_model, num_classes=2):\n",
        "        super(CLIPBertMultimodalClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.clip = clip_model.vision_model\n",
        "        self.text_fc = nn.Linear(bert_model.config.hidden_size, 128)\n",
        "        self.image_fc = nn.Linear(768, 128)  # Adjust input size to match CLIP output\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images):\n",
        "        # Text features from Malayalam BERT\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = self.text_fc(bert_output.last_hidden_state[:, 0, :])\n",
        "\n",
        "        # Image features from CLIP\n",
        "        clip_image_features = self.clip(images)[\"last_hidden_state\"][:, 0, :]\n",
        "        image_features = self.image_fc(clip_image_features)\n",
        "\n",
        "        # Combine features\n",
        "        combined_features = torch.cat((text_features, image_features), dim=1)\n",
        "        logits = self.classifier(combined_features)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "# Dataset paths\n",
        "image_dir_train = '/kaggle/input/misogyny-meme/Train'\n",
        "image_dir_dev = '/kaggle/input/misogyny-meme/Dev'\n",
        "image_dir_test = '/kaggle/input/misogyny-meme/Test'\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = MalayalamMemeDatasetCLIP(\n",
        "    csv_file=\"/kaggle/input/misogyny-meme/Train/train.csv\",\n",
        "    img_dir=image_dir_train,\n",
        "    max_len=128,\n",
        ")\n",
        "\n",
        "val_dataset = MalayalamMemeDatasetCLIP(\n",
        "    csv_file=\"/kaggle/input/misogyny-meme/Dev/dev.csv\",\n",
        "    img_dir=image_dir_dev,\n",
        "    max_len=128,\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "#test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = CLIPBertMultimodalClassifier(bert_model, clip_model).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss, train_correct = 0, 0\n",
        "        for input_ids, attention_mask, images, labels in train_loader:\n",
        "            input_ids, attention_mask, images, labels = (\n",
        "                input_ids.to(device),\n",
        "                attention_mask.to(device),\n",
        "                images.to(device),\n",
        "                labels.to(device),\n",
        "            )\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "        train_acc = train_correct / len(train_loader.dataset)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "        evaluate_model(model, val_loader, criterion)\n",
        "\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss, val_correct = 0, 0\n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, images, labels in val_loader:\n",
        "            input_ids, attention_mask, images, labels = (\n",
        "                input_ids.to(device),\n",
        "                attention_mask.to(device),\n",
        "                images.to(device),\n",
        "                labels.to(device),\n",
        "            )\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Get predicted labels\n",
        "            pred_labels = outputs.argmax(1)\n",
        "            all_true_labels.extend(labels.cpu().numpy())\n",
        "            all_pred_labels.extend(pred_labels.cpu().numpy())\n",
        "\n",
        "            val_correct += (pred_labels == labels).sum().item()\n",
        "\n",
        "    val_acc = val_correct / len(val_loader.dataset)\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_true_labels, all_pred_labels))\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)\n",
        "\n",
        "# Evaluate the model on validation set\n",
        "evaluate_model(model, val_loader, criterion)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T09:51:35.497799Z",
          "iopub.execute_input": "2025-01-28T09:51:35.498153Z",
          "iopub.status.idle": "2025-01-28T09:55:52.725435Z",
          "shell.execute_reply.started": "2025-01-28T09:51:35.498124Z",
          "shell.execute_reply": "2025-01-28T09:55:52.724646Z"
        },
        "colab": {
          "referenced_widgets": [
            "9e645be606eb40ab9d73abd4bfc077a7",
            "11f90faa3491477e94d662bada1654b0",
            "10c7fa28d3e848378ebfde01747e2a85",
            "0ac4eceffedb4833b03f8f788e9814bc",
            "c5ad697ee58842639786365cbdda06c9",
            "20f27484df204639be3d1b960f474ad6",
            "5be9a8ba734b4e29b4b78447f1e45412",
            "e2ebfaf4968c43ac8a7deb0270e2b91c"
          ]
        },
        "id": "o7qdUGraa2Zk",
        "outputId": "646f550e-6349-4d23-a8d1-ff1b8b59264f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of BertModel were not initialized from the model checkpoint at l3cube-pune/malayalam-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e645be606eb40ab9d73abd4bfc077a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11f90faa3491477e94d662bada1654b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10c7fa28d3e848378ebfde01747e2a85"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ac4eceffedb4833b03f8f788e9814bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5ad697ee58842639786365cbdda06c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20f27484df204639be3d1b960f474ad6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5be9a8ba734b4e29b4b78447f1e45412"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2ebfaf4968c43ac8a7deb0270e2b91c"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5, Train Loss: 19.3317, Train Acc: 0.7875\nValidation Loss: 0.3441, Validation Accuracy: 0.8500\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.85      0.91      0.88        97\n           1       0.84      0.76      0.80        63\n\n    accuracy                           0.85       160\n   macro avg       0.85      0.83      0.84       160\nweighted avg       0.85      0.85      0.85       160\n\nEpoch 2/5, Train Loss: 6.0077, Train Acc: 0.9484\nValidation Loss: 0.5320, Validation Accuracy: 0.7937\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.71      0.81        97\n           1       0.67      0.92      0.78        63\n\n    accuracy                           0.79       160\n   macro avg       0.80      0.82      0.79       160\nweighted avg       0.83      0.79      0.80       160\n\nEpoch 3/5, Train Loss: 1.2866, Train Acc: 0.9953\nValidation Loss: 0.4610, Validation Accuracy: 0.8625\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.89      0.89      0.89        97\n           1       0.83      0.83      0.83        63\n\n    accuracy                           0.86       160\n   macro avg       0.86      0.86      0.86       160\nweighted avg       0.86      0.86      0.86       160\n\nEpoch 4/5, Train Loss: 0.3709, Train Acc: 0.9969\nValidation Loss: 0.5260, Validation Accuracy: 0.8812\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.87      0.95      0.91        97\n           1       0.91      0.78      0.84        63\n\n    accuracy                           0.88       160\n   macro avg       0.89      0.86      0.87       160\nweighted avg       0.88      0.88      0.88       160\n\nEpoch 5/5, Train Loss: 0.2120, Train Acc: 0.9969\nValidation Loss: 0.5692, Validation Accuracy: 0.8750\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.87      0.94      0.90        97\n           1       0.89      0.78      0.83        63\n\n    accuracy                           0.88       160\n   macro avg       0.88      0.86      0.87       160\nweighted avg       0.88      0.88      0.87       160\n\nValidation Loss: 0.5692, Validation Accuracy: 0.8750\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.87      0.94      0.90        97\n           1       0.89      0.78      0.83        63\n\n    accuracy                           0.88       160\n   macro avg       0.88      0.86      0.87       160\nweighted avg       0.88      0.88      0.87       160\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, ViTModel, ViTFeatureExtractor\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Modified Dataset class for test data (without labels)\n",
        "class MalayalamMemeTestDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, max_len, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.max_len = max_len\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        text = row[\"transcriptions\"]\n",
        "        img_path = os.path.join(self.img_dir, f\"{row['image_id']}.jpg\")\n",
        "\n",
        "        # Process text\n",
        "        encoded_text = tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoded_text[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoded_text[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Process image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return input_ids, attention_mask, image\n",
        "\n",
        "# Define transformations for image data\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T09:57:20.183308Z",
          "iopub.execute_input": "2025-01-28T09:57:20.18362Z",
          "iopub.status.idle": "2025-01-28T09:57:20.191397Z",
          "shell.execute_reply.started": "2025-01-28T09:57:20.183596Z",
          "shell.execute_reply": "2025-01-28T09:57:20.190511Z"
        },
        "id": "PGNS3eUza2Zl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Function to generate predictions on the test dataset\n",
        "# Function to generate predictions on the test dataset\n",
        "def generate_predictions(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ids = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (input_ids, attention_mask, images) in enumerate(test_loader):\n",
        "            input_ids, attention_mask, images = (\n",
        "                input_ids.to(device),\n",
        "                attention_mask.to(device),\n",
        "                images.to(device),\n",
        "            )\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            pred_labels = outputs.argmax(1).cpu().numpy()\n",
        "            predictions.extend(pred_labels)\n",
        "\n",
        "            # Fetch the corresponding image IDs\n",
        "            batch_ids = test_loader.dataset.data.iloc[\n",
        "                batch_idx * len(pred_labels) : (batch_idx + 1) * len(pred_labels)\n",
        "            ][\"image_id\"]\n",
        "            ids.extend(batch_ids.values)\n",
        "\n",
        "    return ids, predictions\n",
        "\n",
        "\n",
        "\n",
        "# Load the test dataset and create DataLoader\n",
        "test_dataset = MalayalamMemeTestDataset(\n",
        "    csv_file=\"/kaggle/input/misogyny-meme/Test/test.csv\",\n",
        "    img_dir=image_dir_test,\n",
        "    max_len=128,\n",
        "    transform=image_transforms\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Generate predictions for the test dataset\n",
        "test_ids, test_predictions = generate_predictions(model, test_loader)\n",
        "\n",
        "# Create a DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    \"id\": test_ids,\n",
        "    \"predictions\": test_predictions\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_file = \"/kaggle/working/One_by_zero_Malayalam_run2.csv\"\n",
        "submission_df.to_csv(submission_file, index=False, header=False)\n",
        "\n",
        "# Create the zip file for submission\n",
        "import zipfile\n",
        "\n",
        "zip_file = \"/kaggle/working/One_by_zero.run2.zip\"\n",
        "with zipfile.ZipFile(zip_file, 'w') as zf:\n",
        "    zf.write(submission_file, os.path.basename(submission_file))\n",
        "\n",
        "# Output path to the zip file\n",
        "zip_file\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T09:57:25.178743Z",
          "iopub.execute_input": "2025-01-28T09:57:25.179019Z",
          "iopub.status.idle": "2025-01-28T09:57:31.306199Z",
          "shell.execute_reply.started": "2025-01-28T09:57:25.178997Z",
          "shell.execute_reply": "2025-01-28T09:57:31.30551Z"
        },
        "id": "eswpZFwUa2Zl",
        "outputId": "342d8106-bb14-467f-c7e4-41afd7fe5928"
      },
      "outputs": [
        {
          "execution_count": 33,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/kaggle/working/One_by_zero.run2.zip'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the test data with actual labels\n",
        "test_data_labels = pd.read_csv('/kaggle/input/misogyny-meme/Test_Labels/test_with_labels.csv')\n",
        "\n",
        "# Merge predicted labels with actual labels\n",
        "test_data_labels['predictions'] = test_predictions\n",
        "merged_data = test_data_labels[['image_id', 'labels', 'predictions']]\n",
        "\n",
        "# Step 2: Calculate evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(merged_data['labels'], merged_data['predictions'])\n",
        "\n",
        "# Precision, Recall, and F1-score (Macro)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(merged_data['labels'], merged_data['predictions'], average='macro')\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (Macro): {precision:.4f}\")\n",
        "print(f\"Recall (Macro): {recall:.4f}\")\n",
        "print(f\"Macro F1-Score: {f1:.4f}\")\n",
        "\n",
        "# You can also print a detailed classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(merged_data['labels'], merged_data['predictions']))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T09:57:36.875556Z",
          "iopub.execute_input": "2025-01-28T09:57:36.875842Z",
          "iopub.status.idle": "2025-01-28T09:57:36.900007Z",
          "shell.execute_reply.started": "2025-01-28T09:57:36.87582Z",
          "shell.execute_reply": "2025-01-28T09:57:36.899073Z"
        },
        "id": "epNj948Za2Zl",
        "outputId": "c5df16b1-dc69-4a49-c8af-232c3870c7fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 0.8600\nPrecision (Macro): 0.8762\nRecall (Macro): 0.8321\nMacro F1-Score: 0.8451\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.84      0.96      0.89       122\n           1       0.92      0.71      0.80        78\n\n    accuracy                           0.86       200\n   macro avg       0.88      0.83      0.85       200\nweighted avg       0.87      0.86      0.86       200\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "hO0xzBzea2Zl"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}